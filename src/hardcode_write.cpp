// This file is special in that it is included into the autogenerated file,
// and not compiled as a separate unit.

#ifdef COMPILE_LAYER
const VkLayerProperties LAYER_PROPERTIES = {
	"VK_LAYER_ARM_lavatube",
	VK_MAKE_VERSION(1, 1, VK_HEADER_VERSION),
	1,
	"Layer used for generating vulkan API traces",
};
#endif

/// Cached system properties (these may not be in a sane state at shutdown time)
static VkPhysicalDeviceMemoryProperties real_memory_properties GUARDED_BY(frame_mutex) = {};
static std::vector<VkExtensionProperties> instance_extension_properties GUARDED_BY(frame_mutex);
static std::vector<VkExtensionProperties> device_extension_properties GUARDED_BY(frame_mutex);

// variables to tell if the host supports something and it is enabled; TBD: generalize this
static bool enabled_VK_EXT_tooling_info GUARDED_BY(frame_mutex) = false;

// Vulkan has a fundamental problem that it does not require the user to
// tell the API what its memory requirements are for any given memory allocation.
// So the reason for a choice of a particular "memory type index" cannot be
// deduced from its flags. We therefore need to decouple the flags a bit to
// force the user to reveal the reasons for his or her choices. We cannot merge
// any memory type, since they may be highly specialized, but we can split them.
static VkPhysicalDeviceMemoryProperties virtual_memory_properties GUARDED_BY(frame_mutex) = {};
static uint32_t remap_memory_types_to_real[VK_MAX_MEMORY_TYPES] GUARDED_BY(frame_mutex);

static trackable* debug_object_trackable(trace_records& r, VkDebugReportObjectTypeEXT type, uint64_t object)
{
	switch (type)
	{
	case VK_DEBUG_REPORT_OBJECT_TYPE_INSTANCE_EXT: return r.VkInstance_index.at((const VkInstance)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_PHYSICAL_DEVICE_EXT: return r.VkPhysicalDevice_index.at((VkPhysicalDevice)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_DEVICE_EXT: return r.VkDevice_index.at((VkDevice)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_QUEUE_EXT: return (p__virtualqueues) ? (trackable*)object : r.VkQueue_index.at((const VkQueue)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_DEVICE_MEMORY_EXT: return r.VkDeviceMemory_index.at((const VkDeviceMemory)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_SEMAPHORE_EXT: return r.VkSemaphore_index.at((const VkSemaphore)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_COMMAND_BUFFER_EXT: return r.VkCommandBuffer_index.at((const VkCommandBuffer)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_FENCE_EXT: return r.VkFence_index.at((const VkFence)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_BUFFER_EXT: return r.VkBuffer_index.at((const VkBuffer)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_IMAGE_EXT: return r.VkImage_index.at((const VkImage)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_EVENT_EXT: return r.VkEvent_index.at((const VkEvent)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_QUERY_POOL_EXT: return r.VkQueryPool_index.at((const VkQueryPool)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_BUFFER_VIEW_EXT: return r.VkBufferView_index.at((const VkBufferView)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_IMAGE_VIEW_EXT: return r.VkImageView_index.at((const VkImageView)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_SHADER_MODULE_EXT: return r.VkShaderModule_index.at((const VkShaderModule)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_PIPELINE_CACHE_EXT: return r.VkPipelineCache_index.at((const VkPipelineCache)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_PIPELINE_LAYOUT_EXT: return r.VkPipelineLayout_index.at((const VkPipelineLayout)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_RENDER_PASS_EXT: return r.VkRenderPass_index.at((const VkRenderPass)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_PIPELINE_EXT: return r.VkPipeline_index.at((const VkPipeline)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_DESCRIPTOR_SET_LAYOUT_EXT: return r.VkDescriptorSetLayout_index.at((const VkDescriptorSetLayout)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_SAMPLER_EXT: return r.VkSampler_index.at((const VkSampler)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_DESCRIPTOR_POOL_EXT: return r.VkDescriptorPool_index.at((const VkDescriptorPool)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_DESCRIPTOR_SET_EXT: return r.VkDescriptorSet_index.at((const VkDescriptorSet)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_FRAMEBUFFER_EXT: return r.VkFramebuffer_index.at((const VkFramebuffer)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_COMMAND_POOL_EXT: return r.VkCommandPool_index.at((const VkCommandPool)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_SURFACE_KHR_EXT: return r.VkSurfaceKHR_index.at((const VkSurfaceKHR)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_SWAPCHAIN_KHR_EXT: return r.VkSwapchainKHR_index.at((const VkSwapchainKHR)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_DESCRIPTOR_UPDATE_TEMPLATE_KHR_EXT: return r.VkDescriptorUpdateTemplate_index.at((const VkDescriptorUpdateTemplate)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_DISPLAY_KHR_EXT: return r.VkDisplayKHR_index.at((const VkDisplayKHR)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_DISPLAY_MODE_KHR_EXT: return r.VkDisplayModeKHR_index.at((const VkDisplayModeKHR)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_VALIDATION_CACHE_EXT: return r.VkValidationCacheEXT_index.at((const VkValidationCacheEXT)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_SAMPLER_YCBCR_CONVERSION_EXT: return r.VkSamplerYcbcrConversion_index.at((const VkSamplerYcbcrConversion)object);
	case VK_DEBUG_REPORT_OBJECT_TYPE_ACCELERATION_STRUCTURE_KHR_EXT: return r.VkAccelerationStructureKHR_index.at((const VkAccelerationStructureKHR)object);
	// not supported:
	case VK_DEBUG_REPORT_OBJECT_TYPE_DEBUG_REPORT_EXT:
	case VK_DEBUG_REPORT_OBJECT_TYPE_ACCELERATION_STRUCTURE_NV_EXT:
	case VK_DEBUG_REPORT_OBJECT_TYPE_CU_FUNCTION_NVX_EXT:
	case VK_DEBUG_REPORT_OBJECT_TYPE_CU_MODULE_NVX_EXT:
	case VK_DEBUG_REPORT_OBJECT_TYPE_BUFFER_COLLECTION_FUCHSIA_EXT:
	case VK_DEBUG_REPORT_OBJECT_TYPE_UNKNOWN_EXT:
	case VK_DEBUG_REPORT_OBJECT_TYPE_MAX_ENUM_EXT: assert(false); return nullptr;
	}
	return nullptr;
}

static trackable* object_trackable(const trace_records& r, VkObjectType type, uint64_t object)
{
	switch (type)
	{
	case VK_OBJECT_TYPE_INSTANCE: return r.VkInstance_index.at((VkInstance)object);
	case VK_OBJECT_TYPE_PHYSICAL_DEVICE: return r.VkPhysicalDevice_index.at((VkPhysicalDevice)object);
	case VK_OBJECT_TYPE_DEVICE: return r.VkDevice_index.at((VkDevice)object);
	case VK_OBJECT_TYPE_QUEUE: return (p__virtualqueues) ? (trackable*)object : r.VkQueue_index.at((const VkQueue)object);
	case VK_OBJECT_TYPE_DEVICE_MEMORY: return r.VkDeviceMemory_index.at((const VkDeviceMemory)object);
	case VK_OBJECT_TYPE_SEMAPHORE: return r.VkSemaphore_index.at((const VkSemaphore)object);
	case VK_OBJECT_TYPE_COMMAND_BUFFER: return r.VkCommandBuffer_index.at((const VkCommandBuffer)object);
	case VK_OBJECT_TYPE_FENCE: return r.VkFence_index.at((const VkFence)object);
	case VK_OBJECT_TYPE_BUFFER: return r.VkBuffer_index.at((const VkBuffer)object);
	case VK_OBJECT_TYPE_IMAGE: return r.VkImage_index.at((const VkImage)object);
	case VK_OBJECT_TYPE_EVENT: return r.VkEvent_index.at((const VkEvent)object);
	case VK_OBJECT_TYPE_QUERY_POOL: return r.VkQueryPool_index.at((const VkQueryPool)object);
	case VK_OBJECT_TYPE_BUFFER_VIEW: return r.VkBufferView_index.at((const VkBufferView)object);
	case VK_OBJECT_TYPE_IMAGE_VIEW: return r.VkImageView_index.at((const VkImageView)object);
	case VK_OBJECT_TYPE_SHADER_MODULE: return r.VkShaderModule_index.at((const VkShaderModule)object);
	case VK_OBJECT_TYPE_PIPELINE_CACHE: return r.VkPipelineCache_index.at((const VkPipelineCache)object);
	case VK_OBJECT_TYPE_PIPELINE_LAYOUT: return r.VkPipelineLayout_index.at((const VkPipelineLayout)object);
	case VK_OBJECT_TYPE_RENDER_PASS: return r.VkRenderPass_index.at((const VkRenderPass)object);
	case VK_OBJECT_TYPE_PIPELINE: return r.VkPipeline_index.at((const VkPipeline)object);
	case VK_OBJECT_TYPE_DESCRIPTOR_SET_LAYOUT: return r.VkDescriptorSetLayout_index.at((const VkDescriptorSetLayout)object);
	case VK_OBJECT_TYPE_SAMPLER: return r.VkSampler_index.at((const VkSampler)object);
	case VK_OBJECT_TYPE_DESCRIPTOR_POOL: return r.VkDescriptorPool_index.at((const VkDescriptorPool)object);
	case VK_OBJECT_TYPE_DESCRIPTOR_SET: return r.VkDescriptorSet_index.at((const VkDescriptorSet)object);
	case VK_OBJECT_TYPE_FRAMEBUFFER: return r.VkFramebuffer_index.at((const VkFramebuffer)object);
	case VK_OBJECT_TYPE_COMMAND_POOL: return r.VkCommandPool_index.at((const VkCommandPool)object);
	case VK_OBJECT_TYPE_SURFACE_KHR: return r.VkSurfaceKHR_index.at((const VkSurfaceKHR)object);
	case VK_OBJECT_TYPE_SWAPCHAIN_KHR: return r.VkSwapchainKHR_index.at((const VkSwapchainKHR)object);
	case VK_OBJECT_TYPE_DESCRIPTOR_UPDATE_TEMPLATE: return r.VkDescriptorUpdateTemplate_index.at((const VkDescriptorUpdateTemplate)object);
	case VK_OBJECT_TYPE_PRIVATE_DATA_SLOT: return r.VkPrivateDataSlot_index.at((const VkPrivateDataSlot)object);
	case VK_OBJECT_TYPE_DISPLAY_KHR: return r.VkDisplayKHR_index.at((const VkDisplayKHR)object);
	case VK_OBJECT_TYPE_DISPLAY_MODE_KHR: return r.VkDisplayModeKHR_index.at((const VkDisplayModeKHR)object);
	case VK_OBJECT_TYPE_ACCELERATION_STRUCTURE_KHR: return r.VkAccelerationStructureKHR_index.at((const VkAccelerationStructureKHR)object);
	case VK_OBJECT_TYPE_VALIDATION_CACHE_EXT: return r.VkValidationCacheEXT_index.at((const VkValidationCacheEXT)object);
	case VK_OBJECT_TYPE_DEFERRED_OPERATION_KHR: return r.VkDeferredOperationKHR_index.at((const VkDeferredOperationKHR)object);
	case VK_OBJECT_TYPE_MICROMAP_EXT: return r.VkMicromapEXT_index.at((const VkMicromapEXT)object);
	case VK_OBJECT_TYPE_SAMPLER_YCBCR_CONVERSION_KHR: return r.VkSamplerYcbcrConversion_index.at((const VkSamplerYcbcrConversion)object);
	case VK_OBJECT_TYPE_SHADER_EXT: return r.VkShaderEXT_index.at((const VkShaderEXT)object);
	case VK_OBJECT_TYPE_VIDEO_SESSION_KHR: return r.VkVideoSessionKHR_index.at((const VkVideoSessionKHR)object);
	case VK_OBJECT_TYPE_VIDEO_SESSION_PARAMETERS_KHR: return r.VkVideoSessionParametersKHR_index.at((const VkVideoSessionParametersKHR)object);
	case VK_OBJECT_TYPE_DEBUG_REPORT_CALLBACK_EXT: return r.VkDebugReportCallbackEXT_index.at((const VkDebugReportCallbackEXT)object);
	case VK_OBJECT_TYPE_DEBUG_UTILS_MESSENGER_EXT: return r.VkDebugUtilsMessengerEXT_index.at((const VkDebugUtilsMessengerEXT)object);
	// not supported:
	case VK_OBJECT_TYPE_CU_MODULE_NVX:
	case VK_OBJECT_TYPE_CU_FUNCTION_NVX:
	case VK_OBJECT_TYPE_ACCELERATION_STRUCTURE_NV:
	case VK_OBJECT_TYPE_PERFORMANCE_CONFIGURATION_INTEL:
	case VK_OBJECT_TYPE_INDIRECT_COMMANDS_LAYOUT_NV:
	case VK_OBJECT_TYPE_BUFFER_COLLECTION_FUCHSIA:
	case VK_OBJECT_TYPE_OPTICAL_FLOW_SESSION_NV:
	case VK_OBJECT_TYPE_UNKNOWN:
	case VK_OBJECT_TYPE_MAX_ENUM: assert(false); return nullptr;
	}
	return nullptr;
}

static void trace_post_vkAcquireNextImageKHR(lava_file_writer& writer, VkResult result, VkDevice device, VkSwapchainKHR swapchain, uint64_t timeout, VkSemaphore semaphore, VkFence fence, uint32_t* pImageIndex)
{
	DLOG("Acquired swapchain image index=%u", *pImageIndex);
	if (result != VK_SUCCESS) ILOG("vkAcquireNextImageKHR error: %s", errorString(result));
	assert(result == VK_SUCCESS);
}

static void trace_post_vkAcquireNextImage2KHR(lava_file_writer& writer, VkResult result, VkDevice device, const VkAcquireNextImageInfoKHR* pAcquireInfo, uint32_t* pImageIndex)
{
	DLOG("Acquired swapchain image index=%u", *pImageIndex);
	if (result != VK_SUCCESS) ILOG("vkAcquireNextImageKHR error: %s", errorString(result));
	assert(result == VK_SUCCESS);
}

static void trace_post_vkGetPhysicalDeviceFormatProperties(lava_file_writer& writer, VkPhysicalDevice physicalDevice, VkFormat format, VkFormatProperties* pFormatProperties)
{
	// TBD implement format compatibility screening here
}

static void trace_post_vkGetPhysicalDeviceFormatProperties2(lava_file_writer& writer, VkPhysicalDevice physicalDevice, VkFormat format, VkFormatProperties2* pFormatProperties)
{
	trace_post_vkGetPhysicalDeviceFormatProperties(writer, physicalDevice, format, &pFormatProperties->formatProperties);
}

static void trace_post_vkGetPhysicalDeviceFormatProperties2KHR(lava_file_writer& writer, VkPhysicalDevice physicalDevice, VkFormat format, VkFormatProperties2* pFormatProperties)
{
	trace_post_vkGetPhysicalDeviceFormatProperties(writer, physicalDevice, format, &pFormatProperties->formatProperties);
}

static void trace_post_vkGetPhysicalDeviceFeatures(lava_file_writer& writer, VkPhysicalDevice physicalDevice, VkPhysicalDeviceFeatures* pFeatures)
{
	// TBD implement feature compatibility screening here
}

static void trace_post_vkGetPhysicalDeviceFeatures2(lava_file_writer& writer, VkPhysicalDevice physicalDevice, VkPhysicalDeviceFeatures2* pFeatures)
{
	// TBD implement feature compatibility screening here

	VkPhysicalDeviceVulkan11Features* vk11 = (VkPhysicalDeviceVulkan11Features*)find_extension(pFeatures, VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_VULKAN_1_1_FEATURES);
	if (vk11)
	{
		vk11->protectedMemory = VK_FALSE; // this will not ever work
		vk11->variablePointers = VK_FALSE; // not supported yet
		vk11->variablePointersStorageBuffer = VK_FALSE; // not supported yet
	}

	VkBenchmarkingTRACETOOLTEST* benchmarking = (VkBenchmarkingTRACETOOLTEST*)find_extension(pFeatures, VK_STRUCTURE_TYPE_BENCHMARKING_TRACETOOLTEST);
	if (benchmarking)
	{
		benchmarking->flags = 0;
		benchmarking->fixedTimeStep = 30;
		benchmarking->disablePerformanceAdaptation = VK_TRUE;
		benchmarking->disableVendorAdaptation = VK_TRUE;
		benchmarking->disableLoadingFrames = VK_FALSE;
		benchmarking->visualSettings = 0;
		benchmarking->scenario = 0;
		benchmarking->loopTime = 0;
		benchmarking->tracingFlags = (VkTracingFlagsTRACETOOLTEST)(VK_TRACING_NO_COHERENT_MEMORY_BIT_TRACETOOLTEST | VK_TRACING_NO_MEMORY_ALIASING_BIT_TRACETOOLTEST
			| VK_TRACING_NO_POINTER_OFFSETS_BIT_TRACETOOLTEST | VK_TRACING_NO_JUST_IN_TIME_REUSE_BIT_TRACETOOLTEST);
	}
}

static void trace_post_vkGetPhysicalDeviceFeatures2KHR(lava_file_writer& writer, VkPhysicalDevice physicalDevice, VkPhysicalDeviceFeatures2* pFeatures)
{
	trace_post_vkGetPhysicalDeviceFeatures2(writer, physicalDevice, pFeatures);
}

static void trace_post_vkGetPhysicalDeviceProperties(lava_file_writer& writer, VkPhysicalDevice physicalDevice, VkPhysicalDeviceProperties* pProperties)
{
	memcpy(pProperties->pipelineCacheUUID, writer.parent->fakeUUID, VK_UUID_SIZE);

	// TBD implement property compatibility screening here
}

static void trace_post_vkGetPhysicalDeviceSurfaceCapabilitiesKHR(lava_file_writer& writer, VkResult result, VkPhysicalDevice physicalDevice, VkSurfaceKHR surface, VkSurfaceCapabilitiesKHR* pSurfaceCapabilities)
{
	assert(result == VK_SUCCESS);
	const unsigned swapchains = num_swapchains(); // could also be something other than 3, if user overrides it
	if (pSurfaceCapabilities->minImageCount <= swapchains && swapchains != 0)
	{
		pSurfaceCapabilities->maxImageCount = swapchains;
	}
	if (pSurfaceCapabilities->maxImageCount >= swapchains && swapchains != 0)
	{
		pSurfaceCapabilities->minImageCount = swapchains;
	}
}

static void extend_bits(VkMemoryRequirements* pMemoryRequirements)
{
	frame_mutex.lock();
	assert(virtual_memory_properties.memoryTypeCount > 0);
	// extend the bits with our virtual memory types
	for (uint32_t i = real_memory_properties.memoryTypeCount; i < virtual_memory_properties.memoryTypeCount; i++)
	{
		if ((1 << remap_memory_types_to_real[i]) & pMemoryRequirements->memoryTypeBits) // is supported?
		{
			pMemoryRequirements->memoryTypeBits |= 1 << i; // yes, set
		}
		else
		{
			pMemoryRequirements->memoryTypeBits &= ~(1 << i); // no, clear
		}
	}
	// extend alignment
	pMemoryRequirements->alignment = std::max<VkDeviceSize>(pMemoryRequirements->alignment, 256);
	frame_mutex.unlock();
}

static void trace_post_vkGetBufferMemoryRequirements(lava_file_writer& writer, VkDevice device, VkBuffer buffer, VkMemoryRequirements* pMemoryRequirements)
{
	auto* buffer_data = writer.parent->records.VkBuffer_index.at(buffer);
	buffer_data->req = *pMemoryRequirements;
	extend_bits(pMemoryRequirements);
}

static void trace_post_vkGetImageMemoryRequirements(lava_file_writer& writer, VkDevice device, VkImage image, VkMemoryRequirements* pMemoryRequirements)
{
	auto* image_data = writer.parent->records.VkImage_index.at(image);
	image_data->req = *pMemoryRequirements;
	extend_bits(pMemoryRequirements);
}

static void inject_dedicated_allocation(lava_file_writer& writer, VkBaseOutStructure* pMemoryRequirements, bool image)
{
	if ((p__dedicated_image == 0 && image) || (p__dedicated_buffer == 0 && !image)) return;

	VkMemoryDedicatedRequirements* info = (VkMemoryDedicatedRequirements*)find_extension(pMemoryRequirements, VK_STRUCTURE_TYPE_MEMORY_DEDICATED_REQUIREMENTS);
	if (!info)
	{
		info = writer.pool.allocate<VkMemoryDedicatedRequirements>(1); // reserve memory for dedicated allocation extension structure
		info->sType = VK_STRUCTURE_TYPE_MEMORY_DEDICATED_REQUIREMENTS;
		info->pNext = pMemoryRequirements->pNext;
		info->prefersDedicatedAllocation = VK_FALSE;
		info->requiresDedicatedAllocation = VK_FALSE;
		pMemoryRequirements->pNext = (VkBaseOutStructure*)info;
	}

	if (image)
	{
		if (p__dedicated_image == 1) info->prefersDedicatedAllocation = VK_TRUE;
		else if (p__dedicated_image == 2) info->requiresDedicatedAllocation = VK_TRUE;
	}
	else // buffer
	{
		if (p__dedicated_buffer == 1) info->prefersDedicatedAllocation = VK_TRUE;
		else if (p__dedicated_buffer == 2) info->requiresDedicatedAllocation = VK_TRUE;
	}
}

static void trace_post_vkGetBufferMemoryRequirements2(lava_file_writer& writer, VkDevice device, const VkBufferMemoryRequirementsInfo2* pInfo, VkMemoryRequirements2* pMemoryRequirements)
{
	inject_dedicated_allocation(writer, (VkBaseOutStructure*)pMemoryRequirements, false);
	trace_post_vkGetBufferMemoryRequirements(writer, device, pInfo->buffer, &pMemoryRequirements->memoryRequirements);
}

static void trace_post_vkGetImageMemoryRequirements2(lava_file_writer& writer, VkDevice device, const VkImageMemoryRequirementsInfo2* pInfo, VkMemoryRequirements2* pMemoryRequirements)
{
	inject_dedicated_allocation(writer, (VkBaseOutStructure*)pMemoryRequirements, true);
	trace_post_vkGetImageMemoryRequirements(writer, device, pInfo->image, &pMemoryRequirements->memoryRequirements);
}

static void trace_post_vkGetBufferMemoryRequirements2KHR(lava_file_writer& writer, VkDevice device, const VkBufferMemoryRequirementsInfo2* pInfo, VkMemoryRequirements2* pMemoryRequirements)
{
	trace_post_vkGetBufferMemoryRequirements2(writer, device, pInfo, pMemoryRequirements);
}

static void trace_post_vkGetImageMemoryRequirements2KHR(lava_file_writer& writer, VkDevice device, const VkImageMemoryRequirementsInfo2* pInfo, VkMemoryRequirements2* pMemoryRequirements)
{
	trace_post_vkGetImageMemoryRequirements2(writer, device, pInfo, pMemoryRequirements);
}

static void trace_post_vkGetDeviceBufferMemoryRequirements(lava_file_writer& writer, VkDevice device, const VkDeviceBufferMemoryRequirements* pInfo, VkMemoryRequirements2* pMemoryRequirements)
{
	inject_dedicated_allocation(writer, (VkBaseOutStructure*)pMemoryRequirements, false);
	extend_bits(&pMemoryRequirements->memoryRequirements);
}

static void trace_post_vkGetDeviceBufferMemoryRequirementsKHR(lava_file_writer& writer, VkDevice device, const VkDeviceBufferMemoryRequirements* pInfo, VkMemoryRequirements2* pMemoryRequirements)
{
	trace_post_vkGetDeviceBufferMemoryRequirements(writer, device, pInfo, pMemoryRequirements);
}

static void trace_post_vkGetDeviceImageMemoryRequirements(lava_file_writer& writer, VkDevice device, const VkDeviceImageMemoryRequirements* pInfo, VkMemoryRequirements2* pMemoryRequirements)
{
	inject_dedicated_allocation(writer, (VkBaseOutStructure*)pMemoryRequirements, true);
	extend_bits(&pMemoryRequirements->memoryRequirements);
}

static void trace_post_vkGetDeviceImageMemoryRequirementsKHR(lava_file_writer& writer, VkDevice device, const VkDeviceImageMemoryRequirements* pInfo, VkMemoryRequirements2* pMemoryRequirements)
{
	trace_post_vkGetDeviceImageMemoryRequirements(writer, device, pInfo, pMemoryRequirements);
}

static void trace_post_vkGetDeviceImageSparseMemoryRequirements(lava_file_writer& writer, VkDevice device, const VkDeviceImageMemoryRequirements* pInfo,
	uint32_t* pSparseMemoryRequirementCount, VkSparseImageMemoryRequirements2* pSparseMemoryRequirements)
{
	for (uint32_t i = 0; i < *pSparseMemoryRequirementCount && pSparseMemoryRequirements; i++)
	{
		// TBD
	}
}

static void trace_post_vkGetDeviceImageSparseMemoryRequirementsKHR(lava_file_writer& writer, VkDevice device, const VkDeviceImageMemoryRequirements* pInfo,
	uint32_t* pSparseMemoryRequirementCount, VkSparseImageMemoryRequirements2* pSparseMemoryRequirements)
{
	trace_post_vkGetDeviceImageSparseMemoryRequirements(writer, device, pInfo, pSparseMemoryRequirementCount, pSparseMemoryRequirements);
}

static void trace_post_vkBindImageMemory(lava_file_writer& writer, VkResult result, VkDevice device, VkImage image, VkDeviceMemory memory, VkDeviceSize memoryOffset)
{
	writer.parent->memory_mutex.lock();
	assert(memory != VK_NULL_HANDLE);
	assert(result == VK_SUCCESS);
	auto* image_data = writer.parent->records.VkImage_index.at(image);
	auto* memory_data = writer.parent->records.VkDeviceMemory_index.at(memory);
	assert(image_data->backing == 0); // cannot re-bind
	image_data->backing = memory;
	image_data->offset = memoryOffset;
	if (image_data->req.size == 0) // no prior call to get reqs for this object
	{
		wrap_vkGetImageMemoryRequirements(device, image, &image_data->req);
	}
	image_data->size = image_data->req.size; // we do not try to second guess this for images
	image_data->accessible = ((image_data->tiling != VK_IMAGE_TILING_OPTIMAL) && (memory_data->propertyFlags & VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT));
	writer.parent->memory_mutex.unlock();
}

static void trace_post_vkBindBufferMemory(lava_file_writer& writer, VkResult result, VkDevice device, VkBuffer buffer, VkDeviceMemory memory, VkDeviceSize memoryOffset)
{
	writer.parent->memory_mutex.lock();
	assert(memory != VK_NULL_HANDLE);
	assert(result == VK_SUCCESS);
	auto* buffer_data = writer.parent->records.VkBuffer_index.at(buffer);
	auto* memory_data = writer.parent->records.VkDeviceMemory_index.at(memory);
	assert(buffer_data->backing == 0); // cannot re-bind
	buffer_data->backing = memory;
	buffer_data->offset = memoryOffset;
	if (buffer_data->req.size == 0) // no prior call to get reqs for this object
	{
		wrap_vkGetBufferMemoryRequirements(device, buffer, &buffer_data->req);
	}
	// we pass in size recorded from vkCreateBuffer, which is the actually used size, rather than required allocation size
	buffer_data->accessible = (memory_data->propertyFlags & VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT);
	writer.parent->memory_mutex.unlock();
}

static void trace_post_vkBindImageMemory2(lava_file_writer& writer, VkResult result, VkDevice device, uint32_t bindInfoCount, const VkBindImageMemoryInfo* pBindInfos)
{
	assert(result == VK_SUCCESS);
	for (unsigned i = 0; i < bindInfoCount; i++)
	{
		trace_post_vkBindImageMemory(writer, result, device, pBindInfos[i].image, pBindInfos[i].memory, pBindInfos[i].memoryOffset);
	}
}

static void trace_post_vkBindImageMemory2KHR(lava_file_writer& writer, VkResult result, VkDevice device, uint32_t bindInfoCount, const VkBindImageMemoryInfo* pBindInfos)
{
	trace_post_vkBindImageMemory2(writer, result, device, bindInfoCount, pBindInfos);
}

static void trace_post_vkBindBufferMemory2(lava_file_writer& writer, VkResult result, VkDevice device, uint32_t bindInfoCount, const VkBindBufferMemoryInfo* pBindInfos)
{
	assert(result == VK_SUCCESS);
	for (unsigned i = 0; i < bindInfoCount; i++)
	{
		trace_post_vkBindBufferMemory(writer, result, device, pBindInfos[i].buffer, pBindInfos[i].memory, pBindInfos[i].memoryOffset);
	}
}

static void trace_post_vkBindBufferMemory2KHR(lava_file_writer& writer, VkResult result, VkDevice device, uint32_t bindInfoCount, const VkBindBufferMemoryInfo* pBindInfos)
{
	trace_post_vkBindBufferMemory2(writer, result, device, bindInfoCount, pBindInfos);
}

static void trace_post_vkCmdBindDescriptorSets(lava_file_writer& writer,
	VkCommandBuffer                             commandBuffer,
	VkPipelineBindPoint                         pipelineBindPoint,
	VkPipelineLayout                            layout,
	uint32_t                                    firstSet,
	uint32_t                                    descriptorSetCount,
	const VkDescriptorSet*                      pDescriptorSets,
	uint32_t                                    dynamicOffsetCount,
	const uint32_t*                             pDynamicOffsets)
{
	uint32_t dynamic_index = 0;
	auto* cmdbuf_data = writer.parent->records.VkCommandBuffer_index.at(commandBuffer);
	for (unsigned i = 0; i < descriptorSetCount; i++)
	{
		const auto* tds = writer.parent->records.VkDescriptorSet_index.at(pDescriptorSets[i]);
		cmdbuf_data->touch_merge(tds->touched);
		for (unsigned j = 0; j < tds->dynamic_buffers.size(); j++)
		{
			if (tds->dynamic_buffers.at(j).buffer == VK_NULL_HANDLE) continue;
			auto* buffer_data = writer.parent->records.VkBuffer_index.at(tds->dynamic_buffers.at(j).buffer);
			VkDeviceSize size = tds->dynamic_buffers.at(j).range;
			VkDeviceSize offset = tds->dynamic_buffers.at(j).offset + pDynamicOffsets[dynamic_index + j];
			if (size == VK_WHOLE_SIZE) size = buffer_data->size - offset;
			cmdbuf_data->touch(buffer_data, offset, size, __LINE__);
		}
		dynamic_index += tds->dynamic_buffers.size();
		assert(dynamic_index <= dynamicOffsetCount);
	}
}

static void handle_VkWriteDescriptorSets(lava_file_writer& writer, uint32_t descriptorWriteCount, const VkWriteDescriptorSet* pDescriptorWrites, bool clear)
{
	for (unsigned i = 0; i < descriptorWriteCount; i++)
	{
		const VkDescriptorType type = pDescriptorWrites[i].descriptorType;
		auto* tds = writer.parent->records.VkDescriptorSet_index.at(pDescriptorWrites[i].dstSet);
		// TBD I do not think this is correct. We are allowed to keep descriptor state for bindings not touched here from a previous call.
		// Not sure how to handle this well, and not seen any content where this breaks anything, but should fix it...
		if (clear) { tds->touched.clear(); tds->dynamic_buffers.clear(); }
	}
	for (unsigned i = 0; i < descriptorWriteCount; i++)
	{
		const VkDescriptorType type = pDescriptorWrites[i].descriptorType;
		auto* tds = writer.parent->records.VkDescriptorSet_index.at(pDescriptorWrites[i].dstSet);

		switch (type)
		{
		case VK_DESCRIPTOR_TYPE_SAMPLER:
		case VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER:
		case VK_DESCRIPTOR_TYPE_SAMPLED_IMAGE:
		case VK_DESCRIPTOR_TYPE_STORAGE_IMAGE:
		case VK_DESCRIPTOR_TYPE_INPUT_ATTACHMENT:
			for (unsigned j = 0; j < pDescriptorWrites[i].descriptorCount; j++)
			{
				if (pDescriptorWrites[i].pImageInfo[j].imageView == VK_NULL_HANDLE) continue;
				auto* imageview_data = writer.parent->records.VkImageView_index.at(pDescriptorWrites[i].pImageInfo[j].imageView);
				auto* image_data = writer.parent->records.VkImage_index.at(imageview_data->image);
				tds->touch(image_data, 0, image_data->size, __LINE__);
			}
			break;
		case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER:
		case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER:
			for (unsigned j = 0; j < pDescriptorWrites[i].descriptorCount; j++)
			{
				if (pDescriptorWrites[i].pBufferInfo[j].buffer == VK_NULL_HANDLE) continue;
				auto* buffer_data = writer.parent->records.VkBuffer_index.at(pDescriptorWrites[i].pBufferInfo[j].buffer);
				VkDeviceSize size = pDescriptorWrites[i].pBufferInfo[j].range;
				if (size == VK_WHOLE_SIZE) size = buffer_data->size - pDescriptorWrites[i].pBufferInfo[j].offset;
				tds->touch(buffer_data, pDescriptorWrites[i].pBufferInfo[j].offset, size, __LINE__);
			}
			break;
		case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC:
		case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER_DYNAMIC:
			for (unsigned j = 0; j < pDescriptorWrites[i].descriptorCount; j++)
			{
				tds->dynamic_buffers.push_back(pDescriptorWrites[i].pBufferInfo[j]);
			}
			break;
		case VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER:
		case VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER:
			for (unsigned j = 0; j < pDescriptorWrites[i].descriptorCount; j++)
			{
				if (pDescriptorWrites[i].pTexelBufferView[j] == VK_NULL_HANDLE) continue;
				auto* bufferview_data = writer.parent->records.VkBufferView_index.at(pDescriptorWrites[i].pTexelBufferView[j]);
				auto* buffer_data = writer.parent->records.VkBuffer_index.at(bufferview_data->buffer);
				tds->touch(buffer_data, bufferview_data->offset, bufferview_data->range, __LINE__);
			}
			break;
		case VK_DESCRIPTOR_TYPE_INLINE_UNIFORM_BLOCK: // Provided by VK_VERSION_1_3
			{
				auto* ptr = (VkWriteDescriptorSetInlineUniformBlock*)find_extension(pDescriptorWrites[i].pNext, VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET_INLINE_UNIFORM_BLOCK);
				assert(ptr);
				assert(ptr->sType == VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET_INLINE_UNIFORM_BLOCK);
				// nothing more to do here
			}
			break;
		case VK_DESCRIPTOR_TYPE_ACCELERATION_STRUCTURE_KHR: // Provided by VK_KHR_acceleration_structure
			{
				auto* ptr = (VkWriteDescriptorSetAccelerationStructureKHR *)find_extension(pDescriptorWrites[i].pNext, VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET_ACCELERATION_STRUCTURE_KHR);
				assert(ptr);
				assert(ptr->sType == VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET_ACCELERATION_STRUCTURE_KHR);
				assert(ptr->accelerationStructureCount == pDescriptorWrites[i].descriptorCount);
			}
			break;
		case VK_DESCRIPTOR_TYPE_MUTABLE_EXT: // Provided by VK_EXT_mutable_descriptor_type
			ABORT("vkUpdateDescriptorSets using VK_EXT_mutable_descriptor_type not yet implemented");
			break;
		case VK_DESCRIPTOR_TYPE_BLOCK_MATCH_IMAGE_QCOM: // Provided by VK_QCOM_image_processing
		case VK_DESCRIPTOR_TYPE_SAMPLE_WEIGHT_IMAGE_QCOM: // Provided by VK_QCOM_image_processing
			ABORT("VK_QCOM_image_processing not supported");
			break;
		case VK_DESCRIPTOR_TYPE_ACCELERATION_STRUCTURE_NV: // Provided by VK_NV_ray_tracing
			ABORT("VK_NV_ray_tracing not supported");
			break;
		case VK_DESCRIPTOR_TYPE_MAX_ENUM:
			ABORT("Bad descriptor type in vkUpdateDescriptorSets");
			break;
		}
	}
}

static void trace_post_vkCmdPushDescriptorSetKHR(lava_file_writer& writer, VkCommandBuffer commandBuffer, VkPipelineBindPoint pipelineBindPoint, VkPipelineLayout layout, uint32_t set, uint32_t descriptorWriteCount, const VkWriteDescriptorSet* pDescriptorWrites)
{
	handle_VkWriteDescriptorSets(writer, descriptorWriteCount, pDescriptorWrites, false);
}

static void trace_post_vkCmdPushDescriptorSet(lava_file_writer& writer, VkCommandBuffer commandBuffer, VkPipelineBindPoint pipelineBindPoint, VkPipelineLayout layout, uint32_t set, uint32_t descriptorWriteCount, const VkWriteDescriptorSet* pDescriptorWrites)
{
	handle_VkWriteDescriptorSets(writer, descriptorWriteCount, pDescriptorWrites, false);
}

static void trace_post_vkUpdateDescriptorSets(lava_file_writer& writer, VkDevice device, uint32_t descriptorWriteCount, const VkWriteDescriptorSet* pDescriptorWrites, uint32_t descriptorCopyCount, const VkCopyDescriptorSet* pDescriptorCopies)
{
	handle_VkWriteDescriptorSets(writer, descriptorWriteCount, pDescriptorWrites, true);

	// TBD handle copy
	assert(descriptorCopyCount == 0);
}

// combine all updates for each memory into one update list for each device memory object, so we keep the number of map operations to a minimum
static void queue_update(lava_file_writer& writer, trackedqueue* t, VkCommandBuffer cmdbuf, std::unordered_map<VkDeviceMemory, range>& ranges_by_memory, std::unordered_set<trackedcmdbuffer_trace*>& cmdbufs)
{
	auto* cmdbuf_data = writer.parent->records.VkCommandBuffer_index.at(cmdbuf);

	// find span of all device memory objects used
	for (const auto& pair : cmdbuf_data->touched)
	{
		VkDeviceMemory memory = pair.first->backing;
		if (memory == VK_NULL_HANDLE) continue;
		auto* memory_data = writer.parent->records.VkDeviceMemory_index.at(memory);
		range minrange = memory_data->exposed.overlap(pair.second, pair.first->offset);
		if (minrange.last == 0) continue; // has not been exposed, don't need to diff
		assert(minrange.first <= minrange.last);
		cmdbufs.insert(cmdbuf_data);
		if (ranges_by_memory.count(memory) == 0) // if we don't have it yet
		{
			assert(minrange.first <= minrange.last);
			ranges_by_memory[memory] = minrange;
			continue;
		}
		range& m = ranges_by_memory.at(memory);
		assert(m.first <= m.last);
		assert(minrange.first <= minrange.last);
		m.first = std::min(m.first, minrange.first);
		m.last = std::max(m.last, minrange.last);
		assert(m.first <= m.last);
	}
}

static void memory_update(lava_file_writer& writer, trackedqueue* queue_data, const std::unordered_map<VkDeviceMemory, range>& ranges_by_memory, std::unordered_set<trackedcmdbuffer_trace*>& cmdbufs)
{
	// for each, map and update
	for (const auto& pair : ranges_by_memory) // devicememory + max used memory span pair
	{
		auto* memory_data = writer.parent->records.VkDeviceMemory_index.at(pair.first);
		assert(memory_data->propertyFlags & VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT); // or how else could it have been mmapped earlier?
		bool restore = false;
		char* ptr = nullptr;
		unsigned long binding_offset = 0; // at which offset the memory map is bound at
		unsigned long binding_size = 0;
		if (memory_data->ptr && memory_data->offset <= pair.second.first && memory_data->size + memory_data->offset >= pair.second.last) // we already have mapped at least this area
		{
			NEVER("Flushing from existing persistent memory %u", memory_data->index);
			writer.debug.flushes_persistent++;
			ptr = memory_data->ptr;
			binding_offset = memory_data->offset;
			binding_size = memory_data->size;
		}
		else
		{
			binding_offset = pair.second.first;
			assert(pair.second.first <= pair.second.last);
			binding_size = pair.second.last - pair.second.first + 1;
			assert(binding_size <= memory_data->allocationSize);
			if (memory_data->ptr) // remove old memory mapping, if any
			{
				NEVER("Remapping existing persistent memory %u to flush it, mapping to offset=%lu size=%lu", memory_data->index, binding_offset, binding_size);
				writer.debug.flushes_remap++;
				wrap_vkUnmapMemory(queue_data->device, memory_data->backing);
				restore = true;
			}
			else NEVER("Remapping previously mapped memory %u to flush it, mapped to offset=%lu size=%lu", memory_data->index, binding_offset, binding_size);
			VkResult result = wrap_vkMapMemory(queue_data->device, memory_data->backing, binding_offset, binding_size, 0, (void**)&ptr);
			assert(result == VK_SUCCESS);
		}

		const auto* device_data = writer.parent->records.VkDevice_index.at(queue_data->device);
		for (auto& cmdbuf_data : cmdbufs)
		{
			for (auto& objpair : cmdbuf_data->touched) // object pointer + touched range pair
			{
				trackedobject* object_data = objpair.first;
				if (object_data->backing != pair.first) continue; // belongs to different device memory
				assert(object_data->frame_destroyed == -1);
				uint64_t written = 0;
				[[maybe_unused]] uint64_t scanned = 0;
				char* cloneptr = memory_data->clone + object_data->offset;
				char* changedptr = ptr + object_data->offset - binding_offset;
				for (const auto& r : objpair.second.list()) // go through list of touched ranges for our object
				{
					range r2 = { r.first + object_data->offset, r.last + object_data->offset };
					assert(r2.last < object_data->offset + object_data->size);
					range v = memory_data->exposed.fetch(r2, memory_data->ptr != nullptr);
					writer.write_uint8_t((uint8_t)(object_data->type == VK_OBJECT_TYPE_IMAGE) ? PACKET_IMAGE_UPDATE : PACKET_BUFFER_UPDATE);
					writer.write_handle(device_data);
					writer.write_handle(object_data);
					written += writer.write_patch(cloneptr, changedptr, v.first - object_data->offset, v.last - v.first + 1);
					scanned += v.last - v.first + 1;
					object_data->updates++;
					NEVER("flushing obj %u (%lu, %lu) -> (%lu, %lu) -> (%lu, %lu), exposed after (%lu, %lu), total written %lu, memory %u; binding_offset=%lu binding_size=%lu ptr=%p",
					      object_data->index, r.first, r.last, r2.first, r2.last, v.first, v.last, memory_data->exposed.span().first, memory_data->exposed.span().last,
					      (unsigned long)object_data->written, memory_data->index, binding_offset, binding_size, memory_data->ptr);
				}
				object_data->written += written;
				NEVER("%s(%u) offset=%u size=%u memory=%u(total size=%u) written=%u scanned=%u cmdbuf=%u source=%d", pretty_print_VkObjectType(object_data->type), (unsigned)object_data->index, (unsigned)object_data->offset, (unsigned)object_data->size, memory_data->index, (unsigned)memory_data->allocationSize, (unsigned)written, (unsigned)scanned, cmdbuf_data->index, (int)object_data->source);
			}
		}

		if (restore || !memory_data->ptr) wrap_vkUnmapMemory(queue_data->device, pair.first);
		if (restore) // restore old memory mapping, if any
		{
			VkResult result = wrap_vkMapMemory(queue_data->device, memory_data->backing, memory_data->offset, memory_data->size, 0, (void**)&memory_data->ptr);
			assert(result == VK_SUCCESS);
		}
	}
}

static void trace_pre_vkQueueSubmit2(VkQueue queue, uint32_t submitCount, const VkSubmitInfo2* pSubmits, VkFence fence)
{
	lava_writer& instance = lava_writer::instance();
	lava_file_writer& writer = instance.file_writer();
	assert(queue != VK_NULL_HANDLE);
	auto* queue_data = instance.records.VkQueue_index.at(queue);
	queue_data->self_test();

	instance.memory_mutex.lock();
	std::unordered_map<VkDeviceMemory, range> ranges_by_memory;
	std::unordered_set<trackedcmdbuffer_trace*> cmdbufs;
	for (unsigned i = 0; i < submitCount; i++)
	{
		for (unsigned j = 0; j < pSubmits[i].commandBufferInfoCount; j++)
		{
			queue_update(writer, queue_data, pSubmits[i].pCommandBufferInfos[j].commandBuffer, ranges_by_memory, cmdbufs);
		}
	}
	memory_update(writer, queue_data, ranges_by_memory, cmdbufs);
	writer.debug.flushes_queue++;
	instance.memory_mutex.unlock();
}

static void trace_pre_vkQueueSubmit2KHR(VkQueue queue, uint32_t submitCount, const VkSubmitInfo2* pSubmits, VkFence fence)
{
	trace_pre_vkQueueSubmit2(queue, submitCount, pSubmits, fence);
}

static void trace_pre_vkQueueSubmit(VkQueue queue, uint32_t submitCount, const VkSubmitInfo* pSubmits, VkFence fence)
{
	lava_writer& instance = lava_writer::instance();
	lava_file_writer& writer = instance.file_writer();
	auto* queue_data = instance.records.VkQueue_index.at(queue);
	queue_data->self_test();

	instance.memory_mutex.lock();
	std::unordered_map<VkDeviceMemory, range> ranges_by_memory;
	std::unordered_set<trackedcmdbuffer_trace*> cmdbufs;
	for (unsigned i = 0; i < submitCount; i++)
	{
		for (unsigned j = 0; j < pSubmits[i].commandBufferCount; j++)
		{
			queue_update(writer, queue_data, pSubmits[i].pCommandBuffers[j], ranges_by_memory, cmdbufs);
		}
	}
	memory_update(writer, queue_data, ranges_by_memory, cmdbufs);
	writer.debug.flushes_queue++;
	instance.memory_mutex.unlock();
}

static void trace_post_vkQueuePresentKHR(lava_file_writer& writer, VkResult result, VkQueue queue, const VkPresentInfoKHR* pPresentInfo)
{
	assert(pPresentInfo->swapchainCount == 1); // more than one not yet supported
	DLOG("Presenting with swapchain image index %u", pPresentInfo->pImageIndices[0]);
	if (result != VK_SUCCESS) ILOG("vkQueuePresentKHR error: %s", errorString(result));
	const auto* queue_data = writer.parent->records.VkQueue_index.at(queue);
	for (unsigned i = 0; i < pPresentInfo->swapchainCount; i++)
	{
		auto* swapchain_data = writer.parent->records.VkSwapchainKHR_index.at(pPresentInfo->pSwapchains[i]);
		swapchain_data->queue = queue_data->realQueue;
	}
	lava_writer& instance = lava_writer::instance();
	instance.new_frame();
}

/// 'instance' is not safe to use here, since it has already been destroyed
void trace_post_vkDestroyInstance(lava_file_writer& writer, VkInstance instance, const VkAllocationCallbacks* pAllocator)
{
	lava_writer& inst = lava_writer::instance();

	if (instance == VK_NULL_HANDLE) return;
	inst.serialize();
	inst.finish();
	frame_mutex.lock();

	// reset protected internal states
	enabled_VK_EXT_tooling_info = false;
	real_memory_properties = {};
	virtual_memory_properties = {};
	memset(remap_memory_types_to_real, 0, sizeof(remap_memory_types_to_real));
	instance_extension_properties.clear();
	device_extension_properties.clear();
	reset_all(&inst.records);

	frame_mutex.unlock();
}

// TBD - nuke any PROTECTED_BIT memory types
static void setup_virtual_memory(VkPhysicalDevice physicalDevice) REQUIRES(frame_mutex)
{
	// Better call this one unconditionally here to avoid a bunch of checks later if
	// it has actually been called before we need the info.
	wrap_vkGetPhysicalDeviceMemoryProperties(physicalDevice, &real_memory_properties);

	// Split annoyingly broad memory types. Also see comment on virtual_memory_properties at the top of this file.
	virtual_memory_properties = real_memory_properties; // struct copy
	assert(real_memory_properties.memoryTypeCount > 0);
	assert(virtual_memory_properties.memoryTypeCount > 0);
	for (uint32_t i = 0; i < real_memory_properties.memoryTypeCount; i++)
	{
		remap_memory_types_to_real[i] = i;
		if (virtual_memory_properties.memoryTypeCount < VK_MAX_MEMORY_TYPES
		    && (real_memory_properties.memoryTypes[i].propertyFlags & VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT)
		    && (real_memory_properties.memoryTypes[i].propertyFlags & VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT))
		{
			// Original becomes device local only, new one becomes host visible only
			virtual_memory_properties.memoryTypes[i].propertyFlags &= ~VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT;
			virtual_memory_properties.memoryTypes[i].propertyFlags &= ~VK_MEMORY_PROPERTY_HOST_COHERENT_BIT;
			virtual_memory_properties.memoryTypes[i].propertyFlags &= ~VK_MEMORY_PROPERTY_HOST_CACHED_BIT;
			uint32_t newmti = virtual_memory_properties.memoryTypeCount;
			virtual_memory_properties.memoryTypeCount++;
			virtual_memory_properties.memoryTypes[newmti] = real_memory_properties.memoryTypes[i]; // struct copy
			virtual_memory_properties.memoryTypes[newmti].propertyFlags &= ~VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT;
			virtual_memory_properties.memoryTypes[newmti].propertyFlags &= ~VK_MEMORY_PROPERTY_LAZILY_ALLOCATED_BIT; // just in case...
			remap_memory_types_to_real[newmti] = i;
			ILOG("Splitting memory type index %u flags=%u into %u (device local flags=%u) and %u (host visible flags=%u)",
			     i, (unsigned)real_memory_properties.memoryTypes[i].propertyFlags, i, (unsigned)virtual_memory_properties.memoryTypes[i].propertyFlags,
			     newmti, (unsigned)virtual_memory_properties.memoryTypes[newmti].propertyFlags);
		}
		else if ((real_memory_properties.memoryTypes[i].propertyFlags & VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT)
                         && (real_memory_properties.memoryTypes[i].propertyFlags & VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT))
		{
			ELOG("Wanted to create virtual memory type, but we used up all available memory types - we may run into issues!");
		}
	}
}

static void trace_post_vkGetPhysicalDeviceMemoryProperties(lava_file_writer& writer, VkPhysicalDevice physicalDevice, VkPhysicalDeviceMemoryProperties* pMemoryProperties)
{
	frame_mutex.lock();
	if (virtual_memory_properties.memoryTypeCount == 0) // called before vkCreateDevice
	{
		setup_virtual_memory(physicalDevice);
	}
	*pMemoryProperties = virtual_memory_properties;
	frame_mutex.unlock();
}

static void trace_post_vkGetPhysicalDeviceMemoryProperties2(lava_file_writer& writer, VkPhysicalDevice physicalDevice, VkPhysicalDeviceMemoryProperties2* pMemoryProperties)
{
	trace_post_vkGetPhysicalDeviceMemoryProperties(writer, physicalDevice, &pMemoryProperties->memoryProperties);
}

static void modify_instance_extensions() REQUIRES(frame_mutex)
{
	lava_writer& instance = lava_writer::instance();
	Json::Value& r = instance.json();
	r["instancePresented"] = Json::objectValue;
	r["instancePresented"]["extensions"] = Json::arrayValue;

	uint32_t propertyCount = 0;
	VkResult result = wrap_vkEnumerateInstanceExtensionProperties(nullptr, &propertyCount, nullptr); // call first to get correct count on host
	if (result != VK_SUCCESS) ABORT("Failed reading instance extension property count");
	std::vector<VkExtensionProperties> tmp_instance_extension_properties(propertyCount);
	result = wrap_vkEnumerateInstanceExtensionProperties(nullptr, &propertyCount, tmp_instance_extension_properties.data());
	assert(result == VK_SUCCESS);

	for (const auto &ext : tmp_instance_extension_properties)
	{
		r["instancePresented"]["extensions"].append(ext.extensionName);
		instance.meta.device.instance_extensions.insert(ext.extensionName);
	}

	// Present extensions we provide
	VkExtensionProperties benchmarking = {};
	strcpy(benchmarking.extensionName, VK_TRACETOOLTEST_BENCHMARKING_EXTENSION_NAME);
	benchmarking.specVersion = 1;
	instance_extension_properties.push_back(benchmarking);

	for (const auto &ext : tmp_instance_extension_properties)
	{
		// Filter out extensions we don't want presented
		std::string name = ext.extensionName;
		if (name.find("_NV") == std::string::npos && name.find("_AMD") == std::string::npos
		    && name.find("_INTEL") == std::string::npos)
		{
			instance_extension_properties.push_back(ext); // add to list of extensions presented to app
		}
	}
}

static void modify_device_extensions(VkPhysicalDevice physicalDevice) REQUIRES(frame_mutex)
{
	lava_writer& instance = lava_writer::instance();
	Json::Value& r = instance.json();
	r["devicePresented"] = Json::objectValue;
	r["devicePresented"]["extensions"] = Json::arrayValue;

	uint32_t propertyCount = 0;
	VkResult result = wrap_vkEnumerateDeviceExtensionProperties(physicalDevice, nullptr, &propertyCount, nullptr); // call first to get correct count on host
	if (result != VK_SUCCESS) ABORT("Failed reading device extension property count");
	std::vector<VkExtensionProperties> tmp_device_extension_properties(propertyCount);
	result = wrap_vkEnumerateDeviceExtensionProperties(physicalDevice, nullptr, &propertyCount, tmp_device_extension_properties.data());
	assert(result == VK_SUCCESS);

	// Present extensions we provide
	VkExtensionProperties checksumvalidation = {};
	strcpy(checksumvalidation.extensionName, VK_TRACETOOLTEST_CHECKSUM_VALIDATION_EXTENSION_NAME);
	checksumvalidation.specVersion = 1;
	device_extension_properties.push_back(checksumvalidation);

	VkExtensionProperties frame_end = {};
	strcpy(frame_end.extensionName, VK_TRACETOOLTEST_FRAME_END_EXTENSION_NAME);
	frame_end.specVersion = 1;
	device_extension_properties.push_back(frame_end);

	VkExtensionProperties objectproperty = {};
	strcpy(objectproperty.extensionName, VK_TRACETOOLTEST_OBJECT_PROPERTY_EXTENSION_NAME);
	objectproperty.specVersion = 1;
	device_extension_properties.push_back(objectproperty);

	VkExtensionProperties toolinfo = {};
	strcpy(toolinfo.extensionName, VK_EXT_TOOLING_INFO_EXTENSION_NAME);
	toolinfo.specVersion = 1;
	device_extension_properties.push_back(toolinfo);

	for (const auto &ext : tmp_device_extension_properties)
	{
		// Filter out extensions we don't want presented
		std::string name = ext.extensionName;
		instance.meta.device.device_extensions.insert(name);
		r["devicePresented"]["extensions"].append(name);
		if (name.find("_NV") == std::string::npos && name.find("_AMD") == std::string::npos
		    && name.find("_INTEL") == std::string::npos && name != VK_EXT_TOOLING_INFO_EXTENSION_NAME)
		{
			device_extension_properties.push_back(ext); // add to list of extensions presented to app
		}
	}
}

static void trace_pre_vkCreateDevice(VkPhysicalDevice physicalDevice, VkDeviceCreateInfo* pCreateInfo, const VkAllocationCallbacks* pAllocator, VkDevice* pDevice)
{
	lava_writer& instance = lava_writer::instance();
	lava_file_writer& writer = instance.file_writer();

	frame_mutex.lock();
	Json::Value& r = instance.json();

	r["deviceRequested"] = Json::objectValue;
	if (device_extension_properties.size() == 0) modify_device_extensions(physicalDevice); // in case empty

	// -- Save information on app request --

	uint32_t families = 0;
	wrap_vkGetPhysicalDeviceQueueFamilyProperties(physicalDevice, &families, nullptr);
	instance.meta.stored_VkQueueFamilyProperties.resize(families);
	wrap_vkGetPhysicalDeviceQueueFamilyProperties(physicalDevice, &families, instance.meta.stored_VkQueueFamilyProperties.data());

	if (pCreateInfo->pEnabledFeatures)
	{
		instance.meta.app.stored_VkPhysicalDeviceFeatures2 = new VkPhysicalDeviceFeatures2();
		instance.meta.app.stored_VkPhysicalDeviceFeatures2->features = *pCreateInfo->pEnabledFeatures; // struct copy
	}
	VkPhysicalDeviceFeatures2KHR* feat2 = (VkPhysicalDeviceFeatures2KHR*)find_extension(pCreateInfo, VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_FEATURES_2);
	if (feat2 && !pCreateInfo->pEnabledFeatures)
	{
		instance.meta.app.stored_VkPhysicalDeviceFeatures2 = new VkPhysicalDeviceFeatures2();
		*instance.meta.app.stored_VkPhysicalDeviceFeatures2 = *feat2; // struct copy
	}
	if (instance.meta.app.stored_VkPhysicalDeviceFeatures2) r["deviceRequested"]["VkPhysicalDeviceFeatures"] = writeVkPhysicalDeviceFeatures2(*instance.meta.app.stored_VkPhysicalDeviceFeatures2);
	VkPhysicalDeviceVulkan11Features* feat11 = (VkPhysicalDeviceVulkan11Features*)find_extension(pCreateInfo, VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_VULKAN_1_1_FEATURES);
	if (feat11)
	{
		instance.meta.app.stored_VkPhysicalDeviceVulkan11Features = new VkPhysicalDeviceVulkan11Features();
		*instance.meta.app.stored_VkPhysicalDeviceVulkan11Features = *feat11; // struct copy
		r["devicePresented"]["VkPhysicalDeviceVulkan11Features"] = writeVkPhysicalDeviceVulkan11Features(*feat11);
	}
	VkPhysicalDeviceVulkan12Features* feat12 = (VkPhysicalDeviceVulkan12Features*)find_extension(pCreateInfo, VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_VULKAN_1_2_FEATURES);
	if (feat12)
	{
		instance.meta.app.stored_VkPhysicalDeviceVulkan12Features = new VkPhysicalDeviceVulkan12Features();
		*instance.meta.app.stored_VkPhysicalDeviceVulkan12Features = *feat12; // struct copy
		r["devicePresented"]["VkPhysicalDeviceVulkan12Features"] = writeVkPhysicalDeviceVulkan12Features(*feat12);
	}
	VkPhysicalDeviceVulkan13Features* feat13 = (VkPhysicalDeviceVulkan13Features*)find_extension(pCreateInfo, VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_VULKAN_1_3_FEATURES);
	if (feat13)
	{
		instance.meta.app.stored_VkPhysicalDeviceVulkan13Features = new VkPhysicalDeviceVulkan13Features();
		*instance.meta.app.stored_VkPhysicalDeviceVulkan13Features = *feat13; // struct copy
		r["devicePresented"]["VkPhysicalDeviceVulkan13Features"] = writeVkPhysicalDeviceVulkan13Features(*feat13);
	}
	r["deviceRequested"]["enabledExtensions"] = Json::arrayValue;
	for (unsigned i = 0; i < pCreateInfo->enabledExtensionCount; i++)
	{
		r["deviceRequested"]["enabledExtensions"].append(pCreateInfo->ppEnabledExtensionNames[i]);
	}

	// -- Modify app request --

	bool has_VK_EXT_tooling_info = false;
	bool has_VK_KHR_external_memory = false;
	bool has_VK_EXT_external_memory_host = false;
	uint32_t propertyCount = 0;
	[[maybe_unused]] VkResult result = wrap_vkEnumerateDeviceExtensionProperties(physicalDevice, nullptr, &propertyCount, nullptr); // call first to get correct count on host
	assert(result == VK_SUCCESS);
	std::vector<VkExtensionProperties> tmp_device_extension_properties(propertyCount);
	result = wrap_vkEnumerateDeviceExtensionProperties(physicalDevice, nullptr, &propertyCount, tmp_device_extension_properties.data());
	assert(result == VK_SUCCESS);
	for (const auto &ext : tmp_device_extension_properties)
	{
		std::string name = ext.extensionName;
		if (name == VK_EXT_TOOLING_INFO_EXTENSION_NAME) has_VK_EXT_tooling_info = true;
		if (name == VK_KHR_EXTERNAL_MEMORY_EXTENSION_NAME) has_VK_KHR_external_memory = true;
		if (name == VK_EXT_EXTERNAL_MEMORY_HOST_EXTENSION_NAME) has_VK_EXT_external_memory_host = true;
	}

	// Extra extensions to add
	std::vector<std::string> extra_exts;
	if (has_VK_KHR_external_memory) { extra_exts.push_back(VK_KHR_EXTERNAL_MEMORY_EXTENSION_NAME); }
	if (has_VK_EXT_external_memory_host) { extra_exts.push_back(VK_EXT_EXTERNAL_MEMORY_HOST_EXTENSION_NAME); }

	// Remove built-in extensions before sending the requested extension list to the driver
	char** nameptrs = writer.pool.allocate<char*>(pCreateInfo->enabledExtensionCount + extra_exts.size()); // reserve space for all
	unsigned newcount = 0;
	DLOG("Requesting enabled device extension from the driver:");
	for (unsigned i = 0; i < pCreateInfo->enabledExtensionCount; i++)
	{
		const int size = strlen(pCreateInfo->ppEnabledExtensionNames[i]) + 1;
		char* name = writer.pool.allocate<char>(size);
		instance.meta.app.device_extensions.insert(pCreateInfo->ppEnabledExtensionNames[i]);
		memset(name, 0, size);
		strcpy(name, pCreateInfo->ppEnabledExtensionNames[i]);
		if (strcmp(name, VK_EXT_TOOLING_INFO_EXTENSION_NAME) == 0)
		{
			if (!has_VK_EXT_tooling_info) continue; // do not pass to host
			enabled_VK_EXT_tooling_info = true;
		}
		if (strcmp(name, VK_TRACETOOLTEST_CHECKSUM_VALIDATION_EXTENSION_NAME) == 0) continue; // do not pass to host
		if (strcmp(name, VK_TRACETOOLTEST_OBJECT_PROPERTY_EXTENSION_NAME) == 0) continue; // do not pass to host
		if (strcmp(name, VK_TRACETOOLTEST_FRAME_END_EXTENSION_NAME) == 0) continue; // do not pass to host
		if (strcmp(name, VK_KHR_EXTERNAL_MEMORY_EXTENSION_NAME) == 0) has_VK_KHR_external_memory = false; // do not need to add, already there
		if (strcmp(name, VK_EXT_EXTERNAL_MEMORY_HOST_EXTENSION_NAME) == 0) has_VK_EXT_external_memory_host = false; // ditto
		nameptrs[newcount++] = name;
		DLOG("    %s", name);
	}
	// Add extra extensions
	for (const auto& v : extra_exts)
	{
		const int size = v.size() + 1;
		char* name = writer.pool.allocate<char>(v.size() + 1);
		strcpy(name, v.c_str());
		nameptrs[newcount++] = name;
		DLOG("    %s (added by us)", name);
	}
	frame_mutex.unlock();
	pCreateInfo->ppEnabledExtensionNames = nameptrs;
	pCreateInfo->enabledExtensionCount = newcount;
}

static void trace_post_vkCreateDevice(lava_file_writer& writer, VkResult result, VkPhysicalDevice physicalDevice, const VkDeviceCreateInfo* pCreateInfo, const VkAllocationCallbacks* pAllocator, VkDevice* pDevice)
{
	lava_writer& instance = lava_writer::instance();

	frame_mutex.lock();
	Json::Value& r = instance.json();

	// -- Save information on tracing device --
#ifdef VK_USE_PLATFORM_ANDROID_KHR
	r["devicePresented"]["android_hw_level"] = android_hw_level(device_features);
#endif
#ifdef __aarch64__
	r["devicePresented"]["architecture"] = "arm64";
#elif __arm__
	r["devicePresented"]["architecture"] = "arm32";
#elif __i386__
	r["devicePresented"]["architecture"] = "intel";
#else
	r["devicePresented"]["architecture"] = "unknown";
#endif

	if (!instance.meta.device.stored_device_properties)
	{
		instance.meta.device.stored_device_properties = new VkPhysicalDeviceProperties();
		if (VK_VERSION_MAJOR(instance.meta.device.stored_api_version) >= 1 && VK_VERSION_MINOR(instance.meta.device.stored_api_version) >= 1)
		{
			VkPhysicalDeviceProperties2 properties { VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_PROPERTIES_2, nullptr };
			if (VK_VERSION_MAJOR(instance.meta.device.stored_api_version) >= 1 && VK_VERSION_MINOR(instance.meta.device.stored_api_version) >= 2)
			{
				instance.meta.device.stored_driver_properties = new VkPhysicalDeviceDriverProperties();
				instance.meta.device.stored_driver_properties->sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_DRIVER_PROPERTIES;
				properties.pNext = instance.meta.device.stored_driver_properties;
			}
			instance.meta.external_memory = { VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_EXTERNAL_MEMORY_HOST_PROPERTIES_EXT, properties.pNext };
			if (p__external_memory) properties.pNext = &instance.meta.external_memory;
			wrap_vkGetPhysicalDeviceProperties2(physicalDevice, &properties);
			*instance.meta.device.stored_device_properties = properties.properties; // struct copy
		}
		else wrap_vkGetPhysicalDeviceProperties(physicalDevice, instance.meta.device.stored_device_properties); // 1.0 version
	}

	r["devicePresented"]["apiVersion"] = version_to_string(instance.meta.device.stored_device_properties->apiVersion);
	r["devicePresented"]["deviceID"] = instance.meta.device.stored_device_properties->deviceID;
	r["devicePresented"]["deviceName"] = instance.meta.device.stored_device_properties->deviceName;
	r["devicePresented"]["vendorID"] = instance.meta.device.stored_device_properties->vendorID;
	r["devicePresented"]["apiVersion"] = version_to_string(instance.meta.device.stored_device_properties->apiVersion);
	r["devicePresented"]["sparseProperties"] = writeVkPhysicalDeviceSparseProperties(instance.meta.device.stored_device_properties->sparseProperties);
	r["devicePresented"]["limits"] = writeVkPhysicalDeviceLimits(instance.meta.device.stored_device_properties->limits);
	if (instance.meta.device.stored_driver_properties)
	{
		r["devicePresented"]["driverName"] = instance.meta.device.stored_driver_properties->driverName;
		r["devicePresented"]["driverInfo"] = instance.meta.device.stored_driver_properties->driverInfo;
	}

	if (!instance.meta.device.stored_VkPhysicalDeviceVulkan11Features && VK_VERSION_MAJOR(instance.meta.device.stored_api_version) >= 1 && VK_VERSION_MINOR(instance.meta.device.stored_api_version) >= 1)
	{
		instance.meta.device.stored_VkPhysicalDeviceFeatures2 = new VkPhysicalDeviceFeatures2();
		instance.meta.device.stored_VkPhysicalDeviceVulkan11Features = new VkPhysicalDeviceVulkan11Features();
		if (VK_VERSION_MINOR(instance.meta.device.stored_api_version) >= 2)
		{
			instance.meta.device.stored_VkPhysicalDeviceVulkan12Features = new VkPhysicalDeviceVulkan12Features();
			instance.meta.device.stored_VkPhysicalDeviceVulkan12Features->sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_VULKAN_1_2_FEATURES;
			instance.meta.device.stored_VkPhysicalDeviceVulkan11Features->pNext = instance.meta.device.stored_VkPhysicalDeviceVulkan12Features;
		}
		if (VK_VERSION_MINOR(instance.meta.device.stored_api_version) >= 3)
		{
			instance.meta.device.stored_VkPhysicalDeviceVulkan13Features = new VkPhysicalDeviceVulkan13Features();
			instance.meta.device.stored_VkPhysicalDeviceVulkan13Features->sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_VULKAN_1_3_FEATURES;
			instance.meta.device.stored_VkPhysicalDeviceVulkan12Features->pNext = instance.meta.device.stored_VkPhysicalDeviceVulkan13Features;
		}
		instance.meta.device.stored_VkPhysicalDeviceVulkan11Features->sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_VULKAN_1_1_FEATURES;
		VkPhysicalDeviceFeatures2 feat2 = { VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_FEATURES_2, instance.meta.device.stored_VkPhysicalDeviceVulkan11Features };
		wrap_vkGetPhysicalDeviceFeatures2(physicalDevice, &feat2);
		*instance.meta.device.stored_VkPhysicalDeviceFeatures2 = feat2; // struct copy
		r["devicePresented"]["VkPhysicalDeviceVulkan11Features"] = writeVkPhysicalDeviceVulkan11Features(*instance.meta.device.stored_VkPhysicalDeviceVulkan11Features);
		if (instance.meta.device.stored_VkPhysicalDeviceVulkan12Features) r["devicePresented"]["VkPhysicalDeviceVulkan12Features"] = writeVkPhysicalDeviceVulkan12Features(*instance.meta.device.stored_VkPhysicalDeviceVulkan12Features);
		if (instance.meta.device.stored_VkPhysicalDeviceVulkan13Features) r["devicePresented"]["VkPhysicalDeviceVulkan13Features"] = writeVkPhysicalDeviceVulkan13Features(*instance.meta.device.stored_VkPhysicalDeviceVulkan13Features);
	}
	else if (!instance.meta.device.stored_VkPhysicalDeviceFeatures2) // 1.0 version
	{
		instance.meta.device.stored_VkPhysicalDeviceFeatures2 = new VkPhysicalDeviceFeatures2();
		wrap_vkGetPhysicalDeviceFeatures(physicalDevice, &instance.meta.device.stored_VkPhysicalDeviceFeatures2->features);
	}
	r["devicePresented"]["VkPhysicalDeviceFeatures"] = writeVkPhysicalDeviceFeatures2(*instance.meta.device.stored_VkPhysicalDeviceFeatures2);

	if (virtual_memory_properties.memoryTypeCount == 0) // might have been called already
	{
		setup_virtual_memory(physicalDevice);
	}

	const VkBenchmarkingTRACETOOLTEST* benchmarking = (const VkBenchmarkingTRACETOOLTEST*)find_extension(pCreateInfo, (VkStructureType)VK_STRUCTURE_TYPE_BENCHMARKING_TRACETOOLTEST);
	if (benchmarking)
	{
		printf("Application enabled VkBenchmarkingTRACETOOLTEST mode:\n");
		printf("\tfixedTimeStep = %u\n", benchmarking->fixedTimeStep);
		printf("\tdisablePerformanceAdaptation = %s\n", benchmarking->disablePerformanceAdaptation ? "true" : "false");
		printf("\tdisableVendorAdaptation = %s\n", benchmarking->disableVendorAdaptation ? "true" : "false");
		printf("\tdisableLoadingFrames = %s\n", benchmarking->disableVendorAdaptation ? "true" : "false");
		printf("\tvisualSettings = %u\n", benchmarking->visualSettings);
		printf("\tscenario = %u\n", benchmarking->scenario);
		printf("\tloopTime = %u\n", benchmarking->loopTime);
		std::string s;
		if (benchmarking->tracingFlags & VK_TRACING_NO_COHERENT_MEMORY_BIT_TRACETOOLTEST) s += "no_coherent_memory ";
		if (benchmarking->tracingFlags & VK_TRACING_NO_SUBALLOCATION_BIT_TRACETOOLTEST) s += "no_suballocation ";
		if (benchmarking->tracingFlags & VK_TRACING_NO_MEMORY_ALIASING_BIT_TRACETOOLTEST) s += "no_memory_aliasing ";
		if (benchmarking->tracingFlags & VK_TRACING_NO_POINTER_OFFSETS_BIT_TRACETOOLTEST) s += "no_pointer_offsets ";
		if (benchmarking->tracingFlags & VK_TRACING_NO_JUST_IN_TIME_REUSE_BIT_TRACETOOLTEST) s += "no_just_in_time_reuse ";
		printf("\ttracingFlags = %s\n", s.c_str());
	}

	frame_mutex.unlock();
}

static void trace_pre_vkCreateInstance(VkInstanceCreateInfo* pCreateInfo, const VkAllocationCallbacks* pAllocator, VkInstance* pInstance)
{
	lava_writer& instance = lava_writer::instance();
	lava_file_writer& writer = instance.file_writer();

	frame_mutex.lock();
	if (instance_extension_properties.size() == 0) modify_instance_extensions(); // in case empty

	// Upgrade Vulkan version used to at least 1.1 to support our injected functionality
	if (pCreateInfo->pApplicationInfo == nullptr)
	{
		VkApplicationInfo* pApplicationInfo = writer.pool.allocate<VkApplicationInfo>(1);
		pApplicationInfo->sType = VK_STRUCTURE_TYPE_APPLICATION_INFO;
		pApplicationInfo->pNext = nullptr;
		pApplicationInfo->pApplicationName = nullptr; // TBD grab from cmd line?
		pApplicationInfo->applicationVersion = 0; // could possibly grab from android app metadata?
		pApplicationInfo->pEngineName = nullptr;
		pApplicationInfo->engineVersion = 0;
		pApplicationInfo->apiVersion = VK_API_VERSION_1_1;
		pCreateInfo->pApplicationInfo = pApplicationInfo;
	}
	else if (pCreateInfo->pApplicationInfo->apiVersion == 0 || pCreateInfo->pApplicationInfo->apiVersion == VK_API_VERSION_1_0)
	{
		VkApplicationInfo* pApplicationInfo = const_cast<VkApplicationInfo*>(pCreateInfo->pApplicationInfo);
		pApplicationInfo->apiVersion = VK_API_VERSION_1_1;
	}

	// -- Save information on app request --

	Json::Value& r = instance.json();
	r["instanceRequested"] = Json::objectValue;
	r["instanceRequested"]["flags"] = Json::arrayValue;
	if (pCreateInfo->flags == VK_INSTANCE_CREATE_ENUMERATE_PORTABILITY_BIT_KHR) r["instanceRequested"]["flags"].append("EnumeratePortabilityKHR");
	r["instanceRequested"]["enabledExtensions"] = Json::arrayValue;
	r["applicationInfo"] = Json::objectValue;
	if (pCreateInfo->pApplicationInfo)
	{
		if (pCreateInfo->pApplicationInfo->pApplicationName) r["applicationInfo"]["applicationName"] = pCreateInfo->pApplicationInfo->pApplicationName;
		r["applicationInfo"]["applicationVersion"] = version_to_string(pCreateInfo->pApplicationInfo->applicationVersion);
		if (pCreateInfo->pApplicationInfo->pEngineName) r["applicationInfo"]["engineName"] = pCreateInfo->pApplicationInfo->pEngineName;
		r["applicationInfo"]["engineVersion"] = version_to_string(pCreateInfo->pApplicationInfo->engineVersion);
		r["applicationInfo"]["apiVersion"] = version_to_string(pCreateInfo->pApplicationInfo->apiVersion);
	}
	for (unsigned i = 0; i < pCreateInfo->enabledExtensionCount; i++)
	{
		r["instanceRequested"]["enabledExtensions"].append(pCreateInfo->ppEnabledExtensionNames[i]);
		instance.meta.app.instance_extensions.insert(pCreateInfo->ppEnabledExtensionNames[i]);
	}

	// Remove built-in extensions before sending the requested extension list to the driver
	char** nameptrs = writer.pool.allocate<char*>(pCreateInfo->enabledExtensionCount); // reserve space for all
	unsigned newcount = 0;
	DLOG("Requesting enabled instance extension from the driver:");
	for (unsigned i = 0; i < pCreateInfo->enabledExtensionCount; i++)
	{
		const int size = strlen(pCreateInfo->ppEnabledExtensionNames[i]) + 1;
		char* name = writer.pool.allocate<char>(size);
		memset(name, 0, size);
		strcpy(name, pCreateInfo->ppEnabledExtensionNames[i]);
		if (strcmp(name, VK_TRACETOOLTEST_BENCHMARKING_EXTENSION_NAME) == 0) continue; // do not pass to host
		nameptrs[newcount++] = name;
		DLOG("    %s", name);
	}
	frame_mutex.unlock();
	pCreateInfo->ppEnabledExtensionNames = nameptrs;
	pCreateInfo->enabledExtensionCount = newcount;
}

static void trace_post_vkCreateInstance(lava_file_writer& writer, VkResult result, const VkInstanceCreateInfo* pCreateInfo, const VkAllocationCallbacks* pAllocator, VkInstance* pInstance)
{
	assert(result == VK_SUCCESS);
	assert(*pInstance != VK_NULL_HANDLE);

	lava_writer& instance = lava_writer::instance();
	std::string base_out_path = "trace";
	if (pCreateInfo->pApplicationInfo && pCreateInfo->pApplicationInfo->pApplicationName)
	{
		base_out_path = pCreateInfo->pApplicationInfo->pApplicationName;
	}
	const std::string trace_out_path = get_trace_path(base_out_path);
	instance.set(trace_out_path);

	frame_mutex.lock();

	if (pCreateInfo->pApplicationInfo)
	{
		instance.meta.device.stored_api_version = pCreateInfo->pApplicationInfo->apiVersion;
	}
	ILOG("Trace out path set to: %s", trace_out_path.c_str());

	// Save physical device references
	uint32_t num_phys_devices = 0;
	result = wrap_vkEnumeratePhysicalDevices(*pInstance, &num_phys_devices, nullptr);
	assert(result == VK_SUCCESS);
	std::vector<VkPhysicalDevice> physical_devices(num_phys_devices);
	result = wrap_vkEnumeratePhysicalDevices(*pInstance, &num_phys_devices, physical_devices.data());
	assert(result == VK_SUCCESS);
	assert(num_phys_devices == physical_devices.size());
	for (unsigned cur_dev = 0; cur_dev < num_phys_devices; cur_dev++)
	{
		auto* add = writer.parent->records.VkPhysicalDevice_index.add(physical_devices[cur_dev], instance.global_frame);
		add->tid = writer.thread_index();
		add->call = writer.local_call_number;
	}

	frame_mutex.unlock();

	writer.pool.reset();
}

static inline lava_file_writer& write_header(const char* funcname, lava_function_id id, bool thread_barrier = false)
{
	lava_writer& instance = lava_writer::instance();
	lava_file_writer& writer = instance.file_writer();
	if (thread_barrier) { frame_mutex.lock(); writer.inject_thread_barrier(); frame_mutex.unlock(); }
	writer.write_api_command(id);
	DLOG("[t%02d %06d] Seq %s%s", writer.thread_index(), writer.local_call_number, funcname, thread_barrier ? " (prefaced by thread barrier)" : "");
	return writer;
}

static VkResult common_vkGetPhysicalDeviceToolProperties(lava_file_writer& writer, VkPhysicalDevice physicalDevice, uint32_t* pToolCount, VkPhysicalDeviceToolPropertiesEXT* pToolProperties)
{
	// -- Declarations --
	assert(physicalDevice != 0);
	const auto* physicaldevice_data = writer.parent->records.VkPhysicalDevice_index.at(physicalDevice);
	writer.write_handle(physicaldevice_data);
	writer.write_uint8_t((pToolProperties) ? 1 : 0);
	// -- Execute --
	// We assume anything passed to us is available to use, and will hide our filled out data to other layers, and assume they will do the same to us.
	if (pToolProperties != nullptr && *pToolCount > 0)
	{
		pToolProperties[0].sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_TOOL_PROPERTIES_EXT;
		pToolProperties[0].pNext = nullptr;
		strcpy(pToolProperties[0].name, "lavatube");
		strcpy(pToolProperties[0].version, "0.0.1"); // TBD read from our defines
		pToolProperties[0].purposes = VK_TOOL_PURPOSE_TRACING_BIT_EXT | VK_TOOL_PURPOSE_MODIFYING_FEATURES_BIT_EXT | VK_TOOL_PURPOSE_ADDITIONAL_FEATURES_BIT_EXT | VK_TOOL_PURPOSE_DEBUG_MARKERS_BIT_EXT;
		strcpy(pToolProperties[0].description, "Tracing layer");
		strcpy(pToolProperties[0].layer, "VK_LAYER_ARM_lavatube");
		pToolProperties = ((*pToolCount > 1) ? &pToolProperties[1] : nullptr); // hide our entry from any other layers being called so they don't overwrite it
		(*pToolCount)--;
	}
	VkResult retval = VK_SUCCESS;
	frame_mutex.lock();
	if (enabled_VK_EXT_tooling_info && wrap_vkGetPhysicalDeviceToolPropertiesEXT) // if supported by host (or other layer)
	{
		retval = wrap_vkGetPhysicalDeviceToolPropertiesEXT(physicalDevice, pToolCount, pToolProperties);
	}
	frame_mutex.unlock();
	(*pToolCount)++;
	// -- Post --
	writer.write_uint32_t(retval);
	// -- Return --
	return retval;
}

VKAPI_ATTR VkResult VKAPI_CALL trace_vkGetPhysicalDeviceToolPropertiesEXT(VkPhysicalDevice physicalDevice, uint32_t* pToolCount, VkPhysicalDeviceToolPropertiesEXT* pToolProperties)
{
	uint32_t physicaldevice_index = 0;
	lava_file_writer& writer = write_header("vkGetPhysicalDeviceToolPropertiesEXT", VKGETPHYSICALDEVICETOOLPROPERTIESEXT);
	return common_vkGetPhysicalDeviceToolProperties(writer, physicalDevice, pToolCount, pToolProperties);
}

VKAPI_ATTR VkResult VKAPI_CALL trace_vkGetPhysicalDeviceToolProperties(VkPhysicalDevice physicalDevice, uint32_t* pToolCount, VkPhysicalDeviceToolPropertiesEXT* pToolProperties)
{
	lava_file_writer& writer = write_header("vkGetPhysicalDeviceToolProperties", VKGETPHYSICALDEVICETOOLPROPERTIES);
	return common_vkGetPhysicalDeviceToolProperties(writer, physicalDevice, pToolCount, pToolProperties);
}

VKAPI_ATTR PFN_vkVoidFunction VKAPI_CALL trace_vkGetDeviceProcAddr(VkDevice device, const char* pName)
{
	lava_writer& tracer = lava_writer::instance();
	lava_file_writer& writer = tracer.file_writer();
	DLOG2("Not tracing vkGetDeviceProcAddr(device, %s)", pName);
	return writer.parent->records.trace_getcall(pName);
}

VKAPI_ATTR PFN_vkVoidFunction VKAPI_CALL trace_vkGetInstanceProcAddr(VkInstance instance, const char* pName)
{
	lava_writer& tracer = lava_writer::instance();
	lava_file_writer& writer = tracer.file_writer();
	DLOG2("Not tracing vkGetInstanceProcAddr(instance, %s)", pName);
	return writer.parent->records.trace_getcall(pName);
}

// "must never call down the chain" and must be accessibly directly through dlsym
VKAPI_ATTR VkResult vkNegotiateLoaderLayerInterfaceVersion(VkNegotiateLayerInterface *pVersionStruct)
{
	DLOG("Negotiating layer version - loader wants version %d\n", (int)pVersionStruct->loaderLayerInterfaceVersion);
	assert(pVersionStruct->sType == LAYER_NEGOTIATE_INTERFACE_STRUCT);
	pVersionStruct->loaderLayerInterfaceVersion = 2;
	pVersionStruct->pfnGetInstanceProcAddr = trace_vkGetInstanceProcAddr;
	pVersionStruct->pfnGetDeviceProcAddr = trace_vkGetDeviceProcAddr;
	pVersionStruct->pfnGetPhysicalDeviceProcAddr = nullptr; // TBD implement this when loader issues are sorted out
	return VK_SUCCESS;
}

static void write_VkBenchmarkingTRACETOOLTEST(lava_file_writer& writer, const VkBenchmarkingTRACETOOLTEST* sptr)
{
	writer.write_uint32_t(sptr->sType);
	assert(sptr->sType == VK_STRUCTURE_TYPE_BENCHMARKING_TRACETOOLTEST);
	write_extension(writer, (VkBaseOutStructure*)sptr->pNext);
	writer.write_uint32_t((uint32_t)sptr->flags);
	writer.write_uint32_t(sptr->fixedTimeStep);
	writer.write_uint32_t(sptr->disablePerformanceAdaptation);
	writer.write_uint32_t(sptr->disableVendorAdaptation);
	writer.write_uint32_t(sptr->disableLoadingFrames);
	writer.write_uint32_t(sptr->visualSettings);
	writer.write_uint32_t(sptr->scenario);
	writer.write_uint32_t(sptr->loopTime);
	writer.write_uint32_t(sptr->tracingFlags);
}

VKAPI_ATTR void VKAPI_CALL trace_vkFrameEndTRACETOOLTEST(VkDevice device)
{
	lava_file_writer& writer = write_header("vkFrameEndTRACETOOLTEST", VKFRAMEENDTRACETOOLTEST);
	writer.write_handle(writer.parent->records.VkDevice_index.at(device));
	writer.parent->new_frame();
}

// This function is _not_ traced.
VKAPI_ATTR uint64_t VKAPI_CALL trace_vkGetDeviceTracingObjectPropertyTRACETOOLTEST(VkDevice device, VkObjectType objectType, uint64_t objectHandle, VkTracingObjectPropertyTRACETOOLTEST valueType)
{
	lava_writer& tracer = lava_writer::instance();
	lava_file_writer& writer = tracer.file_writer();
	uint64_t retval = 0;
	switch (valueType)
	{
	case VK_TRACING_OBJECT_PROPERTY_INDEX_TRACETOOLTEST:
		retval = object_trackable(writer.parent->records, objectType, objectHandle)->index;
		break;
	case VK_TRACING_OBJECT_PROPERTY_MARKED_RANGES_TRACETOOLTEST:
		if (objectType == VK_OBJECT_TYPE_DEVICE_MEMORY)
		{
			writer.parent->records.VkDeviceMemory_index.at((VkDeviceMemory)objectHandle)->self_test();
			retval = writer.parent->records.VkDeviceMemory_index.at((VkDeviceMemory)objectHandle)->exposed.size();
		}
		else if (objectType == VK_OBJECT_TYPE_COMMAND_BUFFER)
		{
			writer.parent->records.VkCommandBuffer_index.at((VkCommandBuffer)objectHandle)->self_test();
			for (auto pair : writer.parent->records.VkCommandBuffer_index.at((VkCommandBuffer)objectHandle)->touched) retval += pair.second.size();
		}
		else assert(false);
		break;
	case VK_TRACING_OBJECT_PROPERTY_MARKED_BYTES_TRACETOOLTEST:
		if (objectType == VK_OBJECT_TYPE_DEVICE_MEMORY)
		{
			writer.parent->records.VkDeviceMemory_index.at((VkDeviceMemory)objectHandle)->self_test();
			retval = writer.parent->records.VkDeviceMemory_index.at((VkDeviceMemory)objectHandle)->exposed.bytes();
		}
		else if (objectType == VK_OBJECT_TYPE_COMMAND_BUFFER)
		{
			writer.parent->records.VkCommandBuffer_index.at((VkCommandBuffer)objectHandle)->self_test();
			for (auto pair : writer.parent->records.VkCommandBuffer_index.at((VkCommandBuffer)objectHandle)->touched) retval += pair.second.bytes();
		}
		else assert(false);
		break;
	case VK_TRACING_OBJECT_PROPERTY_MARKED_OBJECTS_TRACETOOLTEST:
		assert(objectType == VK_OBJECT_TYPE_COMMAND_BUFFER);
		writer.parent->records.VkCommandBuffer_index.at((VkCommandBuffer)objectHandle)->self_test();
		retval = writer.parent->records.VkCommandBuffer_index.at((VkCommandBuffer)objectHandle)->touched.size();
		break;
	case VK_TRACING_OBJECT_PROPERTY_SIZE_TRACETOOLTEST:
		if (objectType == VK_OBJECT_TYPE_DEVICE_MEMORY)
		{
			writer.parent->records.VkDeviceMemory_index.at((VkDeviceMemory)objectHandle)->self_test();
			retval = writer.parent->records.VkDeviceMemory_index.at((VkDeviceMemory)objectHandle)->allocationSize;
		}
		else if (objectType == VK_OBJECT_TYPE_IMAGE)
		{
			writer.parent->records.VkImage_index.at((VkImage)objectHandle)->self_test();
			retval = writer.parent->records.VkImage_index.at((VkImage)objectHandle)->size;
		}
		else if (objectType == VK_OBJECT_TYPE_BUFFER)
		{
			writer.parent->records.VkBuffer_index.at((VkBuffer)objectHandle)->self_test();
			retval = writer.parent->records.VkBuffer_index.at((VkBuffer)objectHandle)->size;
		}
		else assert(false);
		break;
	case VK_TRACING_OBJECT_PROPERTY_UPDATES_COUNT_TRACETOOLTEST:
		if (objectType == VK_OBJECT_TYPE_IMAGE)
		{
			writer.parent->records.VkImage_index.at((VkImage)objectHandle)->self_test();
			retval = writer.parent->records.VkImage_index.at((VkImage)objectHandle)->updates;
		}
		else if (objectType == VK_OBJECT_TYPE_BUFFER)
		{
			writer.parent->records.VkBuffer_index.at((VkBuffer)objectHandle)->self_test();
			retval = writer.parent->records.VkBuffer_index.at((VkBuffer)objectHandle)->updates;
		}
		else if (objectType == VK_OBJECT_TYPE_COMMAND_BUFFER)
		{
			writer.parent->records.VkCommandBuffer_index.at((VkCommandBuffer)objectHandle)->self_test();
			for (auto pair : writer.parent->records.VkCommandBuffer_index.at((VkCommandBuffer)objectHandle)->touched) retval += pair.first->updates;
		} else assert(false);
		break;
	case VK_TRACING_OBJECT_PROPERTY_UPDATES_BYTES_TRACETOOLTEST:
		if (objectType == VK_OBJECT_TYPE_IMAGE)
		{
			retval = writer.parent->records.VkImage_index.at((VkImage)objectHandle)->written;
		}
		else if (objectType == VK_OBJECT_TYPE_BUFFER)
		{
			retval = writer.parent->records.VkBuffer_index.at((VkBuffer)objectHandle)->written;
		}
		else if (objectType == VK_OBJECT_TYPE_COMMAND_BUFFER)
		{
			writer.parent->records.VkCommandBuffer_index.at((VkCommandBuffer)objectHandle)->self_test();
			for (auto pair : writer.parent->records.VkCommandBuffer_index.at((VkCommandBuffer)objectHandle)->touched) retval += pair.first->written;
		} else assert(false);
		break;
	case VK_TRACING_OBJECT_PROPERTY_BACKING_STORE_TRACETOOLTEST:
		if (objectType == VK_OBJECT_TYPE_IMAGE)
		{
			retval = (uint64_t)writer.parent->records.VkImage_index.at((VkImage)objectHandle)->backing;
		}
		else if (objectType == VK_OBJECT_TYPE_BUFFER)
		{
			retval = (uint64_t)writer.parent->records.VkBuffer_index.at((VkBuffer)objectHandle)->backing;
		}
		else if (objectType == VK_OBJECT_TYPE_COMMAND_BUFFER)
		{
			retval = (uint64_t)writer.parent->records.VkCommandBuffer_index.at((VkCommandBuffer)objectHandle)->pool;
		} else assert(false);
		break;
	default:
		assert(false); // unknown valueType
		break;
	}
	return retval;
}

/// The backing memory MUST NOT be already memory mapped
VKAPI_ATTR void VKAPI_CALL trace_vkSyncBufferTRACETOOLTEST(VkDevice device, VkBuffer buffer)
{
	lava_file_writer& writer = lava_writer::instance().file_writer();
	const auto* device_data = writer.parent->records.VkDevice_index.at(device);
	auto* buffer_data = writer.parent->records.VkBuffer_index.at(buffer);
	auto* memory_data = writer.parent->records.VkDeviceMemory_index.at(buffer_data->backing);
	if (memory_data->ptr != nullptr) ABORT("Memory cannot be memory mapped already when running vkSyncBufferTRACETOOLTEST!");
	range v = memory_data->exposed.fetch_os(buffer_data->offset, buffer_data->size, false); // clear exposure
	(void)v;
	char* ptr = nullptr;
	VkResult result = wrap_vkMapMemory(device, memory_data->backing, buffer_data->offset, buffer_data->size, 0, (void**)&ptr);
	assert(result == VK_SUCCESS);
	writer.write_uint8_t((uint8_t)PACKET_BUFFER_UPDATE);
	writer.write_handle(device_data);
	writer.write_handle(buffer_data);
	buffer_data->written += writer.write_patch(memory_data->clone + buffer_data->offset, ptr, 0, buffer_data->size);
	buffer_data->updates++;
	wrap_vkUnmapMemory(device, memory_data->backing);

	(void)write_header("vkSyncBufferTRACETOOLTEST", VKSYNCBUFFERTRACETOOLTEST);
	writer.write_handle(device_data);
	writer.write_handle(buffer_data);
}

VKAPI_ATTR uint32_t VKAPI_CALL trace_vkAssertBufferTRACETOOLTEST(VkDevice device, VkBuffer buffer)
{
	lava_file_writer& writer = write_header("vkAssertBufferTRACETOOLTEST", VKASSERTBUFFERTRACETOOLTEST);
	auto* buffer_data = writer.parent->records.VkBuffer_index.at(buffer);
	auto* memory_data = writer.parent->records.VkDeviceMemory_index.at(buffer_data->backing);
	writer.write_handle(writer.parent->records.VkDevice_index.at(device));
	writer.write_handle(buffer_data);
	uint32_t checksum = 0;
	if (memory_data->ptr && (memory_data->offset <= buffer_data->offset) && (memory_data->offset + memory_data->size > buffer_data->offset + buffer_data->size)) // we already have it mapped
	{
		checksum = adler32((unsigned char*)(memory_data->ptr + buffer_data->offset), buffer_data->size);
		DLOG2("branch1 buffer=%u offset=%u size=%u checksum=%u first byte=%u", buffer_data->index, (unsigned)buffer_data->offset, (unsigned)buffer_data->size, checksum, (unsigned)*(memory_data->ptr + buffer_data->offset));
	}
	else if (memory_data->ptr) // mapped but not including our object
	{
		uint8_t* ptr = nullptr;
		wrap_vkUnmapMemory(device, memory_data->backing);
		VkResult result = wrap_vkMapMemory(device, buffer_data->backing, buffer_data->offset, buffer_data->size, 0, (void**)&ptr);
		if (result != VK_SUCCESS) ABORT("Failed to map memory in vkAssertBuffer");
		checksum = adler32((unsigned char*)ptr, buffer_data->size);
		DLOG2("branch2 buffer=%u offset=%u size=%u checksum=%u first byte=%u", buffer_data->index, (unsigned)buffer_data->offset, (unsigned)buffer_data->size, checksum, (unsigned)ptr[0]);
		wrap_vkUnmapMemory(device, buffer_data->backing);
		result = wrap_vkMapMemory(device, memory_data->backing, memory_data->offset, memory_data->size, 0, (void**)&ptr); // restore back
		assert(result == VK_SUCCESS);
	}
	else // not mapped at all
	{
		uint8_t* ptr = nullptr;
		VkResult result = wrap_vkMapMemory(device, buffer_data->backing, buffer_data->offset, buffer_data->size, 0, (void**)&ptr);
		if (result != VK_SUCCESS) ABORT("Failed to map memory in vkAssertBuffer");
		checksum = adler32((unsigned char*)ptr, buffer_data->size);
		DLOG2("branch3 buffer=%u offset=%u size=%u checksum=%u first byte=%u", buffer_data->index, (unsigned)buffer_data->offset, (unsigned)buffer_data->size, checksum, (unsigned)ptr[0]);
		wrap_vkUnmapMemory(device, buffer_data->backing);
	}
	writer.write_uint32_t(checksum);
	return checksum;
}

void trace_post_vkMapMemory(lava_file_writer& writer, VkResult result, VkDevice device, VkDeviceMemory memory, VkDeviceSize offset, VkDeviceSize size, VkMemoryMapFlags flags, void** ppData)
{
	writer.parent->memory_mutex.lock();
	auto* memory_data = writer.parent->records.VkDeviceMemory_index.at(memory);
	memory_data->ptr = (char*)*ppData;
	memory_data->offset = offset;

	if (size == VK_WHOLE_SIZE || size + offset > memory_data->allocationSize)
	{
		memory_data->size = memory_data->allocationSize - offset;
	}
	else
	{
		memory_data->size = size;
	}

	if (!memory_data->clone)
	{
		memory_data->clone = (char*)calloc(1, memory_data->allocationSize);
	}

	if (memory_data->propertyFlags & VK_MEMORY_PROPERTY_HOST_COHERENT_BIT)
	{
		memory_data->exposed.add_os(offset, memory_data->size);
	}

	writer.parent->memory_mutex.unlock();
}

void trace_post_vkMapMemory2KHR(lava_file_writer& writer, VkResult result, VkDevice device, const VkMemoryMapInfoKHR* pMemoryMapInfo, void** ppData)
{
	trace_post_vkMapMemory(writer, result, device, pMemoryMapInfo->memory, pMemoryMapInfo->offset, pMemoryMapInfo->size, pMemoryMapInfo->flags, ppData);
}

void trace_post_vkFlushMappedMemoryRanges(lava_file_writer& writer, VkResult result, VkDevice device, uint32_t memoryRangeCount, const VkMappedMemoryRange* pMemoryRanges)
{
	writer.parent->memory_mutex.lock();
	assert(result == VK_SUCCESS);
	// The memory must be memory mapped
	for (unsigned i = 0; i < memoryRangeCount; i++)
	{
		VkMappedMemoryRange v = pMemoryRanges[i];
		auto* memory_data = writer.parent->records.VkDeviceMemory_index.at(v.memory);
		VkDeviceSize size = v.size;
		if (v.size == VK_WHOLE_SIZE)
		{
			size = memory_data->allocationSize - v.offset;
		}
		assert(memory_data->ptr != nullptr && memory_data->size != 0); // the memory must be memory mapped
		memory_data->exposed.add_os(v.offset, size);
	}
	writer.parent->memory_mutex.unlock();
}

void trace_pre_vkFreeMemory(VkDevice device, VkDeviceMemory memory, const VkAllocationCallbacks* pAllocator)
{
	lava_writer& instance = lava_writer::instance();
	lava_file_writer& writer = instance.file_writer();

	if (memory != VK_NULL_HANDLE)
	{
		auto* memory_data = writer.parent->records.VkDeviceMemory_index.at(memory);

		// "If a memory object is mapped at the time it is freed, it is implicitly unmapped."
		if (memory_data->ptr && (memory_data->propertyFlags & VK_MEMORY_PROPERTY_HOST_COHERENT_BIT))
		{
			instance.memory_mutex.lock();
			memory_data->ptr = nullptr;
			memory_data->offset = 0;
			memory_data->size = 0;
			instance.memory_mutex.unlock();
		}

		if (memory_data->clone)
		{
			free(memory_data->clone);
			memory_data->clone = nullptr;
		}

		if (p__external_memory && memory_data->extmem)
		{
			free(memory_data->extmem);
			memory_data->extmem = nullptr;
		}

		memory_data->ptr = nullptr;
		memory_data->exposed.clear();
	}
}

void trace_post_vkCreateSwapchainKHR(lava_file_writer& writer, VkResult result, VkDevice device, const VkSwapchainCreateInfoKHR* pCreateInfo, const VkAllocationCallbacks* pAllocator, VkSwapchainKHR* pSwapchain)
{
	assert(result == VK_SUCCESS);
	uint32_t count = 0;
	wrap_vkGetSwapchainImagesKHR(device, *pSwapchain, &count, nullptr);
	if (count != pCreateInfo->minImageCount) ILOG("Requested %u swapchain images, got %u instead", pCreateInfo->minImageCount, count);
	std::vector<VkImage> pSwapchainImages(count);
	wrap_vkGetSwapchainImagesKHR(device, *pSwapchain, &count, pSwapchainImages.data());
	for (unsigned i = 0; i < count; i++)
	{
		auto* add = writer.parent->records.VkImage_index.add(pSwapchainImages[i], lava_writer::instance().global_frame);
		add->sharingMode = pCreateInfo->imageSharingMode;
		add->is_swapchain_image = true;
		add->tiling = VK_IMAGE_TILING_OPTIMAL;
		add->usage = pCreateInfo->imageUsage;
		add->imageType = VK_IMAGE_TYPE_2D;
		add->flags = pCreateInfo->flags;
		add->format = pCreateInfo->imageFormat;
		DLOG("Image index %u is swapchain image %u", add->index, i);
	}
}

VKAPI_ATTR VkResult VKAPI_CALL trace_vkGetSwapchainImagesKHR(VkDevice device, VkSwapchainKHR swapchain, uint32_t* pSwapchainImageCount, VkImage* pSwapchainImages)
{
	lava_file_writer& writer = write_header("vkGetSwapchainImagesKHR", VKGETSWAPCHAINIMAGESKHR);
	writer.write_handle(writer.parent->records.VkDevice_index.at(device));
	writer.write_handle(writer.parent->records.VkSwapchainKHR_index.at(swapchain));
	writer.write_uint8_t(pSwapchainImages ? 1 : 0);
	// Execute
	VkResult retval = wrap_vkGetSwapchainImagesKHR(device, swapchain, pSwapchainImageCount, pSwapchainImages);
	writer.write_uint32_t(retval);
	// Post
	writer.write_uint32_t(*pSwapchainImageCount);
	for (uint32_t i = 0; pSwapchainImages && i < *pSwapchainImageCount; i++)
	{
		writer.write_handle(writer.parent->records.VkImage_index.at(pSwapchainImages[i]));
	}
	// Return
	return retval;
}

VKAPI_ATTR VkResult VKAPI_CALL trace_vkCreateHeadlessSurfaceEXT(VkInstance instance, const VkHeadlessSurfaceCreateInfoEXT* pCreateInfo, const VkAllocationCallbacks* pAllocator, VkSurfaceKHR* pSurface)
{
	lava_file_writer& writer = write_header("vkCreateHeadlessSurfaceEXT", VKCREATEHEADLESSSURFACEEXT);
	writer.write_handle(writer.parent->records.VkInstance_index.at(instance));
	writer.write_uint32_t(pCreateInfo->sType);
	write_extension(writer, (VkBaseOutStructure*)pCreateInfo->pNext);
	writer.write_uint32_t(static_cast<uint32_t>(pCreateInfo->flags));
	writer.write_int32_t(0); // reserved
	writer.write_int32_t(0); // reserved
	writer.write_int32_t(0); // reserved
	writer.write_int32_t(0); // reserved
	writer.write_int32_t(0); // reserved
	writer.write_int32_t(0); // reserved
	// Execute
	PFN_vkCreateHeadlessSurfaceEXT hack_vkCreateHeadlessSurfaceEXT = reinterpret_cast<PFN_vkCreateHeadlessSurfaceEXT>(wrap_vkGetInstanceProcAddr(instance,"vkCreateHeadlessSurfaceEXT"));
	assert(hack_vkCreateHeadlessSurfaceEXT);
	VkResult retval = hack_vkCreateHeadlessSurfaceEXT(instance, pCreateInfo, pAllocator, pSurface);
	writer.write_uint32_t(retval);
	// Post
	const trackable* surface_data = writer.parent->records.VkSurfaceKHR_index.add(*pSurface, lava_writer::instance().global_frame);
	DLOG("insert VkSurfaceKHR into vkCreateHeadlessSurfaceEXT index %u", (unsigned)surface_data->index);
	writer.write_handle(surface_data); // id tracking
	// Return
	return retval;
}

#ifdef VK_USE_PLATFORM_DIRECTFB_EXT

VKAPI_ATTR VkResult VKAPI_CALL vkCreateDirectFBSurfaceEXT(VkInstance instance, const VkDirectFBSurfaceCreateInfoEXT* pCreateInfo, const VkAllocationCallbacks* pAllocator, VkSurfaceKHR* pSurface)
{
	assert(false);
	return VK_SUCCESS;
}

VKAPI_ATTR VkBool32 VKAPI_CALL vkGetPhysicalDeviceDirectFBPresentationSupportEXT(VkPhysicalDevice physicalDevice, uint32_t queueFamilyIndex, IDirectFB* dfb)
{
	assert(false);
	return VK_FALSE;
}

#endif

#ifdef VK_USE_PLATFORM_WIN32_KHR

VKAPI_ATTR VkResult VKAPI_CALL vkCreateWin32SurfaceKHR(VkInstance instance, const VkWin32SurfaceCreateInfoKHR* pCreateInfo, const VkAllocationCallbacks* pAllocator, VkSurfaceKHR* pSurface)
{
	assert(false);
	return VK_SUCCESS;
}

#endif

#ifdef VK_USE_PLATFORM_XLIB_KHR // vkCreateXlibSurfaceKHR

VKAPI_ATTR VkResult VKAPI_CALL trace_vkCreateXlibSurfaceKHR(VkInstance instance, const VkXlibSurfaceCreateInfoKHR* pCreateInfo, const VkAllocationCallbacks* pAllocator, VkSurfaceKHR* pSurface)
{
	// -- Declarations --
	uint8_t pCreateInfo_opt = 0;
	lava_file_writer& writer = write_header("vkCreateXlibSurfaceKHR", VKCREATEXLIBSURFACEKHR);
	writer.write_handle(writer.parent->records.VkInstance_index.at(instance));
	writer.write_uint32_t(pCreateInfo->sType);
	assert(pCreateInfo->sType == VK_STRUCTURE_TYPE_XLIB_SURFACE_CREATE_INFO_KHR);
	write_extension(writer, (VkBaseOutStructure*)pCreateInfo->pNext);
	// Write out xlib info
	writer.write_uint32_t(static_cast<uint32_t>(pCreateInfo->flags));
	XWindowAttributes attr = {};
	Status s = XGetWindowAttributes(pCreateInfo->dpy, pCreateInfo->window, &attr);
	writer.write_uint32_t(attr.x);
	writer.write_uint32_t(attr.y);
	writer.write_uint32_t(attr.width);
	writer.write_uint32_t(attr.height);
	writer.write_uint32_t(attr.border_width);
	writer.write_uint32_t(attr.depth); // bpp
	DLOG2("\twindow found at position (%d, %d) of size (%d, %d)", attr.x, attr.y, attr.width, attr.height);
	// -- Execute --
	VkResult retval = wrap_vkCreateXlibSurfaceKHR(instance, pCreateInfo, pAllocator, pSurface);
	writer.write_uint32_t(retval);
	// -- Post --
	const trackable* surface_data = writer.parent->records.VkSurfaceKHR_index.add(*pSurface, lava_writer::instance().global_frame);
	DLOG("insert VkSurfaceKHR into vkCreateXlibSurfaceKHR index %u", (unsigned)surface_data->index);
	writer.write_handle(surface_data); // id tracking
	// -- Return --
	return retval;
}

#endif // VK_USE_PLATFORM_XLIB_KHR 1

#ifdef VK_USE_PLATFORM_XCB_KHR // vkCreateXcbSurfaceKHR

// see https://xcb.freedesktop.org/windowcontextandmanipulation/
VKAPI_ATTR VkResult VKAPI_CALL trace_vkCreateXcbSurfaceKHR(VkInstance instance, const VkXcbSurfaceCreateInfoKHR* pCreateInfo, const VkAllocationCallbacks* pAllocator, VkSurfaceKHR* pSurface)
{
	// First request window info
	xcb_get_geometry_cookie_t geom_cookie;
	xcb_get_geometry_reply_t *geom_reply;
	xcb_query_tree_cookie_t tree_cookie;
	xcb_query_tree_reply_t *tree_reply;
	xcb_translate_coordinates_cookie_t translate_cookie;
	xcb_translate_coordinates_reply_t *trans_reply;

	geom_cookie = xcb_get_geometry(pCreateInfo->connection, pCreateInfo->window);
	tree_cookie = xcb_query_tree(pCreateInfo->connection, pCreateInfo->window);

	// Declarations
	lava_file_writer& writer = write_header("vkCreateXcbSurfaceKHR", VKCREATEXCBSURFACEKHR);
	writer.write_handle(writer.parent->records.VkInstance_index.at(instance));
	writer.write_uint32_t(pCreateInfo->sType);
	assert(pCreateInfo->sType == VK_STRUCTURE_TYPE_XCB_SURFACE_CREATE_INFO_KHR);
	write_extension(writer, (VkBaseOutStructure*)pCreateInfo->pNext);

	// Get XCB info
	geom_reply = xcb_get_geometry_reply(pCreateInfo->connection, geom_cookie, nullptr);
	assert(geom_reply);
	tree_reply = xcb_query_tree_reply(pCreateInfo->connection, tree_cookie, nullptr);
	assert(tree_reply);
	translate_cookie = xcb_translate_coordinates(pCreateInfo->connection, pCreateInfo->window, tree_reply->parent, geom_reply->x, geom_reply->y);
	trans_reply = xcb_translate_coordinates_reply(pCreateInfo->connection, translate_cookie, nullptr);
	assert(trans_reply);

	// Write out XCB info
	writer.write_uint32_t(static_cast<uint32_t>(pCreateInfo->flags));
	writer.write_int32_t(trans_reply->dst_x);
	writer.write_int32_t(trans_reply->dst_y);
	writer.write_int32_t(geom_reply->width);
	writer.write_int32_t(geom_reply->height);
	writer.write_int32_t(geom_reply->border_width); // just FYI
	writer.write_int32_t(geom_reply->depth); // just FYI
	DLOG("window size written as size=%u,%u pos=%u,%u", (unsigned)geom_reply->width, (unsigned)geom_reply->height, (unsigned)trans_reply->dst_x, (unsigned)trans_reply->dst_y);

	// Execute
	VkResult retval = wrap_vkCreateXcbSurfaceKHR(instance, pCreateInfo, pAllocator, pSurface);
	writer.write_uint32_t(retval);
	assert(retval == VK_SUCCESS);

	// Post
	const auto* surface_data = writer.parent->records.VkSurfaceKHR_index.add(*pSurface, lava_writer::instance().global_frame);
	DLOG("insert VkSurfaceKHR into vkCreateXcbSurfaceKHR index %u", (unsigned)surface_data->index);
	writer.write_handle(surface_data); // id tracking
	free(geom_reply);
	free(tree_reply);
	free(trans_reply);

	// Return
	return retval;
}

#endif // VK_USE_PLATFORM_XCB_KHR

#ifdef VK_USE_PLATFORM_WAYLAND_KHR // vkCreateWaylandSurfaceKHR

VKAPI_ATTR VkResult VKAPI_CALL trace_vkCreateWaylandSurfaceKHR(VkInstance instance, const VkWaylandSurfaceCreateInfoKHR* pCreateInfo, const VkAllocationCallbacks* pAllocator, VkSurfaceKHR* pSurface)
{
	// Declarations
	lava_file_writer& writer = write_header("vkCreateWaylandSurfaceKHR", VKCREATEWAYLANDSURFACEKHR);
	writer.write_handle(writer.parent->records.VkInstance_index.at(instance));
	writer.write_uint32_t(pCreateInfo->sType);
	assert(pCreateInfo->sType == VK_STRUCTURE_TYPE_WAYLAND_SURFACE_CREATE_INFO_KHR);
	write_extension(writer, (VkBaseOutStructure*)pCreateInfo->pNext);
	writer.write_uint32_t(static_cast<uint32_t>(pCreateInfo->flags));
	writer.write_int32_t(0); // x, TBD
	writer.write_int32_t(0); // y, TBD
	writer.write_int32_t(0); // width, TBD
	writer.write_int32_t(0); // height, TBD
	writer.write_int32_t(0); // reserved
	// Execute
	VkResult retval = wrap_vkCreateWaylandSurfaceKHR(instance, pCreateInfo, pAllocator, pSurface);
	writer.write_uint32_t(retval);
	assert(retval == VK_SUCCESS);
	// Post
	const auto* surface_data = writer.parent->records.VkSurfaceKHR_index.add(*pSurface, lava_writer::instance().global_frame);
	writer.write_handle(surface_data); // id tracking
	// Return
	return retval;
}

#endif // VK_USE_PLATFORM_WAYLAND_KHR

VKAPI_ATTR void VKAPI_CALL trace_vkDestroySurfaceKHR(VkInstance instance, VkSurfaceKHR surface, const VkAllocationCallbacks* pAllocator)
{
	// Declarations
	uint8_t surface_opt;
	lava_file_writer& writer = write_header("vkDestroySurfaceKHR", VKDESTROYSURFACEKHR);
	writer.write_handle(writer.parent->records.VkInstance_index.at(instance));
	surface_opt = (surface != 0); // whether we should load this optional value
	writer.write_uint8_t(surface_opt);
	if (surface_opt)
	{
		const auto* surface_data = writer.parent->records.VkSurfaceKHR_index.unset(surface, lava_writer::instance().global_frame);
		writer.write_handle(surface_data);
	}
	// Execute
	wrap_vkDestroySurfaceKHR(instance, surface, pAllocator);
}

/// Map virtual queue to real queue. Since we cannot reverse map from one family to two, the two graphics queues must be on the same
/// family, and we only check the first one.
static void internalGetDeviceQueue(const std::vector<VkQueueFamilyProperties>& props, uint32_t queueIndex, uint32_t& realIndex, uint32_t& realFamily)
{
	realFamily = 0;
	for (const auto& f : props)
	{
		if (f.queueFlags & VK_QUEUE_GRAPHICS_BIT)
		{
			assert(queueIndex <= 1);
			if (queueIndex == 0) realIndex = 0; // real first queue
			else if (queueIndex == 1 && f.queueCount == 1) realIndex = 0; // fake second queue redirecting to first real queue
			else realIndex = 1; // real second queue
			return;
		}
		realFamily++;
	}
}

VKAPI_ATTR void VKAPI_CALL trace_vkGetDeviceQueue2(VkDevice device, const VkDeviceQueueInfo2* pQueueInfo, VkQueue* pQueue)
{
	lava_file_writer& writer = write_header("vkGetDeviceQueue2", VKGETDEVICEQUEUE2);
	writer.write_handle(writer.parent->records.VkDevice_index.at(device));
	trackeddevice* device_data = writer.parent->records.VkDevice_index.at(device);
	write_VkDeviceQueueInfo2(writer, pQueueInfo);
	uint32_t realIndex = pQueueInfo->queueIndex;
	uint32_t realFamily = pQueueInfo->queueFamilyIndex;

	if (p__virtualqueues == 0)
	{
		wrap_vkGetDeviceQueue2(device, pQueueInfo, pQueue);
	}
	else // remap
	{
		assert(pQueueInfo->queueIndex < 2);
		assert(pQueueInfo->queueFamilyIndex == 0);
		internalGetDeviceQueue(writer.parent->meta.stored_VkQueueFamilyProperties, pQueueInfo->queueIndex, realIndex, realFamily);
		VkDeviceQueueInfo2 info = *pQueueInfo;
		info.queueFamilyIndex = realFamily;
		info.queueIndex = realIndex;
		wrap_vkGetDeviceQueue2(device, &info, pQueue);
	}

	if (!writer.parent->records.VkQueue_index.contains(*pQueue))
	{
		auto* queue_data = writer.parent->records.VkQueue_index.add(*pQueue, lava_writer::instance().global_frame);
		queue_data->queueIndex = pQueueInfo->queueIndex;
		queue_data->queueFamily = pQueueInfo->queueFamilyIndex;
		queue_data->queueFlags = pQueueInfo->flags;
		queue_data->device = device;
		queue_data->realIndex = realIndex;
		queue_data->realFamily = realFamily;
		queue_data->realQueue = *pQueue;
		queue_data->tid = writer.thread_index();
		queue_data->call = writer.local_call_number;
		queue_data->physicalDevice = writer.parent->records.VkDevice_index.at(device)->physicalDevice;
	}
	auto* queue_data = writer.parent->records.VkQueue_index.at(*pQueue);
	assert(queue_data);
	if (p__virtualqueues != 0) *pQueue = (VkQueue)queue_data;
	queue_data->self_test();
	writer.write_handle(queue_data);
}

VKAPI_ATTR void VKAPI_CALL trace_vkGetDeviceQueue(VkDevice device, uint32_t queueFamilyIndex, uint32_t queueIndex, VkQueue* pQueue)
{
	lava_file_writer& writer = write_header("vkGetDeviceQueue", VKGETDEVICEQUEUE);
	auto* device_data = writer.parent->records.VkDevice_index.at(device);
	const bool virtual_family = (writer.parent->meta.stored_VkQueueFamilyProperties.at(queueFamilyIndex).queueFlags & VK_QUEUE_GRAPHICS_BIT) && p__virtualqueues;

	writer.write_handle(device_data);
	writer.write_uint32_t(virtual_family ? LAVATUBE_VIRTUAL_QUEUE : queueFamilyIndex);
	writer.write_uint32_t(queueIndex);

	uint32_t realIndex = queueIndex;
	uint32_t realFamily = queueFamilyIndex;

	// Execute
	if (p__virtualqueues == 0)
	{
		wrap_vkGetDeviceQueue(device, queueFamilyIndex, queueIndex, pQueue);
	}
	else // remap
	{
		assert(queueIndex < 2);
		assert(queueFamilyIndex == 0);
		internalGetDeviceQueue(writer.parent->meta.stored_VkQueueFamilyProperties, queueIndex, realIndex, realFamily);
		wrap_vkGetDeviceQueue(device, realFamily, realIndex, pQueue);
	}

	// Post
	if (!writer.parent->records.VkQueue_index.contains(*pQueue))
	{
		auto* queue_data = writer.parent->records.VkQueue_index.add(*pQueue, lava_writer::instance().global_frame);
		queue_data->queueIndex = queueIndex;
		queue_data->queueFamily = queueFamilyIndex;
		queue_data->queueFlags = writer.parent->meta.stored_VkQueueFamilyProperties[queueFamilyIndex].queueFlags;
		queue_data->device = device;
		queue_data->realIndex = realIndex;
		queue_data->realFamily = realFamily;
		queue_data->realQueue = *pQueue;
		queue_data->tid = writer.thread_index();
		queue_data->call = writer.local_call_number;
		queue_data->physicalDevice = writer.parent->records.VkDevice_index.at(device)->physicalDevice;
	}
	auto* queue_data = writer.parent->records.VkQueue_index.at(*pQueue);
	assert(queue_data);
	if (p__virtualqueues != 0) *pQueue = (VkQueue)queue_data;
	queue_data->self_test();
	writer.write_handle(queue_data);
}

#ifdef VK_USE_PLATFORM_ANDROID_KHR // vkCreateAndroidSurfaceKHR

#include "android_utils.h"

VKAPI_ATTR VkResult VKAPI_CALL trace_vkCreateAndroidSurfaceKHR(VkInstance instance, const VkAndroidSurfaceCreateInfoKHR* pCreateInfo, const VkAllocationCallbacks* pAllocator, VkSurfaceKHR* pSurface)
{
	// Declarations
	lava_file_writer& writer = write_header("vkCreateAndroidSurfaceKHR", VKCREATEANDROIDSURFACEKHR);
	writer.write_handle(writer.parent->records.VkInstance_index.at(instance));
	writer.write_uint32_t(pCreateInfo->sType);
	assert(pCreateInfo->sType == VK_STRUCTURE_TYPE_ANDROID_SURFACE_CREATE_INFO_KHR);
	write_extension(writer, (VkBaseOutStructure*)pCreateInfo->pNext);
	writer.write_uint32_t(static_cast<uint32_t>(pCreateInfo->flags));
	writer.write_int32_t(0); // x position - TBD
	writer.write_int32_t(0); // y position - TBD
	writer.write_int32_t(ANativeWindow_getWidth(pCreateInfo->window));
	writer.write_int32_t(ANativeWindow_getHeight(pCreateInfo->window));
	writer.write_int32_t(0);
	writer.write_int32_t(ANativeWindow_getFormat(pCreateInfo->window));
	// Execute
	VkResult retval = wrap_vkCreateAndroidSurfaceKHR(instance, pCreateInfo, pAllocator, pSurface);
	// Post
	const auto* surface_data = writer.parent->records.VkSurfaceKHR_index.add(*pSurface, lava_writer::instance().global_frame);
	writer.write_handle(surface_data); // id tracking
	// Return
	return retval;
}

VKAPI_ATTR uint32_t VKAPI_CALL bytes_per_pixel(uint32_t format)
{
	switch (format) {
		case AHARDWAREBUFFER_FORMAT_BLOB:
		case AHARDWAREBUFFER_FORMAT_S8_UINT:
			return 1;
		case AHARDWAREBUFFER_FORMAT_R5G6B5_UNORM:
		case AHARDWAREBUFFER_FORMAT_D16_UNORM:
			return 2;
		case AHARDWAREBUFFER_FORMAT_R8G8B8_UNORM:
		case AHARDWAREBUFFER_FORMAT_D24_UNORM:
			return 3;
		case AHARDWAREBUFFER_FORMAT_R8G8B8A8_UNORM:
		case AHARDWAREBUFFER_FORMAT_R8G8B8X8_UNORM:
		case AHARDWAREBUFFER_FORMAT_D24_UNORM_S8_UINT:
		case AHARDWAREBUFFER_FORMAT_R10G10B10A2_UNORM:
		case AHARDWAREBUFFER_FORMAT_D32_FLOAT:
			return 4;
		case AHARDWAREBUFFER_FORMAT_D32_FLOAT_S8_UINT:
			return 5;
		case AHARDWAREBUFFER_FORMAT_R16G16B16A16_FLOAT:
			return 8;
	}

	return 0;
}

VKAPI_ATTR void VKAPI_CALL save_hw_buffer(const AHardwareBuffer* buffer)
{
	lava_file_writer& writer = lava_writer::instance().file_writer();
	AHardwareBuffer_Desc hw_buffer_description;
	AHardwareBuffer_describe(buffer, &hw_buffer_description);

	uint32_t bpp = bytes_per_pixel(hw_buffer_description.format);

	writer.write_uint32_t(hw_buffer_description.width);
	writer.write_uint32_t(hw_buffer_description.height);
	writer.write_uint32_t(hw_buffer_description.layers);
	writer.write_uint32_t(hw_buffer_description.format);
	writer.write_uint64_t(hw_buffer_description.usage);
	writer.write_uint32_t(hw_buffer_description.stride);
	writer.write_uint32_t(hw_buffer_description.rfu0);
	writer.write_uint64_t(hw_buffer_description.rfu1);
	writer.write_uint32_t(bpp);
}

VKAPI_ATTR VkResult VKAPI_CALL trace_vkGetAndroidHardwareBufferPropertiesANDROID(VkDevice device, const AHardwareBuffer* buffer, VkAndroidHardwareBufferPropertiesANDROID* pProperties)
{
	// Declarations
	lava_file_writer& writer = write_header("vkGetAndroidHardwareBufferPropertiesANDROID", VKGETANDROIDHARDWAREBUFFERPROPERTIESANDROID);
	writer.write_handle(writer.parent->records.VkDevice_index.at(device));

	// Store metadata for debug purposes
	save_hw_buffer(buffer);

	// Execute
	VkResult retval = wrap_vkGetAndroidHardwareBufferPropertiesANDROID(device, buffer, pProperties);
	writer.write_uint32_t(retval);
	// Post
	writer.write_uint32_t(pProperties->sType);
	assert(pProperties->sType == VK_STRUCTURE_TYPE_ANDROID_HARDWARE_BUFFER_PROPERTIES_ANDROID);
	write_extension(writer, (VkBaseOutStructure*)pProperties->pNext);
	writer.write_uint64_t(pProperties->allocationSize);
	writer.write_uint32_t(pProperties->memoryTypeBits);
	// Return
	return retval;
}

VKAPI_ATTR VkResult VKAPI_CALL trace_vkGetMemoryAndroidHardwareBufferANDROID(VkDevice device, const VkMemoryGetAndroidHardwareBufferInfoANDROID* pInfo, struct AHardwareBuffer** pBuffer)
{
	// Declarations
	const dummy_ext* ext_tmp_VVkMemoryGetAndroidHardwareBufferInfoANDROID = nullptr;
	lava_file_writer& writer = write_header("vkGetMemoryAndroidHardwareBufferANDROID", VKGETMEMORYANDROIDHARDWAREBUFFERANDROID);
	writer.write_handle(writer.parent->records.VkDevice_index.at(device));
	writer.write_uint32_t(pInfo->sType);
	assert(pInfo->sType == VK_STRUCTURE_TYPE_MEMORY_GET_ANDROID_HARDWARE_BUFFER_INFO_ANDROID);
	write_extension(writer, (VkBaseOutStructure*)pInfo->pNext);
	writer.write_handle(writer.parent->records.VkDeviceMemory_index.at(pInfo->memory));
	// Execute
	VkResult retval = wrap_vkGetMemoryAndroidHardwareBufferANDROID(device, pInfo, pBuffer);
	writer.write_uint32_t(retval);
	// Post
	// Store metadata for debug purposes
	save_hw_buffer(*pBuffer);
	return retval;
}

#endif // VK_USE_PLATFORM_ANDROID_KHR

VKAPI_ATTR VkResult VKAPI_CALL trace_vkEnumerateInstanceLayerProperties(uint32_t* pPropertyCount, VkLayerProperties* pProperties)
{
	lava_file_writer& writer = write_header("vkEnumerateInstanceLayerProperties", VKENUMERATEINSTANCELAYERPROPERTIES);
	writer.write_uint8_t(pProperties ? 1 : 0);
	// Execute
#ifdef COMPILE_LAYER
	VkResult retval = VK_SUCCESS;

	if (pProperties == nullptr) {
		*pPropertyCount = 1;
	} else {
		if ((pPropertyCount != nullptr) && (*pPropertyCount >= 1)) {
			memcpy(pProperties, &LAYER_PROPERTIES, sizeof(LAYER_PROPERTIES));
			*pPropertyCount = 1;
		} else {
			retval = VK_INCOMPLETE;
		}
	}
#else
	VkResult retval = wrap_vkEnumerateInstanceLayerProperties(pPropertyCount, pProperties);
#endif

	writer.write_uint32_t(retval);
	// Post
	// Return
	return retval;
}

VKAPI_ATTR VkResult VKAPI_CALL trace_vkEnumerateInstanceExtensionProperties(const char* pLayerName, uint32_t* pPropertyCount, VkExtensionProperties* pProperties)
{
	lava_file_writer& writer = write_header("vkEnumerateInstanceExtensionProperties", VKENUMERATEINSTANCEEXTENSIONPROPERTIES);
	writer.write_string(pLayerName);
	writer.write_uint8_t(pProperties ? 1 : 0);
	// Execute
#ifdef COMPILE_LAYER
	if ((pLayerName != nullptr) && (strcmp(pLayerName, LAYER_PROPERTIES.layerName) == 0))
	{
		*pPropertyCount = 0;
		writer.write_uint32_t(VK_SUCCESS);
		return VK_SUCCESS;
	}
#endif
	frame_mutex.lock();
	if (instance_extension_properties.size() == 0)
	{
		modify_instance_extensions(); // add our own extensions
	}
	VkResult retval = VK_SUCCESS;
	if (pProperties == nullptr)
	{
		*pPropertyCount = instance_extension_properties.size();
	}
	else
	{
		for (unsigned i = 0; i < std::min<unsigned>(instance_extension_properties.size(), *pPropertyCount); i++)
		{
			memcpy(&pProperties[i], &instance_extension_properties[i], sizeof(VkExtensionProperties));
		}
		if (instance_extension_properties.size() > *pPropertyCount) retval = VK_INCOMPLETE;
	}
	frame_mutex.unlock();
	writer.write_uint32_t(retval);
	return retval;
}

VKAPI_ATTR VkResult VKAPI_CALL trace_vkEnumerateDeviceLayerProperties(VkPhysicalDevice physicalDevice, uint32_t* pPropertyCount, VkLayerProperties* pProperties)
{
	lava_file_writer& writer = write_header("vkEnumerateDeviceLayerProperties", VKENUMERATEDEVICELAYERPROPERTIES);
	uint8_t initialized = static_cast<uint8_t>(writer.parent->records.VkPhysicalDevice_index.contains(physicalDevice));
	writer.write_uint8_t(initialized);
	if (initialized)
	{
		writer.write_handle(writer.parent->records.VkPhysicalDevice_index.at(physicalDevice));
	}
	writer.write_uint8_t(pProperties ? 1 : 0);

	// Execute
#ifdef COMPILE_LAYER
	VkResult retval = VK_SUCCESS;
	if (pProperties == nullptr)
	{
		*pPropertyCount = 1;
	} else {
		if ((pPropertyCount != nullptr) && (*pPropertyCount >= 1)) {
			memcpy(pProperties, &LAYER_PROPERTIES, sizeof(LAYER_PROPERTIES));
			*pPropertyCount = 1;
		} else {
			retval = VK_INCOMPLETE;
		}
	}
#else
	VkResult retval = wrap_vkEnumerateDeviceLayerProperties(physicalDevice, pPropertyCount, pProperties);
#endif
	writer.write_uint32_t(retval);
	return retval;
}

VKAPI_ATTR VkResult VKAPI_CALL trace_vkEnumerateDeviceExtensionProperties(VkPhysicalDevice physicalDevice, const char* pLayerName, uint32_t* pPropertyCount, VkExtensionProperties* pProperties)
{
	lava_file_writer& writer = write_header("vkEnumerateDeviceExtensionProperties", VKENUMERATEDEVICEEXTENSIONPROPERTIES);
	uint8_t initialized = static_cast<uint8_t>(writer.parent->records.VkPhysicalDevice_index.contains(physicalDevice));
	writer.write_uint8_t(initialized);
	if (initialized)
	{
		writer.write_handle(writer.parent->records.VkPhysicalDevice_index.at(physicalDevice));
	}

	writer.write_string(pLayerName);
	writer.write_uint8_t(pProperties ? 1 : 0);
	// Execute
#ifdef COMPILE_LAYER
	if ((pLayerName != nullptr) && (strcmp(pLayerName, LAYER_PROPERTIES.layerName) == 0))
	{
		*pPropertyCount = 0;
		writer.write_uint32_t(VK_SUCCESS);
		return VK_SUCCESS;
	}
#endif
	frame_mutex.lock();
	if (device_extension_properties.size() == 0)
	{
		modify_device_extensions(physicalDevice);
	}
	VkResult retval = VK_SUCCESS;
	if (pProperties == nullptr)
	{
		*pPropertyCount = device_extension_properties.size();
	}
	else
	{
		for (unsigned i = 0; i < std::min<unsigned>(device_extension_properties.size(), *pPropertyCount); i++)
		{
			memcpy(&pProperties[i], &device_extension_properties[i], sizeof(VkExtensionProperties));
		}
		if (device_extension_properties.size() > *pPropertyCount) retval = VK_INCOMPLETE;
	}
	frame_mutex.unlock();
	writer.write_uint32_t(retval);
	return VK_SUCCESS;
}

// Following RenderDoc we enforce a randomly generated UUID each run, which should force the app to discard any stored shader cache data.
void trace_post_vkGetPipelineCacheData(lava_file_writer& writer, VkResult result, VkDevice device, VkPipelineCache pipelineCache, size_t* pDataSize, void* pData)
{
	lava_writer& instance = lava_writer::instance();

	if (pData && *pDataSize != 0)
	{
		uint32_t *ptr = (uint32_t *)pData;
		assert(ptr[1] == VK_PIPELINE_CACHE_HEADER_VERSION_ONE);
		memcpy(ptr + 4, instance.fakeUUID, VK_UUID_SIZE);
	}
}

#ifdef VK_USE_PLATFORM_XLIB_KHR // vkCreateXlibSurfaceKHR

VKAPI_ATTR VkBool32 VKAPI_CALL trace_vkGetPhysicalDeviceXlibPresentationSupportKHR(VkPhysicalDevice physicalDevice, uint32_t queueFamilyIndex, Display* dpy, VisualID visualID)
{
	lava_file_writer& writer = write_header("vkGetPhysicalDeviceXlibPresentationSupportKHR", VKGETPHYSICALDEVICEXLIBPRESENTATIONSUPPORTKHR);
	const bool virtual_family = (writer.parent->meta.stored_VkQueueFamilyProperties.at(queueFamilyIndex).queueFlags & VK_QUEUE_GRAPHICS_BIT) && p__virtualqueues;
	writer.write_handle(writer.parent->records.VkPhysicalDevice_index.at(physicalDevice));
	writer.write_uint32_t(virtual_family ? LAVATUBE_VIRTUAL_QUEUE : queueFamilyIndex);
	VkBool32 retval = wrap_vkGetPhysicalDeviceXlibPresentationSupportKHR(physicalDevice, queueFamilyIndex, dpy, visualID);
	writer.write_uint32_t(retval);
	return retval;
}

#endif

VKAPI_ATTR void VKAPI_CALL trace_vkGetPhysicalDeviceQueueFamilyProperties(VkPhysicalDevice physicalDevice, uint32_t* pQueueFamilyPropertyCount, VkQueueFamilyProperties* pQueueFamilyProperties)
{
	lava_file_writer& writer = write_header("vkGetPhysicalDeviceQueueFamilyProperties", VKGETPHYSICALDEVICEQUEUEFAMILYPROPERTIES);
	trackable* physicaldevice_data = writer.parent->records.VkPhysicalDevice_index.at(physicalDevice);
	writer.write_handle(physicaldevice_data);
	writer.write_uint8_t((pQueueFamilyProperties) ? 1 : 0);
	if (p__virtualqueues == 0)
	{
		wrap_vkGetPhysicalDeviceQueueFamilyProperties(physicalDevice, pQueueFamilyPropertyCount, pQueueFamilyProperties);
	}
	else // virtual queues
	{
		if (pQueueFamilyProperties == nullptr)
		{
			*pQueueFamilyPropertyCount = 1;
		}
		else // fill the first and only passed in property struct
		{
			uint32_t timestampValidBits = 0;
			uint32_t sparseBits = 0;
			uint32_t count = 0;
			wrap_vkGetPhysicalDeviceQueueFamilyProperties(physicalDevice, &count, nullptr);
			std::vector<VkQueueFamilyProperties> props(count);
			wrap_vkGetPhysicalDeviceQueueFamilyProperties(physicalDevice, &count, props.data());
			for (const auto& f : props)
			{
				if (f.queueFlags & VK_QUEUE_GRAPHICS_BIT)
				{
					assert(f.minImageTransferGranularity.width == 1 && f.minImageTransferGranularity.height == 1 && f.minImageTransferGranularity.depth == 1);
					if ((f.queueFlags & VK_QUEUE_SPARSE_BINDING_BIT)) sparseBits = VK_QUEUE_SPARSE_BINDING_BIT;
					timestampValidBits = f.timestampValidBits;
					break; // both our virtual graphics queues are on the first queue family supporting graphics
				}
			}

			pQueueFamilyProperties->queueFlags = VK_QUEUE_GRAPHICS_BIT | VK_QUEUE_COMPUTE_BIT | VK_QUEUE_TRANSFER_BIT | sparseBits;
			pQueueFamilyProperties->queueCount = 2;
			pQueueFamilyProperties->timestampValidBits = timestampValidBits;
			pQueueFamilyProperties->minImageTransferGranularity.width = 1;
			pQueueFamilyProperties->minImageTransferGranularity.height = 1;
			pQueueFamilyProperties->minImageTransferGranularity.depth = 1;
		}
	}
}

// --- JSON helpers ---
// only write out static info that does not change for the duration of the trace

static Json::Value trackable_json(const trackable* t)
{
	Json::Value v;
	v["frame_created"] = t->frame_created;
	v["frame_destroyed"] = t->frame_destroyed;
	if (!t->name.empty()) v["name"] = t->name;
	return v;
}

static Json::Value trackedbuffer_json(const trackedbuffer* t)
{
	Json::Value v = trackable_json(t);
	v["size"] = (Json::Value::UInt64)t->size;
	v["flags"] = (unsigned)t->flags;
	v["sharingMode"] = (unsigned)t->sharingMode;
	v["usage"] = (unsigned)t->usage;
	v["req_size"] = (Json::Value::UInt64)t->req.size;
	v["req_alignment"] = (unsigned)t->req.alignment;
	v["written"] = (Json::Value::UInt64)t->written;
	v["updates"] = (unsigned)t->updates;
	return v;
}

static Json::Value trackedimage_json(const trackedimage* t)
{
	Json::Value v = trackable_json(t);
	v["tiling"] = (unsigned)t->tiling;
	v["flags"] = (unsigned)t->flags;
	v["sharingMode"] = (unsigned)t->sharingMode;
	v["usage"] = (unsigned)t->usage;
	v["imageType"] = (unsigned)t->imageType;
	v["req_size"] = (Json::Value::UInt64)t->req.size;
	v["req_alignment"] = (unsigned)t->req.alignment;
	v["format"] = (unsigned)t->format;
	v["written"] = (Json::Value::UInt64)t->written;
	v["updates"] = (unsigned)t->updates;
	Json::Value arr = Json::arrayValue;
	arr.append((unsigned)t->extent.width);
	arr.append((unsigned)t->extent.height);
	arr.append((unsigned)t->extent.depth);
	v["extent"] = arr;
	v["initalLayout"] = (unsigned)t->initialLayout;
	v["samples"] = (unsigned)t->samples;
	v["mipLevels"] = (unsigned)t->mipLevels;
	v["arrayLayers"] = (unsigned)t->arrayLayers;
	return v;
}

static Json::Value trackedswapchain_trace_json(const trackedswapchain_trace* t)
{
	Json::Value v = trackable_json(t);
	v["imageFormat"] = (unsigned)t->info.imageFormat;
	v["imageUsage"] = (unsigned)t->info.imageUsage;
	v["imageSharingMode"] = (unsigned)t->info.imageSharingMode;
	v["width"] = (unsigned)t->info.imageExtent.width;
	v["height"] = (unsigned)t->info.imageExtent.height;
	return v;
}

static Json::Value trackedcmdbuffer_trace_json(const trackedcmdbuffer_trace* t)
{
	Json::Value v = trackable_json(t);
	v["pool"] = (unsigned)t->pool_index;
	return v;
}

static Json::Value trackedimageview_json(const trackedimageview* t)
{
	Json::Value v = trackable_json(t);
	v["image"] = (unsigned)t->image_index;
	return v;
}

static Json::Value trackedbufferview_json(const trackedbufferview* t)
{
	Json::Value v = trackable_json(t);
	v["buffer"] = (unsigned)t->buffer_index;
	return v;
}

static Json::Value trackeddescriptorset_trace_json(const trackeddescriptorset_trace* t)
{
	Json::Value v = trackable_json(t);
	v["pool"] = (unsigned)t->pool_index;
	return v;
}

static Json::Value trackedqueue_json(const trackedqueue* t)
{
	Json::Value v = trackable_json(t);
	v["queueFamily"] = (unsigned)t->queueFamily;
	v["queueIndex"] = (unsigned)t->queueIndex;
	v["queueFlags"] = (unsigned)t->queueFlags;
	return v;
}

static Json::Value trackedevent_trace_json(const trackedevent_trace* t)
{
	Json::Value v = trackable_json(t);
	return v;
}

static Json::Value trackedmemory_json(const trackedmemory* t)
{
	Json::Value v = trackable_json(t);
	return v;
}

static Json::Value trackedfence_json(const trackedfence* t)
{
	Json::Value v = trackable_json(t);
	v["flags"] = (unsigned)t->flags;
	return v;
}

static Json::Value trackedpipeline_json(const trackedpipeline* t)
{
	Json::Value v = trackable_json(t);
	v["flags"] = (unsigned)t->flags;
	v["type"] = (unsigned)t->type;
	return v;
}

static Json::Value trackedcommandpool_trace_json(const trackedcommandpool_trace* t)
{
	Json::Value v = trackable_json(t);
	return v;
}

static Json::Value trackeddescriptorpool_trace_json(const trackeddescriptorpool_trace* t)
{
	Json::Value v = trackable_json(t);
	return v;
}

static Json::Value trackeddevice_json(const trackeddevice* t)
{
	Json::Value v = trackable_json(t);
	return v;
}

static Json::Value trackedframebuffer_json(const trackedframebuffer* t)
{
	Json::Value v = trackable_json(t);
	v["width"] = t->width;
	v["height"] = t->height;
	v["layers"] = t->layers;
	return v;
}

static Json::Value trackedrenderpass_json(const trackedrenderpass* t)
{
	Json::Value v = trackable_json(t);
	return v;
}
