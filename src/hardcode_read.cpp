// This file is special in that it is included into the autogenerated file,
// and not compiled as a separate unit.

// TBD: move a lot of these into tracked objects instead
static VkInstance stored_instance = VK_NULL_HANDLE;
static VkPhysicalDevice selected_physical_device = VK_NULL_HANDLE;
static VkDebugReportCallbackEXT stored_callback = VK_NULL_HANDLE;
static bool callback_initialized = false;
static bool has_pipeline_feedback = false;
static bool has_pipeline_control = false;
static bool has_debug_report = false;
static bool has_debug_utils = false;
static int has_dedicated_allocation = 0;
static uint32_t selected_queue_family_index = 0xdeadbeef;
static VkPhysicalDeviceFeatures2 stored_VkPhysicalDeviceFeatures2 = { VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_FEATURES_2, nullptr };
static VkPhysicalDeviceVulkan11Features stored_VkPhysicalDeviceVulkan11Features = {};
static VkPhysicalDeviceVulkan12Features stored_VkPhysicalDeviceVulkan12Features = {};
static VkPhysicalDeviceVulkan13Features stored_VkPhysicalDeviceVulkan13Features = {};
static std::vector<VkQueueFamilyProperties> device_VkQueueFamilyProperties;
static bool has_VkPhysicalDeviceFeatures2 = false;
static bool has_VkPhysicalDeviceVulkan11Features = false;
static bool has_VkPhysicalDeviceVulkan12Features = false;
static bool has_VkPhysicalDeviceVulkan13Features = false;
static bool host_has_frame_boundary = false;

// this is a big hack until we have something better
void reset_for_tools()
{
	stored_VkPhysicalDeviceFeatures2 = { VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_FEATURES_2, nullptr };
	stored_VkPhysicalDeviceVulkan13Features = {};
	stored_VkPhysicalDeviceVulkan12Features = {};
	stored_VkPhysicalDeviceVulkan11Features = {};
	device_VkQueueFamilyProperties.clear();
	has_VkPhysicalDeviceFeatures2 = false;
	has_VkPhysicalDeviceVulkan11Features = false;
	has_VkPhysicalDeviceVulkan12Features = false;
	has_VkPhysicalDeviceVulkan13Features = false;
	host_has_frame_boundary = false;
	stored_instance = VK_NULL_HANDLE;
	selected_physical_device = VK_NULL_HANDLE;
	stored_callback = VK_NULL_HANDLE;
	has_pipeline_feedback = false;
	has_pipeline_control = false;
	has_debug_report = false;
	has_debug_utils = false;
	has_dedicated_allocation = 0;
	selected_queue_family_index = 0xdeadbeef;
	callback_initialized = false;
	stored_instance = VK_NULL_HANDLE;
	reset_all();
}

static void memory_report_callback(
	const VkDeviceMemoryReportCallbackDataEXT*  pCallbackData,
	void*                                       pUserData)
{
	// TBD
}

static VkBool32 VKAPI_PTR messenger_callback(
	VkDebugUtilsMessageSeverityFlagBitsEXT           messageSeverity,
	VkDebugUtilsMessageTypeFlagsEXT                  messageTypes,
	const VkDebugUtilsMessengerCallbackDataEXT*      pCallbackData,
	void*                                            pUserData)
{
	if (!is_debug() && (messageSeverity == VK_DEBUG_UTILS_MESSAGE_SEVERITY_VERBOSE_BIT_EXT || messageSeverity == VK_DEBUG_UTILS_MESSAGE_SEVERITY_INFO_BIT_EXT)) return VK_TRUE;
	ILOG("messenger (s%d, t%d): %s", (int)messageSeverity, (int)messageTypes, pCallbackData->pMessage);
	return VK_TRUE;
}

static VkBool32 VKAPI_PTR debug_report_callback(
	VkDebugReportFlagsEXT                       flags,
	VkDebugReportObjectTypeEXT                  objectType,
	uint64_t                                    object,
	size_t                                      location,
	int32_t                                     messageCode,
	const char*                                 pLayerPrefix,
	const char*                                 pMessage,
	void*                                       pUserData)
{
	if (((flags & VK_DEBUG_REPORT_INFORMATION_BIT_EXT) || (flags & VK_DEBUG_REPORT_DEBUG_BIT_EXT)) && !is_debug()) return VK_TRUE;
	ILOG("%s (%d): %s", pLayerPrefix, messageCode, pMessage);
	return VK_TRUE;
}

static uint32_t choose_memory_type(VkPhysicalDevice physical_device, uint32_t type_filter, VkMemoryPropertyFlags properties)
{
	VkPhysicalDeviceMemoryProperties memory_properties = {};
	wrap_vkGetPhysicalDeviceMemoryProperties(physical_device, &memory_properties);
	for (uint32_t i = 0; i < memory_properties.memoryTypeCount; ++i)
	{
		if (type_filter & (1u << i) && (memory_properties.memoryTypes[i].propertyFlags & properties) == properties)
		{
			return i;
		}
	}
	properties &= ~(VK_MEMORY_PROPERTY_HOST_CACHED_BIT | VK_MEMORY_PROPERTY_LAZILY_ALLOCATED_BIT);
	for (uint32_t i = 0; i < memory_properties.memoryTypeCount; ++i)
	{
		if (type_filter & (1u << i) && (memory_properties.memoryTypes[i].propertyFlags & properties) == properties)
		{
			return i;
		}
	}
	ILOG("Memory flags requested:");
	if (properties & VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT) ILOG("\tDEVICE_LOCAL");
	if (properties & VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT) ILOG("\tHOST_VISIBLE");
	if (properties & VK_MEMORY_PROPERTY_HOST_COHERENT_BIT) ILOG("\tHOST_COHERENT");
	if (properties & VK_MEMORY_PROPERTY_HOST_CACHED_BIT) ILOG("\tHOST_CACHED");
	if (properties & VK_MEMORY_PROPERTY_LAZILY_ALLOCATED_BIT) ILOG("\tLAZILY_ALLOCATED");
	if (properties & VK_MEMORY_PROPERTY_PROTECTED_BIT) ILOG("\tPROTECTED");
	ABORT("Failed to find required memory type (filter=%u, props=%u)", type_filter, (unsigned)properties);
	return 0;
}

static VkDeviceAddress get_buffer_device_address(VkDevice device, VkBuffer buffer)
{
	if (buffer == VK_NULL_HANDLE) return 0;
	VkBufferDeviceAddressInfo info = { VK_STRUCTURE_TYPE_BUFFER_DEVICE_ADDRESS_INFO, nullptr };
	info.buffer = buffer;
	if (wrap_vkGetBufferDeviceAddress) return wrap_vkGetBufferDeviceAddress(device, &info);
	if (wrap_vkGetBufferDeviceAddressKHR) return wrap_vkGetBufferDeviceAddressKHR(device, &info);
	if (wrap_vkGetBufferDeviceAddressEXT) return wrap_vkGetBufferDeviceAddressEXT(device, &info);
	return 0;
}

static char* mem_map(lava_file_reader& reader, VkDevice device, const suballoc_location& loc);
static void mem_unmap(lava_file_reader& reader, VkDevice device, const suballoc_location& loc, VkMarkedOffsetsARM* ar, char* ptr);
static void replay_register_raytracing_callbacks(lava_reader& reader);
static void replay_track_vkCmdBindPipeline(callback_context& cb, VkCommandBuffer commandBuffer, VkPipelineBindPoint pipelineBindPoint, VkPipeline pipeline);
static void replay_track_vkGetRayTracingShaderGroupHandlesKHR(callback_context& cb, VkDevice device, VkPipeline pipeline, uint32_t firstGroup, uint32_t groupCount, size_t dataSize, void* pData);
static void replay_track_vkGetRayTracingCaptureReplayShaderGroupHandlesKHR(callback_context& cb, VkDevice device, VkPipeline pipeline, uint32_t firstGroup, uint32_t groupCount, size_t dataSize, void* pData);
static void replay_fixup_commandbuffer_raytracing_sbt(lava_file_reader& reader, trackedcmdbuffer& commandbuffer_data);
static void replay_fixup_commandbuffer_raytracing_instances(lava_file_reader& reader, trackedcmdbuffer& commandbuffer_data);
static void replay_fixup_vkCmdTraceRaysKHR(callback_context& cb, VkCommandBuffer commandBuffer, const VkStridedDeviceAddressRegionKHR* pRaygenShaderBindingTable,
	const VkStridedDeviceAddressRegionKHR* pMissShaderBindingTable, const VkStridedDeviceAddressRegionKHR* pHitShaderBindingTable,
	const VkStridedDeviceAddressRegionKHR* pCallableShaderBindingTable, uint32_t width, uint32_t height, uint32_t depth);
static void replay_fixup_vkCmdTraceRaysIndirectKHR(callback_context& cb, VkCommandBuffer commandBuffer, const VkStridedDeviceAddressRegionKHR* pRaygenShaderBindingTable,
	const VkStridedDeviceAddressRegionKHR* pMissShaderBindingTable, const VkStridedDeviceAddressRegionKHR* pHitShaderBindingTable,
	const VkStridedDeviceAddressRegionKHR* pCallableShaderBindingTable, VkDeviceAddress indirectDeviceAddress);
static void replay_fixup_vkCmdTraceRaysIndirect2KHR(callback_context& cb, VkCommandBuffer commandBuffer, VkDeviceAddress indirectDeviceAddress);

static trackedbuffer* find_buffer_by_replay_address(VkDeviceAddress address, VkDeviceSize size, VkDeviceSize& out_offset)
{
	trackedbuffer* best = nullptr;
	VkDeviceSize best_size = 0;
	for (auto& buffer : VkBuffer_index)
	{
		if (buffer.device_address == 0 || buffer.size == 0) continue;
		if (address < buffer.device_address) continue;
		const VkDeviceSize offset = address - buffer.device_address;
		if (offset + size > buffer.size) continue;
		if (!best || buffer.size > best_size)
		{
			best = &buffer;
			best_size = buffer.size;
			out_offset = offset;
		}
	}
	return best;
}

static bool is_known_replay_as_address(VkDeviceAddress address)
{
	if (address == 0) return false;
	for (const auto& as : VkAccelerationStructureKHR_index)
	{
		if (as.device_address == address) return true;
	}
	return false;
}

static void destroy_internal_buffer(VkDevice device, internal_buffer& buffer)
{
	if (buffer.buffer != VK_NULL_HANDLE) wrap_vkDestroyBuffer(device, buffer.buffer, nullptr);
	if (buffer.memory != VK_NULL_HANDLE) wrap_vkFreeMemory(device, buffer.memory, nullptr);
	buffer = {};
}

static bool create_internal_buffer(VkDevice device, VkPhysicalDevice physical_device, VkDeviceSize size, VkBufferUsageFlags usage,
	VkMemoryPropertyFlags memory_flags, internal_buffer& out)
{
	if (device == VK_NULL_HANDLE || size == 0) return false;
	VkBufferCreateInfo info = { VK_STRUCTURE_TYPE_BUFFER_CREATE_INFO, nullptr };
	info.size = size;
	info.usage = usage;
	info.sharingMode = VK_SHARING_MODE_EXCLUSIVE;
	VkResult result = wrap_vkCreateBuffer(device, &info, nullptr, &out.buffer);
	if (result != VK_SUCCESS)
	{
		ELOG("Failed to create internal buffer (size=%lu, usage=%u)", (unsigned long)size, (unsigned)usage);
		return false;
	}
	VkMemoryRequirements req = {};
	wrap_vkGetBufferMemoryRequirements(device, out.buffer, &req);
	VkMemoryAllocateFlagsInfo alloc_flags = { VK_STRUCTURE_TYPE_MEMORY_ALLOCATE_FLAGS_INFO, nullptr };
	if (usage & VK_BUFFER_USAGE_SHADER_DEVICE_ADDRESS_BIT)
	{
		alloc_flags.flags = VK_MEMORY_ALLOCATE_DEVICE_ADDRESS_BIT_KHR;
	}
	VkMemoryAllocateInfo alloc = { VK_STRUCTURE_TYPE_MEMORY_ALLOCATE_INFO, nullptr };
	alloc.allocationSize = req.size;
	alloc.memoryTypeIndex = choose_memory_type(physical_device, req.memoryTypeBits,
		(memory_flags != 0) ? memory_flags : VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT);
	if (alloc_flags.flags != 0) alloc.pNext = &alloc_flags;
	result = wrap_vkAllocateMemory(device, &alloc, nullptr, &out.memory);
	if (result != VK_SUCCESS)
	{
		wrap_vkDestroyBuffer(device, out.buffer, nullptr);
		out.buffer = VK_NULL_HANDLE;
		ELOG("Failed to allocate internal buffer memory (size=%lu)", (unsigned long)req.size);
		return false;
	}
	result = wrap_vkBindBufferMemory(device, out.buffer, out.memory, 0);
	if (result != VK_SUCCESS)
	{
		wrap_vkFreeMemory(device, out.memory, nullptr);
		wrap_vkDestroyBuffer(device, out.buffer, nullptr);
		out = {};
		ELOG("Failed to bind internal buffer memory (size=%lu)", (unsigned long)req.size);
		return false;
	}
	out.size = size;
	out.usage = usage;
	out.memory_flags = memory_flags;
	if (usage & VK_BUFFER_USAGE_SHADER_DEVICE_ADDRESS_BIT)
	{
		out.device_address = get_buffer_device_address(device, out.buffer);
	}
	return true;
}

static uint64_t debug_object_lookup(VkDebugReportObjectTypeEXT type, uint32_t index)
{
	switch (type)
	{
	case VK_DEBUG_REPORT_OBJECT_TYPE_INSTANCE_EXT: return (uint64_t)stored_instance;
	case VK_DEBUG_REPORT_OBJECT_TYPE_PHYSICAL_DEVICE_EXT: return (uint64_t)selected_physical_device;
	case VK_DEBUG_REPORT_OBJECT_TYPE_DEVICE_EXT: return (uint64_t)index_to_VkDevice.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_QUEUE_EXT: return (uint64_t)index_to_VkQueue.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_DEVICE_MEMORY_EXT: return 0;
	case VK_DEBUG_REPORT_OBJECT_TYPE_SEMAPHORE_EXT: return (uint64_t)index_to_VkSemaphore.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_COMMAND_BUFFER_EXT: return (uint64_t)index_to_VkCommandBuffer.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_FENCE_EXT: return (uint64_t)index_to_VkFence.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_BUFFER_EXT: return (uint64_t)index_to_VkBuffer.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_IMAGE_EXT: return (uint64_t)index_to_VkImage.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_EVENT_EXT: return (uint64_t)index_to_VkEvent.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_QUERY_POOL_EXT: return (uint64_t)index_to_VkQueryPool.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_BUFFER_VIEW_EXT: return (uint64_t)index_to_VkBufferView.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_IMAGE_VIEW_EXT: return (uint64_t)index_to_VkImageView.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_SHADER_MODULE_EXT: return (uint64_t)index_to_VkShaderModule.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_PIPELINE_CACHE_EXT: return (uint64_t)index_to_VkPipelineCache.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_PIPELINE_LAYOUT_EXT: return (uint64_t)index_to_VkPipelineLayout.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_RENDER_PASS_EXT: return (uint64_t)index_to_VkRenderPass.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_PIPELINE_EXT: return (uint64_t)index_to_VkPipeline.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_DESCRIPTOR_SET_LAYOUT_EXT: return (uint64_t)index_to_VkDescriptorSetLayout.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_SAMPLER_EXT: return (uint64_t)index_to_VkSampler.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_DESCRIPTOR_POOL_EXT: return (uint64_t)index_to_VkDescriptorPool.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_DESCRIPTOR_SET_EXT: return (uint64_t)index_to_VkDescriptorSet.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_FRAMEBUFFER_EXT: return (uint64_t)index_to_VkFramebuffer.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_COMMAND_POOL_EXT: return (uint64_t)index_to_VkCommandPool.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_SURFACE_KHR_EXT: return (uint64_t)index_to_VkSurfaceKHR.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_SWAPCHAIN_KHR_EXT: return (uint64_t)index_to_VkSwapchainKHR.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_DESCRIPTOR_UPDATE_TEMPLATE_EXT: return (uint64_t)index_to_VkDescriptorUpdateTemplate.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_DISPLAY_KHR_EXT: return (uint64_t)index_to_VkDisplayKHR.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_DISPLAY_MODE_KHR_EXT: return (uint64_t)index_to_VkDisplayModeKHR.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_ACCELERATION_STRUCTURE_KHR_EXT: return (uint64_t)index_to_VkAccelerationStructureKHR.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_VALIDATION_CACHE_EXT: return (uint64_t)index_to_VkValidationCacheEXT.at(index);
	case VK_DEBUG_REPORT_OBJECT_TYPE_SAMPLER_YCBCR_CONVERSION_KHR_EXT: return (uint64_t)index_to_VkSamplerYcbcrConversion.at(index);
	// these are not supported:
	case VK_DEBUG_REPORT_OBJECT_TYPE_CUDA_MODULE_NV_EXT:
	case VK_DEBUG_REPORT_OBJECT_TYPE_CUDA_FUNCTION_NV_EXT:
	case VK_DEBUG_REPORT_OBJECT_TYPE_DEBUG_REPORT_CALLBACK_EXT_EXT:
	case VK_DEBUG_REPORT_OBJECT_TYPE_CU_MODULE_NVX_EXT:
	case VK_DEBUG_REPORT_OBJECT_TYPE_CU_FUNCTION_NVX_EXT:
	case VK_DEBUG_REPORT_OBJECT_TYPE_ACCELERATION_STRUCTURE_NV_EXT:
	case VK_DEBUG_REPORT_OBJECT_TYPE_BUFFER_COLLECTION_FUCHSIA_EXT:
	case VK_DEBUG_REPORT_OBJECT_TYPE_UNKNOWN_EXT:
	case VK_DEBUG_REPORT_OBJECT_TYPE_MAX_ENUM_EXT: assert(false); return 0;
	}
	return 0;
}

static uint64_t object_lookup(VkObjectType type, uint32_t index)
{
	switch (type)
	{
	case VK_OBJECT_TYPE_INSTANCE: return (uint64_t)stored_instance;
	case VK_OBJECT_TYPE_PHYSICAL_DEVICE: return (uint64_t)selected_physical_device;
	case VK_OBJECT_TYPE_DEVICE: return (uint64_t)index_to_VkDevice.at(index);
	case VK_OBJECT_TYPE_QUEUE: return (uint64_t)index_to_VkQueue.at(index);
	case VK_OBJECT_TYPE_DEVICE_MEMORY: return 0;
	case VK_OBJECT_TYPE_SEMAPHORE: return (uint64_t)index_to_VkSemaphore.at(index);
	case VK_OBJECT_TYPE_COMMAND_BUFFER: return (uint64_t)index_to_VkCommandBuffer.at(index);
	case VK_OBJECT_TYPE_FENCE: return (uint64_t)index_to_VkFence.at(index);
	case VK_OBJECT_TYPE_BUFFER: return (uint64_t)index_to_VkBuffer.at(index);
	case VK_OBJECT_TYPE_IMAGE: return (uint64_t)index_to_VkImage.at(index);
	case VK_OBJECT_TYPE_EVENT: return (uint64_t)index_to_VkEvent.at(index);
	case VK_OBJECT_TYPE_QUERY_POOL: return (uint64_t)index_to_VkQueryPool.at(index);
	case VK_OBJECT_TYPE_BUFFER_VIEW: return (uint64_t)index_to_VkBufferView.at(index);
	case VK_OBJECT_TYPE_IMAGE_VIEW: return (uint64_t)index_to_VkImageView.at(index);
	case VK_OBJECT_TYPE_SHADER_MODULE: return (uint64_t)index_to_VkShaderModule.at(index);
	case VK_OBJECT_TYPE_PIPELINE_CACHE: return (uint64_t)index_to_VkPipelineCache.at(index);
	case VK_OBJECT_TYPE_PIPELINE_LAYOUT: return (uint64_t)index_to_VkPipelineLayout.at(index);
	case VK_OBJECT_TYPE_RENDER_PASS: return (uint64_t)index_to_VkRenderPass.at(index);
	case VK_OBJECT_TYPE_PIPELINE: return (uint64_t)index_to_VkPipeline.at(index);
	case VK_OBJECT_TYPE_DESCRIPTOR_SET_LAYOUT: return (uint64_t)index_to_VkDescriptorSetLayout.at(index);
	case VK_OBJECT_TYPE_SAMPLER: return (uint64_t)index_to_VkSampler.at(index);
	case VK_OBJECT_TYPE_DESCRIPTOR_POOL: return (uint64_t)index_to_VkDescriptorPool.at(index);
	case VK_OBJECT_TYPE_DESCRIPTOR_SET: return (uint64_t)index_to_VkDescriptorSet.at(index);
	case VK_OBJECT_TYPE_FRAMEBUFFER: return (uint64_t)index_to_VkFramebuffer.at(index);
	case VK_OBJECT_TYPE_COMMAND_POOL: return (uint64_t)index_to_VkCommandPool.at(index);
	case VK_OBJECT_TYPE_SURFACE_KHR: return (uint64_t)index_to_VkSurfaceKHR.at(index);
	case VK_OBJECT_TYPE_SWAPCHAIN_KHR: return (uint64_t)index_to_VkSwapchainKHR.at(index);
	case VK_OBJECT_TYPE_DESCRIPTOR_UPDATE_TEMPLATE: return (uint64_t)index_to_VkDescriptorUpdateTemplate.at(index);
	case VK_OBJECT_TYPE_DISPLAY_KHR: return (uint64_t)index_to_VkDisplayKHR.at(index);
	case VK_OBJECT_TYPE_DISPLAY_MODE_KHR: return (uint64_t)index_to_VkDisplayModeKHR.at(index);
	case VK_OBJECT_TYPE_ACCELERATION_STRUCTURE_KHR: return (uint64_t)index_to_VkAccelerationStructureKHR.at(index);
	case VK_OBJECT_TYPE_VALIDATION_CACHE_EXT: return (uint64_t)index_to_VkValidationCacheEXT.at(index);
	case VK_OBJECT_TYPE_DEFERRED_OPERATION_KHR: return (uint64_t)index_to_VkDeferredOperationKHR.at(index);
	case VK_OBJECT_TYPE_MICROMAP_EXT: return (uint64_t)index_to_VkMicromapEXT.at(index);
	case VK_OBJECT_TYPE_PRIVATE_DATA_SLOT: return (uint64_t)index_to_VkPrivateDataSlot.at(index);
	case VK_OBJECT_TYPE_SAMPLER_YCBCR_CONVERSION_KHR: return (uint64_t)index_to_VkSamplerYcbcrConversion.at(index);
	case VK_OBJECT_TYPE_VIDEO_SESSION_KHR: return (uint64_t)index_to_VkVideoSessionKHR.at(index);
	case VK_OBJECT_TYPE_VIDEO_SESSION_PARAMETERS_KHR: return (uint64_t)index_to_VkVideoSessionParametersKHR.at(index);
	case VK_OBJECT_TYPE_SHADER_EXT: return (uint64_t)index_to_VkShaderEXT.at(index);
	case VK_OBJECT_TYPE_DEBUG_REPORT_CALLBACK_EXT: return (uint64_t)index_to_VkDebugReportCallbackEXT.at(index);
	case VK_OBJECT_TYPE_DEBUG_UTILS_MESSENGER_EXT: return (uint64_t)index_to_VkDebugUtilsMessengerEXT.at(index);
	case VK_OBJECT_TYPE_TENSOR_ARM: return (uint64_t)index_to_VkTensorARM.at(index);
	case VK_OBJECT_TYPE_TENSOR_VIEW_ARM: return (uint64_t)index_to_VkTensorViewARM.at(index);
	case VK_OBJECT_TYPE_DATA_GRAPH_PIPELINE_SESSION_ARM: return (uint64_t)index_to_VkDataGraphPipelineSessionARM.at(index);

	// these are not supported:
	case VK_OBJECT_TYPE_EXTERNAL_COMPUTE_QUEUE_NV:
	case VK_OBJECT_TYPE_OPTICAL_FLOW_SESSION_NV:
	case VK_OBJECT_TYPE_BUFFER_COLLECTION_FUCHSIA:
	case VK_OBJECT_TYPE_INDIRECT_COMMANDS_LAYOUT_NV:
	case VK_OBJECT_TYPE_PERFORMANCE_CONFIGURATION_INTEL:
	case VK_OBJECT_TYPE_ACCELERATION_STRUCTURE_NV:
	case VK_OBJECT_TYPE_CU_FUNCTION_NVX:
	case VK_OBJECT_TYPE_CU_MODULE_NVX:
	case VK_OBJECT_TYPE_PIPELINE_BINARY_KHR:
	case VK_OBJECT_TYPE_INDIRECT_COMMANDS_LAYOUT_EXT:
	case VK_OBJECT_TYPE_INDIRECT_EXECUTION_SET_EXT:
	case VK_OBJECT_TYPE_UNKNOWN:
	case VK_OBJECT_TYPE_MAX_ENUM: assert(false); return 0;
	}
	return 0;
}

trackable& object_trackable(VkObjectType type, uint64_t handle)
{
	static trackable dummy;
	switch (type)
	{
	case VK_OBJECT_TYPE_INSTANCE: return VkInstance_index.at(index_to_VkInstance.index(stored_instance));
	case VK_OBJECT_TYPE_PHYSICAL_DEVICE: return VkPhysicalDevice_index.at(index_to_VkPhysicalDevice.index(selected_physical_device));
	case VK_OBJECT_TYPE_DEVICE: return VkDevice_index.at(index_to_VkDevice.index((VkDevice)handle));
	case VK_OBJECT_TYPE_QUEUE: return VkQueue_index.at(index_to_VkQueue.index((VkQueue)handle));
	case VK_OBJECT_TYPE_DEVICE_MEMORY: assert(false); return dummy;
	case VK_OBJECT_TYPE_SEMAPHORE: return VkSemaphore_index.at(index_to_VkSemaphore.index((VkSemaphore)handle));
	case VK_OBJECT_TYPE_COMMAND_BUFFER: return VkCommandBuffer_index.at(index_to_VkCommandBuffer.index((VkCommandBuffer)handle));
	case VK_OBJECT_TYPE_FENCE: return VkFence_index.at(index_to_VkFence.index((VkFence)handle));
	case VK_OBJECT_TYPE_BUFFER: return VkBuffer_index.at(index_to_VkBuffer.index((VkBuffer)handle));
	case VK_OBJECT_TYPE_IMAGE: return VkImage_index.at(index_to_VkImage.index((VkImage)handle));
	case VK_OBJECT_TYPE_EVENT: return VkEvent_index.at(index_to_VkEvent.index((VkEvent)handle));
	case VK_OBJECT_TYPE_QUERY_POOL: return VkQueryPool_index.at(index_to_VkQueryPool.index((VkQueryPool)handle));
	case VK_OBJECT_TYPE_BUFFER_VIEW: return VkBufferView_index.at(index_to_VkBufferView.index((VkBufferView)handle));
	case VK_OBJECT_TYPE_IMAGE_VIEW: return VkImageView_index.at(index_to_VkImageView.index((VkImageView)handle));
	case VK_OBJECT_TYPE_SHADER_MODULE: return VkShaderModule_index.at(index_to_VkShaderModule.index((VkShaderModule)handle));
	case VK_OBJECT_TYPE_PIPELINE_CACHE: return VkPipelineCache_index.at(index_to_VkPipelineCache.index((VkPipelineCache)handle));
	case VK_OBJECT_TYPE_PIPELINE_LAYOUT: return VkPipelineLayout_index.at(index_to_VkPipelineLayout.index((VkPipelineLayout)handle));
	case VK_OBJECT_TYPE_RENDER_PASS: return VkRenderPass_index.at(index_to_VkRenderPass.index((VkRenderPass)handle));
	case VK_OBJECT_TYPE_PIPELINE: return VkPipeline_index.at(index_to_VkPipeline.index((VkPipeline)handle));
	case VK_OBJECT_TYPE_DESCRIPTOR_SET_LAYOUT: return VkDescriptorSetLayout_index.at(index_to_VkDescriptorSetLayout.index((VkDescriptorSetLayout)handle));
	case VK_OBJECT_TYPE_SAMPLER: return VkSampler_index.at(index_to_VkSampler.index((VkSampler)handle));
	case VK_OBJECT_TYPE_DESCRIPTOR_POOL: return VkDescriptorPool_index.at(index_to_VkDescriptorPool.index((VkDescriptorPool)handle));
	case VK_OBJECT_TYPE_DESCRIPTOR_SET: return VkDescriptorSet_index.at(index_to_VkDescriptorSet.index((VkDescriptorSet)handle));
	case VK_OBJECT_TYPE_FRAMEBUFFER: return VkFramebuffer_index.at(index_to_VkFramebuffer.index((VkFramebuffer)handle));
	case VK_OBJECT_TYPE_COMMAND_POOL: return VkCommandPool_index.at(index_to_VkCommandPool.index((VkCommandPool)handle));
	case VK_OBJECT_TYPE_SURFACE_KHR: return VkSurfaceKHR_index.at(index_to_VkSurfaceKHR.index((VkSurfaceKHR)handle));
	case VK_OBJECT_TYPE_SWAPCHAIN_KHR: return VkSwapchainKHR_index.at(index_to_VkSwapchainKHR.index((VkSwapchainKHR)handle));
	case VK_OBJECT_TYPE_DESCRIPTOR_UPDATE_TEMPLATE: return VkDescriptorUpdateTemplate_index.at(index_to_VkDescriptorUpdateTemplate.index((VkDescriptorUpdateTemplate)handle));
	case VK_OBJECT_TYPE_PRIVATE_DATA_SLOT: return VkPrivateDataSlot_index.at(index_to_VkPrivateDataSlot.index((VkPrivateDataSlot)handle));
	case VK_OBJECT_TYPE_DISPLAY_KHR: return VkDisplayKHR_index.at(index_to_VkDisplayKHR.index((VkDisplayKHR)handle));
	case VK_OBJECT_TYPE_DISPLAY_MODE_KHR: return VkDisplayModeKHR_index.at(index_to_VkDisplayModeKHR.index((VkDisplayModeKHR)handle));
	case VK_OBJECT_TYPE_ACCELERATION_STRUCTURE_KHR: return VkAccelerationStructureKHR_index.at(index_to_VkAccelerationStructureKHR.index((VkAccelerationStructureKHR)handle));
	case VK_OBJECT_TYPE_VALIDATION_CACHE_EXT: return VkValidationCacheEXT_index.at(index_to_VkValidationCacheEXT.index((VkValidationCacheEXT)handle));
	case VK_OBJECT_TYPE_DEFERRED_OPERATION_KHR: return VkDeferredOperationKHR_index.at(index_to_VkDeferredOperationKHR.index((VkDeferredOperationKHR)handle));
	case VK_OBJECT_TYPE_MICROMAP_EXT: return VkMicromapEXT_index.at(index_to_VkMicromapEXT.index((VkMicromapEXT)handle));
	case VK_OBJECT_TYPE_SAMPLER_YCBCR_CONVERSION_KHR: return VkSamplerYcbcrConversion_index.at(index_to_VkSamplerYcbcrConversion.index((VkSamplerYcbcrConversion)handle));
	case VK_OBJECT_TYPE_VIDEO_SESSION_KHR: return VkVideoSessionKHR_index.at(index_to_VkVideoSessionKHR.index((VkVideoSessionKHR)handle));
	case VK_OBJECT_TYPE_VIDEO_SESSION_PARAMETERS_KHR: return VkVideoSessionParametersKHR_index.at(index_to_VkVideoSessionParametersKHR.index((VkVideoSessionParametersKHR)handle));
	case VK_OBJECT_TYPE_SHADER_EXT: return VkShaderEXT_index.at(index_to_VkShaderEXT.index((VkShaderEXT)handle));
	case VK_OBJECT_TYPE_DEBUG_REPORT_CALLBACK_EXT: return VkDebugReportCallbackEXT_index.at(index_to_VkDebugReportCallbackEXT.index((VkDebugReportCallbackEXT)handle));
	case VK_OBJECT_TYPE_DEBUG_UTILS_MESSENGER_EXT: return VkDebugUtilsMessengerEXT_index.at(index_to_VkDebugUtilsMessengerEXT.index((VkDebugUtilsMessengerEXT)handle));
	case VK_OBJECT_TYPE_TENSOR_ARM: return VkTensorARM_index.at(index_to_VkTensorARM.index((VkTensorARM)handle));
	case VK_OBJECT_TYPE_TENSOR_VIEW_ARM: return VkTensorViewARM_index.at(index_to_VkTensorViewARM.index((VkTensorViewARM)handle));
	case VK_OBJECT_TYPE_DATA_GRAPH_PIPELINE_SESSION_ARM: return VkDataGraphPipelineSessionARM_index.at(index_to_VkDataGraphPipelineSessionARM.index((VkDataGraphPipelineSessionARM)handle));

	// these are not supported:
	case VK_OBJECT_TYPE_EXTERNAL_COMPUTE_QUEUE_NV:
	case VK_OBJECT_TYPE_OPTICAL_FLOW_SESSION_NV:
	case VK_OBJECT_TYPE_BUFFER_COLLECTION_FUCHSIA:
	case VK_OBJECT_TYPE_INDIRECT_COMMANDS_LAYOUT_NV:
	case VK_OBJECT_TYPE_PERFORMANCE_CONFIGURATION_INTEL:
	case VK_OBJECT_TYPE_ACCELERATION_STRUCTURE_NV:
	case VK_OBJECT_TYPE_CU_FUNCTION_NVX:
	case VK_OBJECT_TYPE_CU_MODULE_NVX:
	case VK_OBJECT_TYPE_PIPELINE_BINARY_KHR:
	case VK_OBJECT_TYPE_INDIRECT_COMMANDS_LAYOUT_EXT:
	case VK_OBJECT_TYPE_INDIRECT_EXECUTION_SET_EXT:
	case VK_OBJECT_TYPE_UNKNOWN:
	case VK_OBJECT_TYPE_MAX_ENUM: assert(false); return dummy;
	}
	assert(false);
	return dummy;
}

void replay_pre_vkQueueSubmit2(lava_file_reader& reader, VkQueue queue, uint32_t submitCount, const VkSubmitInfo2* pSubmits, VkFence fence)
{
	for (uint32_t i = 0; i < submitCount; i++)
	{
		if (!host_has_frame_boundary) purge_extension_parent(const_cast<VkSubmitInfo2*>(&pSubmits[i]), VK_STRUCTURE_TYPE_FRAME_BOUNDARY_EXT);
		for (uint32_t j = 0; j < pSubmits[i].commandBufferInfoCount; j++)
		{
			const VkCommandBuffer command_buffer = pSubmits[i].pCommandBufferInfos[j].commandBuffer;
			const uint32_t commandbuffer_index = index_to_VkCommandBuffer.index(command_buffer);
			if (commandbuffer_index == CONTAINER_INVALID_INDEX) continue;
			trackedcmdbuffer& commandbuffer_data = VkCommandBuffer_index.at(commandbuffer_index);
			replay_fixup_commandbuffer_raytracing_instances(reader, commandbuffer_data);
			replay_fixup_commandbuffer_raytracing_sbt(reader, commandbuffer_data);
		}
	}
}

void replay_pre_vkQueueSubmit2KHR(lava_file_reader& reader, VkQueue queue, uint32_t submitCount, const VkSubmitInfo2KHR* pSubmits, VkFence fence)
{
	replay_pre_vkQueueSubmit2(reader, queue, submitCount, pSubmits, fence);
}

void replay_pre_vkQueueSubmit(lava_file_reader& reader, VkQueue queue, uint32_t submitCount, const VkSubmitInfo* pSubmits, VkFence fence)
{
	for (uint32_t i = 0; i < submitCount; i++)
	{
		if (!host_has_frame_boundary) purge_extension_parent(const_cast<VkSubmitInfo*>(&pSubmits[i]), VK_STRUCTURE_TYPE_FRAME_BOUNDARY_EXT);
		for (uint32_t j = 0; j < pSubmits[i].commandBufferCount; j++)
		{
			const VkCommandBuffer command_buffer = pSubmits[i].pCommandBuffers[j];
			const uint32_t commandbuffer_index = index_to_VkCommandBuffer.index(command_buffer);
			if (commandbuffer_index == CONTAINER_INVALID_INDEX) continue;
			trackedcmdbuffer& commandbuffer_data = VkCommandBuffer_index.at(commandbuffer_index);
			replay_fixup_commandbuffer_raytracing_instances(reader, commandbuffer_data);
			replay_fixup_commandbuffer_raytracing_sbt(reader, commandbuffer_data);
		}
	}
}

void replay_pre_vkBeginCommandBuffer(lava_file_reader& reader, VkCommandBuffer commandBuffer, VkCommandBufferBeginInfo* pBeginInfo)
{
	(void)reader;
	(void)pBeginInfo;
	const uint32_t commandbuffer_index = index_to_VkCommandBuffer.index(commandBuffer);
	if (commandbuffer_index == CONTAINER_INVALID_INDEX) return;
	trackedcmdbuffer& commandbuffer_data = VkCommandBuffer_index.at(commandbuffer_index);
	commandbuffer_data.raytracing_sbt_uses.clear();
	commandbuffer_data.raytracing_instance_uses.clear();
	commandbuffer_data.bound_raytracing_pipeline_index = CONTAINER_INVALID_INDEX;
}

void replay_pre_vkResetCommandBuffer(lava_file_reader& reader, VkCommandBuffer commandBuffer, VkCommandBufferResetFlags flags)
{
	(void)reader;
	(void)flags;
	const uint32_t commandbuffer_index = index_to_VkCommandBuffer.index(commandBuffer);
	if (commandbuffer_index == CONTAINER_INVALID_INDEX) return;
	trackedcmdbuffer& commandbuffer_data = VkCommandBuffer_index.at(commandbuffer_index);
	commandbuffer_data.raytracing_sbt_uses.clear();
	commandbuffer_data.raytracing_instance_uses.clear();
	commandbuffer_data.bound_raytracing_pipeline_index = CONTAINER_INVALID_INDEX;
}

void replay_pre_vkCmdBuildAccelerationStructuresKHR(lava_file_reader& reader, VkCommandBuffer commandBuffer, uint32_t infoCount,
	VkAccelerationStructureBuildGeometryInfoKHR* pInfos, VkAccelerationStructureBuildRangeInfoKHR* const* ppBuildRangeInfos)
{
	if (!reader.run || !pInfos || !ppBuildRangeInfos || commandBuffer == VK_NULL_HANDLE) return;
	const uint32_t commandbuffer_index = index_to_VkCommandBuffer.index(commandBuffer);
	if (commandbuffer_index == CONTAINER_INVALID_INDEX) return;
	trackedcmdbuffer& commandbuffer_data = VkCommandBuffer_index.at(commandbuffer_index);
	const VkDevice device = commandbuffer_data.device;
	const VkPhysicalDevice physical_device = commandbuffer_data.physicalDevice;

	auto record_instance_uses = [&](const VkAccelerationStructureBuildGeometryInfoKHR& info, const VkAccelerationStructureBuildRangeInfoKHR* ranges)
	{
		if (!ranges || info.geometryCount == 0) return;
		for (uint32_t g = 0; g < info.geometryCount; g++)
		{
			const VkAccelerationStructureGeometryKHR* geometry = nullptr;
			if (info.pGeometries) geometry = &info.pGeometries[g];
			else if (info.ppGeometries) geometry = info.ppGeometries[g];
			if (!geometry) continue;
			if (geometry->geometryType != VK_GEOMETRY_TYPE_INSTANCES_KHR) continue;

			const VkAccelerationStructureGeometryInstancesDataKHR& instances = geometry->geometry.instances;
			if (instances.arrayOfPointers)
			{
				ABORT("Acceleration structure instance remap does not support arrayOfPointers on replay");
			}
			if (instances.data.deviceAddress == 0) continue;

			const VkDeviceSize instance_count = ranges[g].primitiveCount;
			if (instance_count == 0) continue;
			trackedcmdbuffer::raytracing_instance_use use = {};
			use.device_address = instances.data.deviceAddress;
			use.primitive_offset = ranges[g].primitiveOffset;
			use.primitive_count = (uint32_t)instance_count;
			commandbuffer_data.raytracing_instance_uses.push_back(use);
		}
	};

	for (uint32_t i = 0; i < infoCount; i++)
	{
		if (pInfos[i].geometryCount == 0) continue;
		record_instance_uses(pInfos[i], ppBuildRangeInfos[i]);
		std::vector<uint32_t> max_primitive_counts(pInfos[i].geometryCount);
		for (uint32_t g = 0; g < pInfos[i].geometryCount; g++)
		{
			max_primitive_counts[g] = ppBuildRangeInfos[i][g].primitiveCount;
		}
		VkAccelerationStructureBuildSizesInfoKHR sizes = { VK_STRUCTURE_TYPE_ACCELERATION_STRUCTURE_BUILD_SIZES_INFO_KHR, nullptr };
		wrap_vkGetAccelerationStructureBuildSizesKHR(device,
			VK_ACCELERATION_STRUCTURE_BUILD_TYPE_DEVICE_KHR,
			&pInfos[i],
			max_primitive_counts.data(),
			&sizes);

		VkDeviceSize scratch_size = sizes.buildScratchSize;
		if (pInfos[i].mode == VK_BUILD_ACCELERATION_STRUCTURE_MODE_UPDATE_KHR)
		{
			scratch_size = sizes.updateScratchSize;
		}
		if (scratch_size == 0) continue;

		if (commandbuffer_data.scratch_buffer.buffer == VK_NULL_HANDLE || commandbuffer_data.scratch_buffer.size < scratch_size)
		{
			if (commandbuffer_data.scratch_buffer.buffer != VK_NULL_HANDLE)
			{
				destroy_internal_buffer(device, commandbuffer_data.scratch_buffer);
			}
			const VkBufferUsageFlags usage = VK_BUFFER_USAGE_STORAGE_BUFFER_BIT | VK_BUFFER_USAGE_SHADER_DEVICE_ADDRESS_BIT;
			if (!create_internal_buffer(device, physical_device, scratch_size, usage, VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT, commandbuffer_data.scratch_buffer))
			{
				ABORT("Failed to allocate scratch buffer for acceleration structure build (size=%lu)", (unsigned long)scratch_size);
			}
		}

		pInfos[i].scratchData.deviceAddress = commandbuffer_data.scratch_buffer.device_address;
	}
}

static void replay_register_raytracing_callbacks(lava_reader& reader)
{
	if (reader.raytracing_callbacks_registered) return;
	reader.raytracing_callbacks_registered = true;
	vkCmdBindPipeline_callbacks.push_back(replay_track_vkCmdBindPipeline);
	vkGetRayTracingShaderGroupHandlesKHR_callbacks.push_back(replay_track_vkGetRayTracingShaderGroupHandlesKHR);
	vkGetRayTracingCaptureReplayShaderGroupHandlesKHR_callbacks.push_back(replay_track_vkGetRayTracingCaptureReplayShaderGroupHandlesKHR);
	vkCmdTraceRaysKHR_callbacks.push_back(replay_fixup_vkCmdTraceRaysKHR);
	vkCmdTraceRaysIndirectKHR_callbacks.push_back(replay_fixup_vkCmdTraceRaysIndirectKHR);
	vkCmdTraceRaysIndirect2KHR_callbacks.push_back(replay_fixup_vkCmdTraceRaysIndirect2KHR);
}

static void replay_track_vkCmdBindPipeline(callback_context& cb, VkCommandBuffer commandBuffer, VkPipelineBindPoint pipelineBindPoint, VkPipeline pipeline)
{
	if (!cb.reader.run) return;
	if (pipelineBindPoint != VK_PIPELINE_BIND_POINT_RAY_TRACING_KHR) return;
	const uint32_t commandbuffer_index = index_to_VkCommandBuffer.index(commandBuffer);
	if (commandbuffer_index == CONTAINER_INVALID_INDEX) return;
	trackedcmdbuffer& commandbuffer_data = VkCommandBuffer_index.at(commandbuffer_index);
	commandbuffer_data.bound_raytracing_pipeline_index = index_to_VkPipeline.index(pipeline);
}

static void store_raytracing_handles(trackedpipeline& pipeline_data, uint32_t firstGroup, uint32_t groupCount, uint32_t handle_size,
	const void* data, bool capture_replay)
{
	if (!data || groupCount == 0 || handle_size == 0) return;
	if (pipeline_data.raytracing_group_handle_size == 0)
	{
		pipeline_data.raytracing_group_handle_size = handle_size;
	}
	else if (pipeline_data.raytracing_group_handle_size != handle_size)
	{
		ABORT("Ray tracing shader group handle size mismatch for pipeline %u", pipeline_data.index);
	}

	const uint32_t required_groups = std::max(pipeline_data.raytracing_group_count, firstGroup + groupCount);
	pipeline_data.raytracing_group_count = required_groups;
	const size_t required_size = (size_t)required_groups * handle_size;
	if (pipeline_data.raytracing_group_handles.size() < required_size)
	{
		pipeline_data.raytracing_group_handles.resize(required_size);
	}
	std::byte* dst = pipeline_data.raytracing_group_handles.data() + (size_t)firstGroup * handle_size;
	memcpy(dst, data, (size_t)groupCount * handle_size);
	if (capture_replay) pipeline_data.raytracing_group_handles_capture_replay = true;
}

static void store_raytracing_handles_replay(trackedpipeline& pipeline_data, uint32_t firstGroup, uint32_t groupCount, uint32_t handle_size, const void* data)
{
	if (!data || groupCount == 0 || handle_size == 0) return;
	if (pipeline_data.raytracing_group_handle_size_replay == 0)
	{
		pipeline_data.raytracing_group_handle_size_replay = handle_size;
	}
	else if (pipeline_data.raytracing_group_handle_size_replay != handle_size)
	{
		ABORT("Ray tracing replay handle size mismatch for pipeline %u", pipeline_data.index);
	}

	const uint32_t required_groups = std::max(pipeline_data.raytracing_group_count, firstGroup + groupCount);
	const size_t required_size = (size_t)required_groups * handle_size;
	if (pipeline_data.raytracing_group_handles_replay.size() < required_size)
	{
		pipeline_data.raytracing_group_handles_replay.resize(required_size);
	}
	std::byte* dst = pipeline_data.raytracing_group_handles_replay.data() + (size_t)firstGroup * handle_size;
	memcpy(dst, data, (size_t)groupCount * handle_size);
}

static void replay_track_vkGetRayTracingShaderGroupHandlesKHR(callback_context& cb, VkDevice device, VkPipeline pipeline, uint32_t firstGroup,
	uint32_t groupCount, size_t dataSize, void* pData)
{
	if (!cb.reader.run || !pData || groupCount == 0 || dataSize == 0) return;
	if (dataSize % groupCount != 0)
	{
		DLOG("vkGetRayTracingShaderGroupHandlesKHR dataSize=%zu not a multiple of groupCount=%u", dataSize, groupCount);
		return;
	}
	const uint32_t pipeline_index = index_to_VkPipeline.index(pipeline);
	if (pipeline_index == CONTAINER_INVALID_INDEX) return;
	trackedpipeline& pipeline_data = VkPipeline_index.at(pipeline_index);

	const uint32_t capture_handle_size = (uint32_t)(dataSize / groupCount);
	if (capture_handle_size >= 8)
	{
		uint32_t h0 = 0;
		uint32_t h1 = 0;
		memcpy(&h0, pData, sizeof(uint32_t));
		memcpy(&h1, reinterpret_cast<const std::byte*>(pData) + sizeof(uint32_t), sizeof(uint32_t));
		DLOG2("Ray tracing capture handles: pipeline=%u firstGroup=%u groups=%u handleSize=%u h0=0x%08x%08x",
			pipeline_index, firstGroup, groupCount, capture_handle_size, h0, h1);
	}
	else
	{
		DLOG2("Ray tracing capture handles: pipeline=%u firstGroup=%u groups=%u handleSize=%u",
			pipeline_index, firstGroup, groupCount, capture_handle_size);
	}
	store_raytracing_handles(pipeline_data, firstGroup, groupCount, capture_handle_size, pData, false);

	if (!wrap_vkGetPhysicalDeviceProperties2 || cb.reader.physicalDevice == VK_NULL_HANDLE) return;
	VkPhysicalDeviceRayTracingPipelinePropertiesKHR props = { VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_RAY_TRACING_PIPELINE_PROPERTIES_KHR, nullptr };
	VkPhysicalDeviceProperties2 props2 = { VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_PROPERTIES_2, &props };
	wrap_vkGetPhysicalDeviceProperties2(cb.reader.physicalDevice, &props2);
	if (props.shaderGroupHandleSize == 0 || !wrap_vkGetRayTracingShaderGroupHandlesKHR) return;
	const size_t replay_size = (size_t)groupCount * props.shaderGroupHandleSize;
	std::vector<std::byte> replay_data(replay_size);
	VkResult result = wrap_vkGetRayTracingShaderGroupHandlesKHR(device, pipeline, firstGroup, groupCount, replay_size, replay_data.data());
	if (result != VK_SUCCESS)
	{
		DLOG("vkGetRayTracingShaderGroupHandlesKHR replay query failed: %s", errorString(result));
		return;
	}
	store_raytracing_handles_replay(pipeline_data, firstGroup, groupCount, props.shaderGroupHandleSize, replay_data.data());
}

static void replay_track_vkGetRayTracingCaptureReplayShaderGroupHandlesKHR(callback_context& cb, VkDevice device, VkPipeline pipeline, uint32_t firstGroup,
	uint32_t groupCount, size_t dataSize, void* pData)
{
	if (!cb.reader.run || !pData || groupCount == 0 || dataSize == 0) return;
	if (dataSize % groupCount != 0)
	{
		DLOG("vkGetRayTracingCaptureReplayShaderGroupHandlesKHR dataSize=%zu not a multiple of groupCount=%u", dataSize, groupCount);
		return;
	}
	const uint32_t pipeline_index = index_to_VkPipeline.index(pipeline);
	if (pipeline_index == CONTAINER_INVALID_INDEX) return;
	trackedpipeline& pipeline_data = VkPipeline_index.at(pipeline_index);

	const uint32_t capture_handle_size = (uint32_t)(dataSize / groupCount);
	store_raytracing_handles(pipeline_data, firstGroup, groupCount, capture_handle_size, pData, true);
}

static int find_raytracing_group_index(const trackedpipeline& pipeline_data, const std::byte* handle_ptr)
{
	const uint32_t handle_size = pipeline_data.raytracing_group_handle_size;
	if (handle_size == 0 || pipeline_data.raytracing_group_handles.empty()) return -1;
	uint32_t group_count = pipeline_data.raytracing_group_count;
	const size_t handle_capacity = pipeline_data.raytracing_group_handles.size();
	const size_t max_groups = handle_capacity / handle_size;
	if (max_groups == 0) return -1;
	if (group_count == 0 || group_count > max_groups) group_count = (uint32_t)max_groups;
	const std::byte* base = pipeline_data.raytracing_group_handles.data();
	for (uint32_t i = 0; i < group_count; i++)
	{
		const std::byte* candidate = base + (size_t)i * handle_size;
		if (memcmp(candidate, handle_ptr, handle_size) == 0) return (int)i;
	}
	return -1;
}

static bool fixup_sbt_region(lava_file_reader& reader, const trackeddevice& device_data, VkDevice device, trackedpipeline& pipeline_data,
	const VkStridedDeviceAddressRegionKHR* region, const char* label)
{
	if (!region || region->deviceAddress == 0 || region->size == 0) return false;
	const uint32_t capture_handle_size = pipeline_data.raytracing_group_handle_size;
	const uint32_t replay_handle_size = pipeline_data.raytracing_group_handle_size_replay;
	if (capture_handle_size == 0 || replay_handle_size == 0) return false;
	if (replay_handle_size > region->stride)
	{
		ABORT("Ray tracing %s SBT stride=%llu is smaller than replay handle size=%u",
			label, (unsigned long long)region->stride, replay_handle_size);
	}
	if (replay_handle_size > capture_handle_size)
	{
		ABORT("Ray tracing %s replay handle size=%u larger than capture handle size=%u",
			label, replay_handle_size, capture_handle_size);
	}
	if (region->stride == 0)
	{
		ABORT("Ray tracing %s SBT stride is zero", label);
	}
	const uint32_t record_count = (uint32_t)(region->size / region->stride);
	if (record_count == 0) return false;

	VkDeviceSize buffer_offset = 0;
	trackedbuffer* buffer_data = find_buffer_by_replay_address(region->deviceAddress, region->size, buffer_offset);
	if (!buffer_data)
	{
		ABORT("Ray tracing %s SBT address 0x%llx is not mapped to a buffer", label, (unsigned long long)region->deviceAddress);
	}
	suballoc_location loc = device_data.allocator->find_buffer_memory(buffer_data->index);
	char* mapped = mem_map(reader, device, loc);
	char* base = mapped + buffer_offset;

	bool updated = false;
	uint32_t matched = 0;
	uint32_t patched = 0;
	bool logged_mismatch = false;
	for (uint32_t i = 0; i < record_count; i++)
	{
		std::byte* record = reinterpret_cast<std::byte*>(base + (size_t)i * region->stride);
		const int group_index = find_raytracing_group_index(pipeline_data, record);
		if (group_index < 0)
		{
			if (!logged_mismatch && pipeline_data.raytracing_group_handles.size() >= 8 && capture_handle_size >= 8)
			{
				uint32_t r0 = 0;
				uint32_t r1 = 0;
				uint32_t c0 = 0;
				uint32_t c1 = 0;
				memcpy(&r0, record, sizeof(uint32_t));
				memcpy(&r1, record + sizeof(uint32_t), sizeof(uint32_t));
				memcpy(&c0, pipeline_data.raytracing_group_handles.data(), sizeof(uint32_t));
				memcpy(&c1, pipeline_data.raytracing_group_handles.data() + sizeof(uint32_t), sizeof(uint32_t));
				DLOG2("Ray tracing %s SBT first record mismatch: record=0x%08x%08x capture=0x%08x%08x",
					label, r0, r1, c0, c1);
				logged_mismatch = true;
			}
			continue;
		}
		matched++;
		const size_t replay_offset = (size_t)group_index * replay_handle_size;
		if (replay_offset + replay_handle_size > pipeline_data.raytracing_group_handles_replay.size())
		{
			mem_unmap(reader, device, loc, nullptr, mapped);
			ABORT("Ray tracing %s replay handle lookup out of bounds", label);
		}
		memcpy(record, pipeline_data.raytracing_group_handles_replay.data() + replay_offset, replay_handle_size);
		updated = true;
		patched++;
	}

	mem_unmap(reader, device, loc, nullptr, mapped);
	if (updated)
	{
		DLOG2("Ray tracing %s SBT patched %u/%u records (buffer=%u offset=%llu)",
			label, patched, record_count, buffer_data->index, (unsigned long long)buffer_offset);
	}
	else
	{
		DLOG2("Ray tracing %s SBT has no matching handles (records=%u, buffer=%u)", label, record_count, buffer_data->index);
	}
	return updated;
}

static void replay_fixup_commandbuffer_raytracing_sbt(lava_file_reader& reader, trackedcmdbuffer& commandbuffer_data)
{
	if (commandbuffer_data.raytracing_sbt_uses.empty()) return;
	const auto& device_data = VkDevice_index.at(commandbuffer_data.device_index);
	const VkDevice device = commandbuffer_data.device;

	for (const auto& use : commandbuffer_data.raytracing_sbt_uses)
	{
		if (use.pipeline_index == CONTAINER_INVALID_INDEX) continue;
		trackedpipeline& pipeline_data = VkPipeline_index.at(use.pipeline_index);
		if (pipeline_data.raytracing_group_handles_capture_replay) continue;
		if (pipeline_data.raytracing_group_handle_size == 0 || pipeline_data.raytracing_group_handles.empty() ||
			pipeline_data.raytracing_group_handle_size_replay == 0 || pipeline_data.raytracing_group_handles_replay.empty())
		{
			DLOG2("Ray tracing SBT fixup skipped: missing shader group handles for pipeline %u", pipeline_data.index);
			continue;
		}
		DLOG2("Ray tracing SBT fixup: pipeline=%u capture_handle=%u replay_handle=%u groups=%u",
			pipeline_data.index, pipeline_data.raytracing_group_handle_size, pipeline_data.raytracing_group_handle_size_replay,
			pipeline_data.raytracing_group_count);
		fixup_sbt_region(reader, device_data, device, pipeline_data, &use.raygen, "raygen");
		fixup_sbt_region(reader, device_data, device, pipeline_data, &use.miss, "miss");
		fixup_sbt_region(reader, device_data, device, pipeline_data, &use.hit, "hit");
		fixup_sbt_region(reader, device_data, device, pipeline_data, &use.callable, "callable");
	}
}

static void replay_fixup_commandbuffer_raytracing_instances(lava_file_reader& reader, trackedcmdbuffer& commandbuffer_data)
{
	if (!reader.run || commandbuffer_data.raytracing_instance_uses.empty()) return;
	const auto& device_data = VkDevice_index.at(commandbuffer_data.device_index);
	const VkDevice device = commandbuffer_data.device;

	for (const auto& use : commandbuffer_data.raytracing_instance_uses)
	{
		if (use.device_address == 0 || use.primitive_count == 0) continue;
		const VkDeviceAddress base_address = use.device_address + use.primitive_offset;
		const VkDeviceSize total_size = (VkDeviceSize)use.primitive_count * sizeof(VkAccelerationStructureInstanceKHR);
		VkDeviceSize buffer_offset = 0;
		trackedbuffer* buffer_data = find_buffer_by_replay_address(base_address, total_size, buffer_offset);
		if (!buffer_data)
		{
			ABORT("Acceleration structure instance buffer address 0x%llx is not mapped to a buffer",
				(unsigned long long)base_address);
		}

		suballoc_location loc = device_data.allocator->find_buffer_memory(buffer_data->index);
		char* mapped = mem_map(reader, device, loc);
		char* base = mapped;
		VkAccelerationStructureInstanceKHR* instance_data = reinterpret_cast<VkAccelerationStructureInstanceKHR*>(base + buffer_offset);

		bool updated = false;
		for (uint32_t idx = 0; idx < use.primitive_count; idx++)
		{
			const VkDeviceAddress current = instance_data[idx].accelerationStructureReference;
			if (current == 0) continue;
			const VkDeviceAddress translated = reader.parent->acceleration_structure_address_remapping.translate_address(current);
			if (translated != 0)
			{
				if (translated != current)
				{
					instance_data[idx].accelerationStructureReference = translated;
					updated = true;
				}
				continue;
			}
			if (!is_known_replay_as_address(current))
			{
				mem_unmap(reader, device, loc, nullptr, mapped);
				ABORT("Acceleration structure instance reference 0x%llx is not remapped for replay",
					(unsigned long long)current);
			}
		}

		mem_unmap(reader, device, loc, nullptr, mapped);
		if (updated)
		{
			DLOG("Remapped %u acceleration structure instance references", (unsigned)use.primitive_count);
		}
	}
}

static void replay_fixup_vkCmdTraceRaysKHR(callback_context& cb, VkCommandBuffer commandBuffer, const VkStridedDeviceAddressRegionKHR* pRaygenShaderBindingTable,
	const VkStridedDeviceAddressRegionKHR* pMissShaderBindingTable, const VkStridedDeviceAddressRegionKHR* pHitShaderBindingTable,
	const VkStridedDeviceAddressRegionKHR* pCallableShaderBindingTable, uint32_t width, uint32_t height, uint32_t depth)
{
	(void)width;
	(void)height;
	(void)depth;
	if (!cb.reader.run) return;
	const uint32_t commandbuffer_index = index_to_VkCommandBuffer.index(commandBuffer);
	if (commandbuffer_index == CONTAINER_INVALID_INDEX) return;
	trackedcmdbuffer& commandbuffer_data = VkCommandBuffer_index.at(commandbuffer_index);
	if (commandbuffer_data.bound_raytracing_pipeline_index == CONTAINER_INVALID_INDEX) return;
	trackedcmdbuffer::raytracing_sbt_use use = {};
	use.pipeline_index = commandbuffer_data.bound_raytracing_pipeline_index;
	if (pRaygenShaderBindingTable) use.raygen = *pRaygenShaderBindingTable;
	if (pMissShaderBindingTable) use.miss = *pMissShaderBindingTable;
	if (pHitShaderBindingTable) use.hit = *pHitShaderBindingTable;
	if (pCallableShaderBindingTable) use.callable = *pCallableShaderBindingTable;
	commandbuffer_data.raytracing_sbt_uses.push_back(use);
}

static void replay_fixup_vkCmdTraceRaysIndirectKHR(callback_context& cb, VkCommandBuffer commandBuffer, const VkStridedDeviceAddressRegionKHR* pRaygenShaderBindingTable,
	const VkStridedDeviceAddressRegionKHR* pMissShaderBindingTable, const VkStridedDeviceAddressRegionKHR* pHitShaderBindingTable,
	const VkStridedDeviceAddressRegionKHR* pCallableShaderBindingTable, VkDeviceAddress indirectDeviceAddress)
{
	(void)indirectDeviceAddress;
	replay_fixup_vkCmdTraceRaysKHR(cb, commandBuffer, pRaygenShaderBindingTable, pMissShaderBindingTable, pHitShaderBindingTable, pCallableShaderBindingTable, 0, 0, 0);
}

static void replay_fixup_vkCmdTraceRaysIndirect2KHR(callback_context& cb, VkCommandBuffer commandBuffer, VkDeviceAddress indirectDeviceAddress)
{
	(void)commandBuffer;
	(void)indirectDeviceAddress;
	if (!cb.reader.run) return;
	DLOG("Ray tracing indirect2 SBT fixup not implemented");
}

void replay_pre_vkCreateSampler(lava_file_reader& reader, VkDevice device, VkSamplerCreateInfo* pCreateInfo, const VkAllocationCallbacks* pAllocator, VkSampler* pSampler)
{
	if (no_anisotropy())
	{
		pCreateInfo->anisotropyEnable = VK_FALSE;
	}
}

void replay_pre_vkCreatePipelineCache(lava_file_reader& reader, VkDevice device, uint32_t device_index, VkPipelineCacheCreateInfo* pCreateInfo, uint32_t cache_index)
{
	if (pCreateInfo->initialDataSize > 0)
	{
		ABORT("vkCreatePipelineCache %u already has a pipeline cache! This should never happen!", cache_index);
	}
	const char* load = load_pipelinecache();
	if (load)
	{
		std::string filename = std::string(load) + "/" + _to_string(cache_index) + ".cache";
		FILE* fp = fopen(filename.c_str(), "r");
		if (fp)
		{
			struct stat st;
			fstat(fileno(fp), &st);
			char* ptr = reader.pool.allocate<char>(st.st_size);
			size_t size = fread(ptr, st.st_size, 1, fp);
			if (size != 1) ELOG("Failed to read pipeline cache from %s: %s", filename.c_str(), strerror(errno));
			fclose(fp);
			pCreateInfo->pInitialData = ptr;
			pCreateInfo->initialDataSize = st.st_size;
			ILOG("Using pipeline cache %u from %s", cache_index, filename.c_str());
		}
		else
		{
			ELOG("Failed to open pipeline cache file %s: %s", filename.c_str(), strerror(errno));
		}
	}
}

void replay_pre_vkDestroyPipelineCache(lava_file_reader& reader, VkDevice device, VkPipelineCache pipelineCache, const VkAllocationCallbacks* pAllocator)
{
	const uint32_t device_index = index_to_VkDevice.index(device);
	const uint32_t cache_index = index_to_VkPipelineCache.index(pipelineCache);
	const char* savedir = save_pipelinecache();
	if (savedir && pipelineCache != VK_NULL_HANDLE)
	{
		mkdir(savedir, 0777); // just in case
		size_t dataSize = 0;
		wrap_vkGetPipelineCacheData(device, pipelineCache, &dataSize, nullptr);
		if (dataSize > 0)
		{
			std::vector<char> data(dataSize);
			wrap_vkGetPipelineCacheData(device, pipelineCache, &dataSize, data.data());
			std::string filename = std::string(savedir) + "/" + _to_string(cache_index) + ".cache";
			FILE* fp = fopen(filename.c_str(), "w");
			if (fp)
			{
				size_t written = fwrite(data.data(), data.size(), 1, fp);
				if (written != 1) ELOG("Failed to write pipeline cache to %s: %s", filename.c_str(), strerror(errno));
				fclose(fp);
			}
			else
			{
				ELOG("Failed to open pipeline cache file %s: %s", filename.c_str(), strerror(errno));
			}
		}
		else
		{
			ELOG("Wanted to save it, but pipeline cache %u is empty!", cache_index);
		}
	}
}

void retrace_vkDestroySurfaceKHR(lava_file_reader& reader)
{
	// Declarations
	VkSurfaceKHR surface;
	// Load
	const uint32_t instance_index = reader.read_handle(DEBUGPARAM("VkInstance"));
	VkInstance instance = index_to_VkInstance.at(instance_index);
	const uint8_t surface_opt = reader.read_uint8_t(); // whether we should load this optional value
	uint32_t surface_index = 0;
	if (surface_opt)
	{
		surface_index = reader.read_handle(DEBUGPARAM("VkSurfaceKHR"));
		surface = index_to_VkSurfaceKHR.at(surface_index);
		if (!is_noscreen() && reader.run)
		{
			VkAllocationCallbacks allocator = {};
			VkAllocationCallbacks* pAllocator = &allocator;
			allocators_set(pAllocator);
			wrap_vkDestroySurfaceKHR(instance, surface, pAllocator);
			window_destroy(instance, surface_index);
		}
		index_to_VkSurfaceKHR.unset(surface_index);
	}
}

static void replay_pre_vkDestroySwapchainKHR(lava_file_reader& reader, VkDevice device, VkSwapchainKHR swapchain, const VkAllocationCallbacks* pAllocator)
{
	trackedswapchain_replay& t = VkSwapchainKHR_index.at(index_to_VkSwapchainKHR.index(swapchain));
	assert(device == t.device);
	for (unsigned i = 0; i < t.virtual_fences.size(); i++)
	{
		if (!t.inflight.at(i)) continue;
		// check status, wait if needed, then delete
		VkResult r = wrap_vkGetFenceStatus(device, t.virtual_fences.at(i));
		if (r == VK_NOT_READY)
		{
			r = wrap_vkWaitForFences(device, 1, &t.virtual_fences.at(i), VK_TRUE, UINT64_MAX);
		}
		wrap_vkDestroyFence(device, t.virtual_fences.at(i), nullptr);
		wrap_vkDestroyImage(device, t.virtual_images.at(i), nullptr);
	}
	if (t.virtual_cmdpool != VK_NULL_HANDLE) wrap_vkResetCommandPool(device, t.virtual_cmdpool, VK_COMMAND_POOL_RESET_RELEASE_RESOURCES_BIT);
	if (t.virtual_cmdpool != VK_NULL_HANDLE) wrap_vkFreeCommandBuffers(device, t.virtual_cmdpool, t.virtual_cmdbuffers.size(), t.virtual_cmdbuffers.data());
	wrap_vkDestroyCommandPool(device, t.virtual_cmdpool, nullptr);
	wrap_vkDestroySemaphore(device, t.virtual_semaphore, nullptr);
}

static void replay_post_vkSubmitDebugUtilsMessageEXT(lava_file_reader& reader,
    VkInstance                                  instance,
    VkDebugUtilsMessageSeverityFlagBitsEXT      messageSeverity,
    VkDebugUtilsMessageTypeFlagsEXT             messageTypes,
    const VkDebugUtilsMessengerCallbackDataEXT* pCallbackData)
{
	if (!pCallbackData) return;
	if (pCallbackData->pObjects && pCallbackData->objectCount > 0 && pCallbackData->pMessage)
	{
		trackable& t = object_trackable(pCallbackData->pObjects[0].objectType, pCallbackData->pObjects[0].objectHandle);
		DLOG("Marker for %s[%d]: " MAKEBLUE("%s"), pretty_print_VkObjectType(pCallbackData->pObjects[0].objectType), (int)t.index, pCallbackData->pMessage);
	}
	else if (pCallbackData->pMessage)
	{
		DLOG("Marker:" MAKEBLUE("%s"), pCallbackData->pMessage);
	}
}

static void replay_post_vkGetAccelerationStructureDeviceAddressKHR(lava_file_reader& reader, VkDeviceAddress result, VkDevice device, const VkAccelerationStructureDeviceAddressInfoKHR* pInfo)
{
	const uint32_t as_index = index_to_VkAccelerationStructureKHR.index(pInfo->accelerationStructure);
	VkAccelerationStructureKHR_index.at(as_index).device_address = result;
}

static void replay_post_vkGetBufferDeviceAddressKHR(lava_file_reader& reader, VkDeviceAddress result, VkDevice device, const VkBufferDeviceAddressInfoKHR* pInfo)
{
	const uint32_t buffer_index = index_to_VkBuffer.index(pInfo->buffer);
	VkBuffer_index.at(buffer_index).device_address = result;
}

static void replay_post_vkGetBufferDeviceAddressEXT(lava_file_reader& reader, VkDeviceAddress result, VkDevice device, const VkBufferDeviceAddressInfo* pInfo)
{
	const uint32_t buffer_index = index_to_VkBuffer.index(pInfo->buffer);
	VkBuffer_index.at(buffer_index).device_address = result;
}

static void replay_post_vkGetBufferDeviceAddress(lava_file_reader& reader, VkDeviceAddress result, VkDevice device, const VkBufferDeviceAddressInfo* pInfo)
{
	const uint32_t buffer_index = index_to_VkBuffer.index(pInfo->buffer);
	VkBuffer_index.at(buffer_index).device_address = result;
}

static void replay_post_vkGetAccelerationStructureBuildSizesKHR(lava_file_reader& reader, VkDevice device,
	VkAccelerationStructureBuildTypeKHR buildType, const VkAccelerationStructureBuildGeometryInfoKHR* pBuildInfo,
	const uint32_t* pMaxPrimitiveCounts, VkAccelerationStructureBuildSizesInfoKHR* pSizeInfo)
{
	if (!reader.run || !pBuildInfo || !pMaxPrimitiveCounts) return;
	VkAccelerationStructureBuildSizesInfoKHR sizes = { VK_STRUCTURE_TYPE_ACCELERATION_STRUCTURE_BUILD_SIZES_INFO_KHR, nullptr };
	wrap_vkGetAccelerationStructureBuildSizesKHR(device, buildType, pBuildInfo, pMaxPrimitiveCounts, &sizes);
	reader.pending_as_build_sizes.push_back(sizes);
}

static void replay_post_vkCreateBuffer(lava_file_reader& reader, VkResult result, VkDevice device, const VkBufferCreateInfo* pCreateInfo, const VkAllocationCallbacks* pAllocator, VkBuffer* pBuffer)
{
	if (result != VK_SUCCESS || !pBuffer || *pBuffer == VK_NULL_HANDLE) return;
	const uint32_t buffer_index = index_to_VkBuffer.index(*pBuffer);
	auto& buf = VkBuffer_index.at(buffer_index);
	if (buf.capture_device_address == 0) return;
	reader.parent->device_address_remapping.add(buf.capture_device_address, &buf);
}

static void replay_post_vkCreateDescriptorUpdateTemplate(lava_file_reader& reader, VkResult result, VkDevice device,
	const VkDescriptorUpdateTemplateCreateInfo* pCreateInfo, const VkAllocationCallbacks* pAllocator, VkDescriptorUpdateTemplate* pDescriptorUpdateTemplate)
{
	(void)device;
	(void)pAllocator;
	if (result != VK_SUCCESS || !pCreateInfo || !pDescriptorUpdateTemplate || *pDescriptorUpdateTemplate == VK_NULL_HANDLE) return;
	const uint32_t template_index = index_to_VkDescriptorUpdateTemplate.index(*pDescriptorUpdateTemplate);
	auto& data = VkDescriptorUpdateTemplate_index.at(template_index);
	data.entries.clear();
	if (pCreateInfo->descriptorUpdateEntryCount && pCreateInfo->pDescriptorUpdateEntries)
	{
		data.entries.assign(pCreateInfo->pDescriptorUpdateEntries, pCreateInfo->pDescriptorUpdateEntries + pCreateInfo->descriptorUpdateEntryCount);
	}
	data.type = pCreateInfo->templateType;
	data.flags = pCreateInfo->flags;
	const uint64_t computed_size = descriptor_update_template_data_size(pCreateInfo);
	if (computed_size != 0 || data.data_size == 0) data.data_size = computed_size;
}

static void replay_post_vkCreateDescriptorUpdateTemplateKHR(lava_file_reader& reader, VkResult result, VkDevice device,
	const VkDescriptorUpdateTemplateCreateInfo* pCreateInfo, const VkAllocationCallbacks* pAllocator, VkDescriptorUpdateTemplate* pDescriptorUpdateTemplate)
{
	replay_post_vkCreateDescriptorUpdateTemplate(reader, result, device, pCreateInfo, pAllocator, pDescriptorUpdateTemplate);
}

static void replay_pre_vkCreateAccelerationStructureKHR(lava_file_reader& reader, VkDevice device,
	VkAccelerationStructureCreateInfoKHR* pCreateInfo, const VkAllocationCallbacks* pAllocator, VkAccelerationStructureKHR* pAccelerationStructure)
{
	if (!reader.run || !pCreateInfo || device == VK_NULL_HANDLE) return;
	VkAccelerationStructureBuildSizesInfoKHR sizes = { VK_STRUCTURE_TYPE_ACCELERATION_STRUCTURE_BUILD_SIZES_INFO_KHR, nullptr };
	bool have_sizes = false;
	{
		if (!reader.pending_as_build_sizes.empty())
		{
			sizes = reader.pending_as_build_sizes.front();
			reader.pending_as_build_sizes.pop_front();
			have_sizes = true;
		}
	}

	VkDeviceSize required_size = pCreateInfo->size;
	if (have_sizes && sizes.accelerationStructureSize > required_size)
	{
		required_size = sizes.accelerationStructureSize;
		pCreateInfo->size = required_size;
	}

	internal_buffer replacement;
	if (pCreateInfo->buffer != VK_NULL_HANDLE)
	{
		const uint32_t buffer_index = index_to_VkBuffer.index(pCreateInfo->buffer);
		if (buffer_index != CONTAINER_INVALID_INDEX)
		{
			trackedbuffer& buf = VkBuffer_index.at(buffer_index);
			const VkDeviceSize needed_size = pCreateInfo->offset + required_size;
			if (buf.size < needed_size)
			{
				const VkBufferUsageFlags usage = buf.usage |
					VK_BUFFER_USAGE_ACCELERATION_STRUCTURE_STORAGE_BIT_KHR |
					VK_BUFFER_USAGE_SHADER_DEVICE_ADDRESS_BIT;
				const VkMemoryPropertyFlags memory_flags = (buf.memory_flags != 0) ? buf.memory_flags : VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT;
				const VkPhysicalDevice physical_device = reader.physicalDevice != VK_NULL_HANDLE ? reader.physicalDevice : selected_physical_device;
				if (!create_internal_buffer(device, physical_device, needed_size, usage, memory_flags, replacement))
				{
					ABORT("Failed to allocate replacement buffer for acceleration structure (size=%lu)", (unsigned long)needed_size);
				}
				pCreateInfo->buffer = replacement.buffer;
				pCreateInfo->offset = 0;
			}
		}
	}

	{
		reader.pending_as_storage_buffers.push_back(replacement);
	}
}

static void replay_pre_vkDestroyBuffer(lava_file_reader& reader, VkDevice device, VkBuffer buffer, const VkAllocationCallbacks* pAllocator)
{
	if (buffer == VK_NULL_HANDLE) return;
	const uint32_t buffer_index = index_to_VkBuffer.index(buffer);
	auto& buf = VkBuffer_index.at(buffer_index);
	if (buf.capture_device_address == 0) return;
	reader.parent->device_address_remapping.remove(buf.capture_device_address, &buf);
}

static void replay_post_vkCreateAccelerationStructureKHR(lava_file_reader& reader, VkResult result, VkDevice device, const VkAccelerationStructureCreateInfoKHR* pCreateInfo, const VkAllocationCallbacks* pAllocator, VkAccelerationStructureKHR* pAccelerationStructure)
{
	internal_buffer replacement;
	bool has_replacement = false;
	{
		if (!reader.pending_as_storage_buffers.empty())
		{
			replacement = reader.pending_as_storage_buffers.front();
			reader.pending_as_storage_buffers.pop_front();
			has_replacement = (replacement.buffer != VK_NULL_HANDLE);
		}
	}

	if (result != VK_SUCCESS || !pAccelerationStructure || *pAccelerationStructure == VK_NULL_HANDLE)
	{
		if (reader.run && has_replacement) destroy_internal_buffer(device, replacement);
		return;
	}
	const uint32_t as_index = index_to_VkAccelerationStructureKHR.index(*pAccelerationStructure);
	auto& as = VkAccelerationStructureKHR_index.at(as_index);
	if (has_replacement)
	{
		as.replay_storage = replacement;
		as.size = pCreateInfo ? pCreateInfo->size : as.size;
		as.offset = pCreateInfo ? pCreateInfo->offset : as.offset;
		as.buffer = replacement.buffer;
		as.buffer_index = CONTAINER_INVALID_INDEX;
	}
	if (as.capture_device_address == 0) return;
	reader.parent->acceleration_structure_address_remapping.add(as.capture_device_address, &as);
}

static void replay_pre_vkDestroyAccelerationStructureKHR(lava_file_reader& reader, VkDevice device, VkAccelerationStructureKHR accelerationStructure, const VkAllocationCallbacks* pAllocator)
{
	if (accelerationStructure == VK_NULL_HANDLE) return;
	const uint32_t as_index = index_to_VkAccelerationStructureKHR.index(accelerationStructure);
	auto& as = VkAccelerationStructureKHR_index.at(as_index);
	if (reader.run && as.replay_storage.buffer != VK_NULL_HANDLE)
	{
		destroy_internal_buffer(device, as.replay_storage);
	}
	if (as.capture_device_address == 0) return;
	reader.parent->acceleration_structure_address_remapping.remove(as.capture_device_address, &as);
}

void replay_post_vkAcquireNextImageKHR(lava_file_reader& reader, VkResult result, VkDevice device, VkSwapchainKHR swapchain, uint64_t timeout,
                                     VkSemaphore semaphore, VkFence fence, uint32_t* pImageIndex)
{
	if (result == VK_INCOMPLETE) return; // means we skipped it
	const uint32_t swapchainkhr_index = index_to_VkSwapchainKHR.index(swapchain);
	const uint32_t next_image = VkSwapchainKHR_index.at(swapchainkhr_index).next_swapchain_image;
	DLOG("Acquired next swapchain image index=%u (stored next image was %u), returned %s", next_image, *pImageIndex, errorString(result));
	assert(is_virtualswapchain() || next_image == *pImageIndex);
}

void replay_post_vkAcquireNextImage2KHR(lava_file_reader& reader, VkResult result, VkDevice device, const VkAcquireNextImageInfoKHR* pAcquireInfo, uint32_t* pImageIndex)
{
	if (result == VK_INCOMPLETE) return; // means we skipped it
	const uint32_t swapchainkhr_index = index_to_VkSwapchainKHR.index(pAcquireInfo->swapchain);
	const uint32_t next_image = VkSwapchainKHR_index.at(swapchainkhr_index).next_swapchain_image;
	DLOG("Acquired next swapchain image index=%u (stored next image was %u), returned %s", next_image, *pImageIndex, errorString(result));
	assert(is_virtualswapchain() || next_image == *pImageIndex);
}

// make or remake swapchain images
static VkSwapchainKHR remake_swapchain(lava_file_reader& reader, VkQueue queue, VkSwapchainKHR old_swapchain, trackedswapchain_replay* data)
{
	assert(reader.run);
	// TBD check surface capabilities, these values may not be supported
	VkSwapchainCreateInfoKHR s = { VK_STRUCTURE_TYPE_SWAPCHAIN_CREATE_INFO_KHR, nullptr };
	s.flags = data->info.flags;
	s.flags &= ~VK_SWAPCHAIN_CREATE_DEFERRED_MEMORY_ALLOCATION_BIT_KHR; // disable lazy swapchain image allocation
	s.surface = data->info.surface;
	if (p__realimages > 0) s.minImageCount = p__realimages;
	else s.minImageCount = data->info.minImageCount;
	s.imageFormat = data->info.imageFormat;
	s.imageColorSpace = data->info.imageColorSpace;
	s.imageExtent = data->info.imageExtent;
	s.imageArrayLayers = data->info.imageArrayLayers;
	s.imageUsage = data->info.imageUsage;
	s.imageSharingMode = data->info.imageSharingMode;
	s.queueFamilyIndexCount = 0;
	s.pQueueFamilyIndices = nullptr;
	s.preTransform = data->info.preTransform;
	s.compositeAlpha = data->info.compositeAlpha;
	s.presentMode = data->info.presentMode;
	s.clipped = data->info.clipped;
	s.oldSwapchain = old_swapchain;
	// make new one
	VkSwapchainKHR swapchain;
	VkResult r = wrap_vkCreateSwapchainKHR(data->device, &s, nullptr, &swapchain);
	assert(r == VK_SUCCESS);
	// delete old one
	const uint32_t old_swapchainkhr_index = index_to_VkSwapchainKHR.index(old_swapchain);
	wrap_vkDestroySwapchainKHR(data->device, old_swapchain, nullptr);
	// replace old->new
	index_to_VkSwapchainKHR.replace(old_swapchainkhr_index, swapchain);
	// replace swapchain images
	uint32_t swapchainImageCount = 0;
	r = wrap_vkGetSwapchainImagesKHR(data->device, swapchain, &swapchainImageCount, nullptr);
	data->pSwapchainImages.resize(swapchainImageCount);
	r = wrap_vkGetSwapchainImagesKHR(data->device, swapchain, &swapchainImageCount, data->pSwapchainImages.data());
	assert(r == VK_SUCCESS);
	(void)r;
	return swapchain;
}

void replay_pre_vkQueuePresentKHR(lava_file_reader& reader, VkQueue queue, VkPresentInfoKHR* pPresentInfo)
{
	if (pPresentInfo->pResults == nullptr) // we always want this info back
	{
		pPresentInfo->pResults = reader.pool.allocate<VkResult>(pPresentInfo->swapchainCount);
	}
	if (is_virtualswapchain())
	{
		VkResult result;
		VkSemaphore* semaphores = reader.pool.allocate<VkSemaphore>(pPresentInfo->waitSemaphoreCount + pPresentInfo->swapchainCount);
		for (uint32_t i = 0; i < pPresentInfo->waitSemaphoreCount; i++) semaphores[i] = pPresentInfo->pWaitSemaphores[i];
		for (uint32_t i = 0; i < pPresentInfo->swapchainCount; i++)
		{
			const uint32_t swapchainkhr_index = index_to_VkSwapchainKHR.index(pPresentInfo->pSwapchains[i]);
			auto& data = VkSwapchainKHR_index.at(swapchainkhr_index);
			// check that commandbuffer is actually available (this SHOULD be a noop!)
			if (data.inflight.at(data.next_stored_image))
			{
				result = wrap_vkWaitForFences(data.device, 1, &data.virtual_fences[data.next_stored_image], VK_TRUE, 0);
				if (result != VK_SUCCESS)
				{
					ELOG("Fence was not ready for commandbuffer %u", data.next_stored_image);
					result = wrap_vkWaitForFences(data.device, 1, &data.virtual_fences[data.next_stored_image], VK_TRUE, UINT64_MAX);
					assert(result == VK_SUCCESS);
				}
				result = wrap_vkResetFences(data.device, 1, &data.virtual_fences[data.next_stored_image]);
				assert(result == VK_SUCCESS);
			}
			data.inflight[data.next_stored_image] = true; // now using it, if we weren't already
			// copy virtual -> real
			semaphores[pPresentInfo->waitSemaphoreCount + i] = data.virtual_semaphore;
			VkCommandBufferBeginInfo command_buffer_begin_info = {};
			command_buffer_begin_info.sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_BEGIN_INFO;
			result = wrap_vkBeginCommandBuffer(data.virtual_cmdbuffers[data.next_stored_image], &command_buffer_begin_info);
			assert(result == VK_SUCCESS);
			std::vector<VkImageMemoryBarrier> image_barriers(2);
			VkImageMemoryBarrier& image_barrier_src = image_barriers.at(0);
			VkImageMemoryBarrier& image_barrier_dst = image_barriers.at(1);
			image_barrier_src.sType = VK_STRUCTURE_TYPE_IMAGE_MEMORY_BARRIER;
			image_barrier_src.pNext = nullptr;
			image_barrier_dst.sType = VK_STRUCTURE_TYPE_IMAGE_MEMORY_BARRIER;
			image_barrier_dst.pNext = nullptr;
			image_barrier_src.srcAccessMask = VK_ACCESS_TRANSFER_WRITE_BIT;
			image_barrier_src.dstAccessMask = VK_ACCESS_COLOR_ATTACHMENT_WRITE_BIT;
			image_barrier_dst.srcAccessMask = VK_ACCESS_TRANSFER_WRITE_BIT;
			image_barrier_dst.dstAccessMask = VK_ACCESS_COLOR_ATTACHMENT_WRITE_BIT;
			image_barrier_src.oldLayout = VK_IMAGE_LAYOUT_PRESENT_SRC_KHR; // TBD could also be VK_IMAGE_LAYOUT_SHARED_PRESENT_KHR
			image_barrier_src.newLayout = VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL;
			image_barrier_dst.oldLayout = VK_IMAGE_LAYOUT_UNDEFINED;
			image_barrier_dst.newLayout = VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL;
			image_barrier_src.srcQueueFamilyIndex = selected_queue_family_index;
			image_barrier_src.dstQueueFamilyIndex = selected_queue_family_index;
			image_barrier_dst.srcQueueFamilyIndex = selected_queue_family_index;
			image_barrier_dst.dstQueueFamilyIndex = selected_queue_family_index;
			image_barrier_src.image = data.virtual_images[data.next_stored_image];
			image_barrier_dst.image = data.pSwapchainImages[data.next_swapchain_image];
			image_barrier_src.subresourceRange = { VK_IMAGE_ASPECT_COLOR_BIT, 0, VK_REMAINING_MIP_LEVELS, 0, VK_REMAINING_ARRAY_LAYERS };
			image_barrier_dst.subresourceRange = { VK_IMAGE_ASPECT_COLOR_BIT, 0, VK_REMAINING_MIP_LEVELS, 0, VK_REMAINING_ARRAY_LAYERS };
			wrap_vkCmdPipelineBarrier(data.virtual_cmdbuffers[data.next_stored_image], VK_PIPELINE_STAGE_TRANSFER_BIT,
				VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT, 0, 0, NULL, 0, NULL, image_barriers.size(), image_barriers.data());
			if (!p__virtualperfmode) // actually do the copy if not in performance mode
			{
				VkMemoryBarrier memory_barrier = {};
				memory_barrier.sType = VK_STRUCTURE_TYPE_MEMORY_BARRIER;
				memory_barrier.srcAccessMask = VK_ACCESS_TRANSFER_WRITE_BIT;
				memory_barrier.dstAccessMask = VK_ACCESS_COLOR_ATTACHMENT_WRITE_BIT;
				wrap_vkCmdCopyImage(data.virtual_cmdbuffers[data.next_stored_image], data.virtual_images[data.next_stored_image], VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL,
					data.pSwapchainImages[data.next_swapchain_image], VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL, 1, &data.virtual_image_copy_region);
			}
			image_barrier_dst.srcAccessMask = VK_ACCESS_TRANSFER_READ_BIT;
			image_barrier_src.oldLayout = VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL;
			image_barrier_src.newLayout = VK_IMAGE_LAYOUT_PRESENT_SRC_KHR; // TBD could also be VK_IMAGE_LAYOUT_SHARED_PRESENT_KHR
			image_barrier_dst.oldLayout = VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL;
			image_barrier_dst.newLayout = VK_IMAGE_LAYOUT_PRESENT_SRC_KHR; // TBD could also be VK_IMAGE_LAYOUT_SHARED_PRESENT_KHR
			wrap_vkCmdPipelineBarrier(data.virtual_cmdbuffers[data.next_stored_image], VK_PIPELINE_STAGE_TRANSFER_BIT,
				VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT, 0, 0, NULL, 0, NULL, image_barriers.size(), image_barriers.data());
			result = wrap_vkEndCommandBuffer(data.virtual_cmdbuffers[data.next_stored_image]);
			assert(result == VK_SUCCESS);
			DLOG("Presenting with virtual swapchain image index %u(0x%lx) instead of real swapchain image %u(0x%lx) on swapchain %lx", data.next_swapchain_image,
				(unsigned long)data.virtual_images[data.next_stored_image], pPresentInfo->pImageIndices[i], (unsigned long)data.pSwapchainImages[data.next_swapchain_image],
				(unsigned long)pPresentInfo->pSwapchains[i]);
			// replace with virtual swapchain image
			const_cast<uint32_t*>(pPresentInfo->pImageIndices)[i] = data.next_swapchain_image;

			VkSubmitInfo submit = {};
			submit.sType = VK_STRUCTURE_TYPE_SUBMIT_INFO;
			submit.commandBufferCount = 1;
			submit.pCommandBuffers = &data.virtual_cmdbuffers[data.next_stored_image];
			submit.signalSemaphoreCount = 1;
			submit.pSignalSemaphores = &data.virtual_semaphore;
			result = wrap_vkQueueSubmit(queue, 1, &submit, data.virtual_fences[data.next_stored_image]);
			assert(result == VK_SUCCESS);
		}
		pPresentInfo->waitSemaphoreCount = pPresentInfo->waitSemaphoreCount + pPresentInfo->swapchainCount;
		pPresentInfo->pWaitSemaphores = semaphores;
	}
	else
	{
		for (uint32_t i = 0; i < pPresentInfo->swapchainCount; i++)
		{
			DLOG("Presenting with swapchain image index %u on swapchain id %lu", pPresentInfo->pImageIndices[i], (unsigned long)pPresentInfo->pSwapchains[i]);
		}
	}
}

static void cleanup_sync(VkQueue queue, uint32_t waitSemaphoreCount, const VkSemaphore *waitSemaphores, uint32_t signalSemaphoreCount, const VkSemaphore *signalSemaphores, VkFence fence)
{
	const VkPipelineStageFlags flags = VK_PIPELINE_STAGE_BOTTOM_OF_PIPE_BIT;
	VkSubmitInfo submit_info = { VK_STRUCTURE_TYPE_SUBMIT_INFO, nullptr, waitSemaphoreCount, waitSemaphores, &flags, 0, nullptr, signalSemaphoreCount, signalSemaphores };
	VkResult result = wrap_vkQueueSubmit(queue, 1, &submit_info, fence);
	assert(result == VK_SUCCESS);
}

void replay_post_vkQueuePresentKHR(lava_file_reader& reader, VkResult result, VkQueue queue, VkPresentInfoKHR* pPresentInfo)
{
	if (is_virtualswapchain() && (result == VK_SUBOPTIMAL_KHR || result == VK_ERROR_OUT_OF_DATE_KHR))
	{
		ILOG("We got %s from vkQueuePresentKHR -- remaking the swapchain!", errorString(result));

		// first, reset our semaphores
		cleanup_sync(queue, pPresentInfo->waitSemaphoreCount, pPresentInfo->pWaitSemaphores, 0, nullptr, VK_NULL_HANDLE);
		pPresentInfo->waitSemaphoreCount--; // remove the one we added

		// second, redo the affected swapchain(s)
		VkSwapchainKHR* pSwapchains = reader.pool.allocate<VkSwapchainKHR>(pPresentInfo->swapchainCount);
		uint32_t swapchainCount = 0;
		uint32_t* pImageIndices = reader.pool.allocate<uint32_t>(pPresentInfo->swapchainCount);
		for (uint32_t i = 0; i < pPresentInfo->swapchainCount; i++)
		{
			if (pPresentInfo->pResults[i] != VK_SUCCESS)
			{
				const uint32_t swapchainkhr_index = index_to_VkSwapchainKHR.index(pPresentInfo->pSwapchains[i]);
				auto& data = VkSwapchainKHR_index.at(swapchainkhr_index);
				pSwapchains[swapchainCount] = remake_swapchain(reader, queue, pPresentInfo->pSwapchains[i], &data);
				pImageIndices[swapchainCount] = pPresentInfo->pImageIndices[i];
				swapchainCount++;
			}
		}
		pPresentInfo->pSwapchains = pSwapchains;
		pPresentInfo->swapchainCount = swapchainCount;
		pPresentInfo->pImageIndices = pImageIndices;

		// third, reissue call using only the failed swapchains
		assert(swapchainCount > 0);
		replay_pre_vkQueuePresentKHR(reader, queue, pPresentInfo);
		result = wrap_vkQueuePresentKHR(queue, pPresentInfo);
		assert(result == VK_SUCCESS);
	}
	else if (!is_virtualswapchain()) assert(result == VK_SUCCESS || result == VK_SUBOPTIMAL_KHR);
}

void replay_pre_vkCreateSharedSwapchainsKHR(lava_file_reader& reader, VkDevice device, uint32_t swapchainCount, VkSwapchainCreateInfoKHR* pCreateInfos, VkAllocationCallbacks* pAllocator, VkSwapchainKHR* pSwapchains)
{
	if (is_virtualswapchain())
	{
		for (uint32_t i = 0; i < swapchainCount; i++)
		{
			if (p__realpresentmode != VK_PRESENT_MODE_MAX_ENUM_KHR) pCreateInfos[i].presentMode = p__realpresentmode;
			if (p__realimages > 0) pCreateInfos[i].minImageCount = p__realimages;
			pCreateInfos[i].imageUsage |= VK_IMAGE_USAGE_TRANSFER_DST_BIT; // make sure it has this
		}
	}
}

void replay_pre_vkCreateSwapchainKHR(lava_file_reader& reader, VkDevice device, VkSwapchainCreateInfoKHR* pCreateInfo, VkAllocationCallbacks* pAllocator, VkSwapchainKHR* pSwapchain)
{
	if (is_virtualswapchain())
	{
		if (p__realpresentmode != VK_PRESENT_MODE_MAX_ENUM_KHR) pCreateInfo->presentMode = p__realpresentmode;
		if (p__realimages > 0) pCreateInfo->minImageCount = p__realimages;
		pCreateInfo->imageUsage |= VK_IMAGE_USAGE_TRANSFER_DST_BIT; // make sure it has this
	}
}

/// Special function to initialize the physical devices absolutely first.
void replay_initialize_vkCreateDevice(lava_file_reader& reader, uint32_t physicaldevice_index)
{
	(void)physicaldevice_index; // TBD use this later for multi-GPU support
	replay_register_raytracing_callbacks(*reader.parent);

	// Find the physical device we want to create, and point all physical device references to it
	uint32_t num_phys_devices = 0;
	VkResult result = wrap_vkEnumeratePhysicalDevices(stored_instance, &num_phys_devices, nullptr);
	assert(result == VK_SUCCESS);
	assert(p__device < (int)num_phys_devices);
	std::vector<VkPhysicalDevice> physical_devices(num_phys_devices);
	result = wrap_vkEnumeratePhysicalDevices(stored_instance, &num_phys_devices, physical_devices.data());
	assert(result == VK_SUCCESS);
	assert(num_phys_devices == physical_devices.size());
	ILOG("Found %d physical devices - picking:", (int)num_phys_devices);
	assert(selected_physical_device != VK_NULL_HANDLE); // we would have needed a temporary one before we got here
	selected_physical_device = VK_NULL_HANDLE; // now pick the correct one
	for (unsigned i = 0; i < physical_devices.size(); i++)
	{
		VkPhysicalDeviceProperties devprops;
		wrap_vkGetPhysicalDeviceProperties(physical_devices[i], &devprops);
		bool is_cpu = (devprops.deviceType == VK_PHYSICAL_DEVICE_TYPE_CPU || devprops.deviceType == VK_PHYSICAL_DEVICE_TYPE_VIRTUAL_GPU);

		if (is_cpu && p__gpu) continue; // not a GPU and we wanted this, skip it
		else if (!is_cpu && p__cpu) continue; // not a GPU simulation on CPU and we wanted this, skip it
		else if (p__device != -1 && (int)i != (int)p__device) continue; // not the physical device we specified, skip it

		selected_physical_device = physical_devices[i]; // select this one
		ILOG("\t%u : %s (API %u.%u)", i, devprops.deviceName, VK_VERSION_MAJOR(devprops.apiVersion), VK_VERSION_MINOR(devprops.apiVersion));
		break;
	}
	if (selected_physical_device == VK_NULL_HANDLE) DIE("No valid physical device of the requested type found!");

	uint32_t families = 0;
	wrap_vkGetPhysicalDeviceQueueFamilyProperties(selected_physical_device, &families, nullptr);
	device_VkQueueFamilyProperties.resize(families);
	wrap_vkGetPhysicalDeviceQueueFamilyProperties(selected_physical_device, &families, device_VkQueueFamilyProperties.data());
	for (unsigned i = 0; i < device_VkQueueFamilyProperties.size(); i++)
	{
		const VkQueueFamilyProperties& p = device_VkQueueFamilyProperties.at(i);
		if ((p.queueFlags & VK_QUEUE_GRAPHICS_BIT) && (p.queueFlags & VK_QUEUE_COMPUTE_BIT) && (p.queueFlags & VK_QUEUE_TRANSFER_BIT)) selected_queue_family_index = i;
		DLOG("Selected queue family: %d", selected_queue_family_index);
	}
	if (selected_queue_family_index == 0xdeadbeef) ABORT("No valid queue family found!");
}

void replay_pre_vkCreateDevice(lava_file_reader& reader, VkPhysicalDevice physicalDevice, VkDeviceCreateInfo* pCreateInfo, const VkAllocationCallbacks* pAllocator, VkDevice* pDevice)
{
	pCreateInfo->enabledLayerCount = 0; // even though implementation should ignore it as per the spec, that is not always the case, so help it along

	// Limit the number of requested queues to what is available
	VkDeviceQueueCreateInfo* queueinfo = reader.pool.allocate<VkDeviceQueueCreateInfo>(pCreateInfo->queueCreateInfoCount);
	for (uint32_t i = 0; i < pCreateInfo->queueCreateInfoCount; i++)
	{
		queueinfo[i] = pCreateInfo->pQueueCreateInfos[i]; // struct copy
		if (queueinfo[i].queueFamilyIndex == selected_queue_family_index
		    && queueinfo[i].queueCount > device_VkQueueFamilyProperties.at(selected_queue_family_index).queueCount)
		{
			queueinfo[i].queueCount = device_VkQueueFamilyProperties.at(selected_queue_family_index).queueCount;
		}
	}
	pCreateInfo->pQueueCreateInfos = queueinfo;

	// Replace stored features with a pruned feature list
	VkBaseOutStructure *ext = (VkBaseOutStructure*)find_extension_parent(pCreateInfo, VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_VULKAN_1_1_FEATURES);
	if (ext && has_VkPhysicalDeviceVulkan11Features)
	{
		stored_VkPhysicalDeviceVulkan11Features.pNext = ext->pNext->pNext;
		ext->pNext = (VkBaseOutStructure*)&stored_VkPhysicalDeviceVulkan11Features;
	}
	ext = (VkBaseOutStructure*)find_extension_parent(pCreateInfo, VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_VULKAN_1_2_FEATURES);
	if (ext && has_VkPhysicalDeviceVulkan12Features)
	{
		stored_VkPhysicalDeviceVulkan12Features.pNext = ext->pNext->pNext;
		ext->pNext = (VkBaseOutStructure*)&stored_VkPhysicalDeviceVulkan12Features;
	}
	ext = (VkBaseOutStructure*)find_extension_parent(pCreateInfo, VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_VULKAN_1_3_FEATURES);
	if (ext && has_VkPhysicalDeviceVulkan13Features)
	{
		stored_VkPhysicalDeviceVulkan13Features.pNext = ext->pNext->pNext;
		ext->pNext = (VkBaseOutStructure*)&stored_VkPhysicalDeviceVulkan13Features;
	}
	ext = (VkBaseOutStructure*)find_extension_parent(pCreateInfo, VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_FEATURES_2);
	if (ext && has_VkPhysicalDeviceFeatures2)
	{
		stored_VkPhysicalDeviceFeatures2.pNext = ext->pNext->pNext;
		ext->pNext = (VkBaseOutStructure*)&stored_VkPhysicalDeviceFeatures2;
		pCreateInfo->pEnabledFeatures = nullptr;
	}
	else if (has_VkPhysicalDeviceFeatures2) // use the old way, just stored in the new way
	{
		pCreateInfo->pEnabledFeatures = &stored_VkPhysicalDeviceFeatures2.features; // struct copy
	}

	// TBD replace feature extensions

	if (no_anisotropy())
	{
		static VkPhysicalDeviceFeatures backup_features = *pCreateInfo->pEnabledFeatures; // struct copy
		backup_features.samplerAnisotropy = VK_FALSE;
		pCreateInfo->pEnabledFeatures = &backup_features;
	}
}

void replay_pre_vkCreateInstance(lava_file_reader& reader, VkInstanceCreateInfo* pCreateInfo, const VkAllocationCallbacks* pAllocator, VkInstance* pInstance)
{
	VkApplicationInfo* pApplicationInfo = reader.pool.allocate<VkApplicationInfo>(1);
	if (pCreateInfo->pApplicationInfo)
	{
		*pApplicationInfo = *pCreateInfo->pApplicationInfo;
	}
	pApplicationInfo->apiVersion = std::max(VK_API_VERSION_1_3, pApplicationInfo->apiVersion);
	pCreateInfo->pApplicationInfo = pApplicationInfo;
}

void replay_post_vkCreateInstance(lava_file_reader& reader, VkResult result, const VkInstanceCreateInfo* pCreateInfo, const VkAllocationCallbacks* pAllocator, VkInstance* pInstance)
{
	if (!pInstance || !*pInstance || *pInstance == VK_NULL_HANDLE)
	{
		ABORT("Failed to create a Vulkan instance: %s", errorString(result));
	}
	stored_instance = *pInstance;

	uint32_t num_phys_devices = 0;
	result = wrap_vkEnumeratePhysicalDevices(*pInstance, &num_phys_devices, nullptr);
	assert(result == VK_SUCCESS);
	assert(num_phys_devices > 0);
	std::vector<VkPhysicalDevice> physical_devices(num_phys_devices);
	result = wrap_vkEnumeratePhysicalDevices(*pInstance, &num_phys_devices, physical_devices.data());
	assert(result == VK_SUCCESS);
	assert(num_phys_devices == physical_devices.size());
	selected_physical_device = physical_devices[0]; // temporary assignment to make commands using physical devices work until we actually pick one

	if (!callback_initialized && wrap_vkCreateDebugReportCallbackEXT && has_debug_report)
	{
		VkDebugReportCallbackCreateInfoEXT drcinfo = {};
		drcinfo.sType = VK_STRUCTURE_TYPE_DEBUG_REPORT_CALLBACK_CREATE_INFO_EXT;
		drcinfo.flags = VK_DEBUG_REPORT_INFORMATION_BIT_EXT | VK_DEBUG_REPORT_WARNING_BIT_EXT | VK_DEBUG_REPORT_ERROR_BIT_EXT;
		drcinfo.pfnCallback = debug_report_callback;
		drcinfo.pUserData = nullptr;
		wrap_vkCreateDebugReportCallbackEXT(*pInstance, &drcinfo, pAllocator, &stored_callback);
		callback_initialized = true;
	}
}

const char* const* device_extensions(VkDeviceCreateInfo* sptr, lava_file_reader& reader, VkPhysicalDevice physicalDevice, uint32_t& len)
{
	bool trace_has_frame_boundary = false;
	static std::vector<const char *> dst;
	static std::vector<std::string> backing;
	const char* const* stored = reader.read_string_array(len); // all extensions used in original
	if (!reader.run) return stored;
	const std::vector<const char*> do_not_copy = {
		VK_KHR_SWAPCHAIN_EXTENSION_NAME, VK_KHR_GET_MEMORY_REQUIREMENTS_2_EXTENSION_NAME,
		VK_KHR_DEDICATED_ALLOCATION_EXTENSION_NAME, VK_EXT_PIPELINE_CREATION_FEEDBACK_EXTENSION_NAME,
		VK_EXT_PIPELINE_CREATION_CACHE_CONTROL_EXTENSION_NAME,
		VK_TRACETOOLTEST_OBJECT_PROPERTY_EXTENSION_NAME, VK_EXT_TOOLING_INFO_EXTENSION_NAME,
		VK_ARM_TRACE_HELPERS_EXTENSION_NAME, VK_ARM_EXPLICIT_HOST_UPDATES_EXTENSION_NAME
	};

	dst.clear();
	backing.clear();

	// Copy over all except platform-specific extensions and potential duplicates
	for (unsigned i = 0; i < len; i++)
	{
		bool nocopy = false;
		for (unsigned j = 0; j < do_not_copy.size(); j++)
		{
			if (strcmp(stored[i], do_not_copy[j]) == 0)
			{
				nocopy = true;
				break;
			}
		}

		if (strcmp(stored[i], "VK_EXT_frame_boundary") == 0)
		{
			trace_has_frame_boundary = true;
			nocopy = true; // add it later
		}

		// Sanity check
		if (is_noscreen() && strcmp(stored[i], "VK_KHR_display_swapchain") == 0)
		{
			ABORT("Cannot use VK_KHR_display_swapchain with none wsi yet");
		}

		if (!nocopy)
		{
			backing.push_back(stored[i]);
		}
	}

	// Find supported extensions
	uint32_t propertyCount = 0;
	VkResult result = wrap_vkEnumerateDeviceExtensionProperties(selected_physical_device, nullptr, &propertyCount, nullptr);
	assert(result == VK_SUCCESS);
	std::vector<VkExtensionProperties> supported_extensions(propertyCount);
	result = wrap_vkEnumerateDeviceExtensionProperties(selected_physical_device, nullptr, &propertyCount, supported_extensions.data());
	assert(result == VK_SUCCESS);
	bool has_swapchain = false;
	for (const VkExtensionProperties& s : supported_extensions)
	{
		if (strcmp(s.extensionName, VK_KHR_SWAPCHAIN_EXTENSION_NAME) == 0) has_swapchain = true;
		if (strcmp(s.extensionName, VK_EXT_PIPELINE_CREATION_FEEDBACK_EXTENSION_NAME) == 0) has_pipeline_feedback = true;
		if (strcmp(s.extensionName, VK_EXT_PIPELINE_CREATION_CACHE_CONTROL_EXTENSION_NAME) == 0) has_pipeline_control = true;
		if (strcmp(s.extensionName, VK_KHR_DEDICATED_ALLOCATION_EXTENSION_NAME) == 0) has_dedicated_allocation++;
		if (strcmp(s.extensionName, VK_KHR_GET_MEMORY_REQUIREMENTS_2_EXTENSION_NAME) == 0) has_dedicated_allocation++;
		if (strcmp(s.extensionName, VK_EXT_FRAME_BOUNDARY_EXTENSION_NAME) == 0) host_has_frame_boundary = true;
	}
	if (!has_swapchain) ABORT("No swapchain extension found - cannot proceed!");

	if (!host_has_frame_boundary && trace_has_frame_boundary)
	{
		ILOG("Replay host does not have frame boundary but trace does -- removing it from the replay!");
		purge_extension_parent(sptr, VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_FRAME_BOUNDARY_FEATURES_EXT);
	}
	else if (trace_has_frame_boundary)
	{
		backing.push_back(VK_EXT_FRAME_BOUNDARY_EXTENSION_NAME);
	}

	// Add device extensions
	backing.push_back(VK_KHR_SWAPCHAIN_EXTENSION_NAME);
	if (use_dedicated_allocation())
	{
		if (has_dedicated_allocation < 2) ABORT("No dedicated allocation support found - aborting!");
		backing.push_back(VK_KHR_GET_MEMORY_REQUIREMENTS_2_EXTENSION_NAME);
		backing.push_back(VK_KHR_DEDICATED_ALLOCATION_EXTENSION_NAME);
	}

	if (has_pipeline_feedback)
	{
		backing.push_back(VK_EXT_PIPELINE_CREATION_FEEDBACK_EXTENSION_NAME);
		DLOG("Enabling pipeline creation feedback extension");
	}

	if (has_pipeline_control)
	{
		backing.push_back(VK_EXT_PIPELINE_CREATION_CACHE_CONTROL_EXTENSION_NAME);
		DLOG("Enabling pipeline creation cache control extension");
	}

	dst.resize(backing.size());
	for (uint32_t i = 0; i < backing.size(); i++)
	{
		dst[i] = backing[i].data();
	}
	len = backing.size();

	DLOG("Enabling %u device extensions:", len);
	for (auto ext_name : backing)
	{
		DLOG("\t %s", ext_name.c_str());
	}

	return dst.data();
}

const char* const* instance_extensions(lava_file_reader& reader, uint32_t& len)
{
	static std::vector<const char *> dst;
	static std::vector<std::string> backing;
	const std::vector<const char*> do_not_copy = {
		VK_KHR_SURFACE_EXTENSION_NAME, "VK_KHR_xcb_surface", "VK_KHR_xlib_surface", VK_KHR_DISPLAY_EXTENSION_NAME,
		"VK_KHR_wayland_surface", "VK_KHR_mir_surface", "VK_KHR_android_surface", "VK_KHR_win32_surface",
		"VK_EXT_headless_surface"
	};
	const char* const* stored = reader.read_string_array(len);
	if (!reader.run) return stored;

	backing.clear();
	dst.clear();

	uint32_t propertyCount = 0;
	VkResult result = wrap_vkEnumerateInstanceExtensionProperties(nullptr, &propertyCount, nullptr);
	assert(result == VK_SUCCESS);
	std::vector<VkExtensionProperties> supported_extensions(propertyCount);
	result = wrap_vkEnumerateInstanceExtensionProperties(nullptr, &propertyCount, supported_extensions.data());
	assert(result == VK_SUCCESS);
	bool has_surface = false;
	DLOG("Supported instance extensions on replay host:");
	for (const VkExtensionProperties& s : supported_extensions)
	{
		if (strcmp(s.extensionName, VK_KHR_SURFACE_EXTENSION_NAME) == 0) has_surface = true;
		if (strcmp(s.extensionName, VK_EXT_DEBUG_REPORT_EXTENSION_NAME) == 0) has_debug_report = true;
		if (strcmp(s.extensionName, VK_EXT_DEBUG_UTILS_EXTENSION_NAME) == 0) has_debug_utils = true;
		DLOG("\t%s", s.extensionName);
	}
	if (!has_debug_report && is_debug()) ELOG("Warning: Debug report extension missing - debug mode will not be fully operational!");
	if (!has_debug_report && is_validation()) ELOG("Warning: Debug report extension missing - validation layer will not be able to report anything!");
	assert(has_surface);

	// Copy over all except platform-specific extensions and potential duplicates
	for (unsigned i = 0; i < len; i++)
	{
		bool nocopy = false;
		for (unsigned j = 0; j < do_not_copy.size(); j++)
		{
			if (strcmp(stored[i], do_not_copy[j]) == 0)
			{
				nocopy = true;
				break;
			}
		}

		if (is_noscreen() && strcmp(stored[i], "VK_KHR_display") == 0)
		{
			ABORT("Cannot use VK_KHR_display with none wsi yet");
		}

		if (!nocopy)
		{
			backing.push_back(stored[i]);
		}
	}

	// Add instance extensions
	backing.push_back(VK_KHR_SURFACE_EXTENSION_NAME);
	if (is_debug() || is_validation())
	{
		if (has_debug_report) backing.push_back(VK_EXT_DEBUG_REPORT_EXTENSION_NAME);
	}
#ifdef VK_USE_PLATFORM_ANDROID_KHR
	backing.push_back(VK_KHR_ANDROID_SURFACE_EXTENSION_NAME);
#else
	const char* winsys = window_winsys();
#ifdef VK_USE_PLATFORM_XCB_KHR
	if (strcmp(winsys, "xcb") == 0 && !is_noscreen())
	{
		backing.push_back(VK_KHR_XCB_SURFACE_EXTENSION_NAME);
	}
#endif
#ifdef VK_USE_PLATFORM_XCB_KHR
	if (strcmp(winsys, "wayland") == 0 && !is_noscreen())
	{
		backing.push_back(VK_KHR_WAYLAND_SURFACE_EXTENSION_NAME);
	}
#endif
#ifdef VK_USE_PLATFORM_XLIB_KHR
	if (strcmp(winsys, "x11") == 0 && !is_noscreen())
	{
		backing.push_back(VK_KHR_XLIB_SURFACE_EXTENSION_NAME);
	}
#endif
	if (strcmp(winsys, "headless") == 0 && !is_noscreen())
	{
		backing.push_back("VK_EXT_headless_surface");
	}
#endif

	dst.resize(backing.size());
	for (uint32_t i = 0; i < backing.size(); i++)
	{
		dst[i] = backing[i].data();
	}
	len = backing.size();

	DLOG("Enabling %u instance extensions:", len);
	for (auto ext_name : backing)
	{
		DLOG("\t %s", ext_name.c_str());
	}

	return dst.data();
}

// for completeness - but this part of the API is never used
const char* const* device_layers(lava_file_reader& reader, uint32_t& len)
{
	const char* const* retval = reader.read_string_array(len);
	return retval;
}

const char* const* instance_layers(lava_file_reader& reader, uint32_t& len)
{
	static std::vector<const char *> dst;
	static std::vector<std::string> backing;
	const char* const* retval = reader.read_string_array(len);
	if (!reader.run) return retval;

	backing.clear();
	dst.clear();

	// Add validation layers, if requested
	uint32_t propertyCount = 0;
	VkResult result = wrap_vkEnumerateInstanceLayerProperties(&propertyCount, nullptr);
	assert(result == VK_SUCCESS);
	std::vector<VkLayerProperties> supported_layers(propertyCount);
	result = wrap_vkEnumerateInstanceLayerProperties(&propertyCount, supported_layers.data());
	assert(result == VK_SUCCESS);
	DLOG("Supported instance layers on replay host:");
	for (const VkLayerProperties& s : supported_layers)
	{
		DLOG("\t%s - %s", s.layerName, s.description);
		if (is_validation() && strcmp(s.layerName, "VK_LAYER_KHRONOS_validation") == 0)
		{
			backing.push_back(s.layerName);
			ILOG("Enabling validation layer");
		}
	}

	len = backing.size();

	// Resize everything else to match
	dst.resize(len);
	for (uint32_t i = 0; i < backing.size(); i++)
	{
		dst[i] = backing[i].data();
	}

	ILOG("Enabling %u layers:", len);
	for (auto l_name : backing)
	{
		ILOG("\t %s", l_name.c_str());
	}

	return dst.data();
}

/// 'instance' is not safe to use here, since it has already been destroyed
void replay_post_vkDestroyInstance(lava_file_reader& reader, VkInstance instance, const VkAllocationCallbacks* pAllocator)
{
	if (instance != VK_NULL_HANDLE)
	{
		reader.parent->finalize(false);
		callback_initialized = false;
		stored_instance = VK_NULL_HANDLE;
		reset_all();
	}
}

void replay_pre_vkDestroyDevice(lava_file_reader& reader, VkDevice device, const VkAllocationCallbacks* pAllocator)
{
	if (device != VK_NULL_HANDLE)
	{
		wrap_vkDeviceWaitIdle(device);
		const uint32_t device_index = index_to_VkDevice.index(device);
		if (device_index != CONTAINER_INVALID_INDEX)
		{
			auto& device_data = VkDevice_index.at(device_index);
			for (auto& as : VkAccelerationStructureKHR_index)
			{
				if (as.parent_device_index == device_index && as.replay_storage.buffer != VK_NULL_HANDLE)
				{
					destroy_internal_buffer(device, as.replay_storage);
				}
			}

			for (auto& cb : VkCommandBuffer_index)
			{
				if (cb.device_index == device_index && cb.scratch_buffer.buffer != VK_NULL_HANDLE)
				{
					destroy_internal_buffer(device, cb.scratch_buffer);
				}
			}
		}
		selected_physical_device = VK_NULL_HANDLE;
	}
}

void replay_pre_vkDestroyInstance(lava_file_reader& reader, VkInstance instance, const VkAllocationCallbacks* pAllocator)
{
	if (stored_callback != VK_NULL_HANDLE)
	{
		wrap_vkDestroyDebugReportCallbackEXT(instance, stored_callback, pAllocator);
		stored_callback = VK_NULL_HANDLE;
	}
}

void retrace_vkGetDeviceProcAddr(lava_file_reader& reader)
{
	const uint32_t device_index = reader.read_handle(DEBUGPARAM("VkDevice"));
	VkDevice device = index_to_VkDevice.at(device_index);
	const char* pName = reader.read_string();
	PFN_vkVoidFunction ptr = nullptr;
	if (reader.run) wrap_vkGetDeviceProcAddr(device, pName);
}

void retrace_vkGetInstanceProcAddr(lava_file_reader& reader)
{
	const uint32_t instance_index = reader.read_handle(DEBUGPARAM("VkInstance"));
	VkInstance instance = index_to_VkInstance.at(instance_index);
	const char* pName = reader.read_string();
	PFN_vkVoidFunction ptr = nullptr;
	if (reader.run) wrap_vkGetInstanceProcAddr(instance, pName);
}

void retrace_vkGetDeviceTracingObjectPropertyTRACETOOLTEST(lava_file_reader& reader)
{
	assert(false);
}

void retrace_vkSyncBufferTRACETOOLTEST(lava_file_reader& reader)
{
	const uint32_t device_index = reader.read_handle(DEBUGPARAM("VkDevice"));
	const uint32_t buffer_index = reader.read_handle(DEBUGPARAM("VkBuffer"));
}

void retrace_vkAssertBufferARM(lava_file_reader& reader)
{
	const uint32_t device_index = reader.read_handle(DEBUGPARAM("VkDevice"));
	const auto& device_data = VkDevice_index.at(device_index);
	const uint32_t buffer_index = reader.read_handle(DEBUGPARAM("VkBuffer"));
	const VkDeviceSize offset = reader.read_uint64_t();
	VkDeviceSize size = reader.read_uint64_t();
	const char* comment = reader.read_string();
	const uint32_t checksum = reader.read_uint32_t();
	trackedobject& tbuf = VkBuffer_index.at(buffer_index);
	VkDevice device = index_to_VkDevice.at(device_index);
	suballoc_location loc = device_data.allocator->find_buffer_memory(buffer_index);
	if (size == VK_WHOLE_SIZE)
	{
		size = tbuf.size - offset; // set to remaining size
	}
	uint8_t* ptr = nullptr;
	if (!reader.run) return;
	VkResult result = wrap_vkMapMemory(device, loc.memory, loc.offset, tbuf.size, 0, (void**)&ptr);
	assert(result == VK_SUCCESS);
	uint32_t checksum_new = adler32((unsigned char*)ptr + offset, size);
	NEVER("buffer %s[%u] validation size=%u off=%u memoff=%u origchecksum=%u newchecksum=%u [first byte is %u, last byte is %u]", comment, buffer_index,
	      (unsigned)size, (unsigned)offset, (unsigned)loc.offset, checksum, checksum_new, ptr[0], ptr[tbuf.size-1]);
	wrap_vkUnmapMemory(device, loc.memory);
	if (checksum != checksum_new && !is_blackhole_mode())
	{
		ABORT("Buffer checksum failed: %s", comment);
	}
}

static void read_VkDataGraphPipelineConstantARM(lava_file_reader& reader, VkDataGraphPipelineConstantARM* sptr)
{
	// TBD
}

void read_VkPhysicalDeviceExplicitHostUpdatesFeaturesARM(lava_file_reader& reader, VkPhysicalDeviceExplicitHostUpdatesFeaturesARM* sptr)
{
	sptr->sType = (VkStructureType)reader.read_uint32_t();
	assert(sptr->sType == VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_EXPLICIT_HOST_UPDATES_FEATURES_ARM);
	read_extension(reader, (VkBaseOutStructure**)&sptr->pNext);
	sptr->explicitHostUpdates = reader.read_uint32_t();
}

void read_VkFlushRangesFlagsARM(lava_file_reader& reader, VkFlushRangesFlagsARM* sptr)
{
	sptr->sType = (VkStructureType)reader.read_uint32_t();
	assert(sptr->sType == VK_STRUCTURE_TYPE_FLUSH_RANGES_FLAGS_ARM);
	read_extension(reader, (VkBaseOutStructure**)&sptr->pNext);
	sptr->flags = reader.read_uint32_t();
}

void read_VkMarkedOffsetsARM(lava_file_reader& reader, VkMarkedOffsetsARM* sptr)
{
	sptr->sType = (VkStructureType)reader.read_uint32_t();
	assert(sptr->sType == VK_STRUCTURE_TYPE_MARKED_OFFSETS_ARM);
	read_extension(reader, (VkBaseOutStructure**)&sptr->pNext);
	sptr->count = reader.read_uint32_t();
	const bool pMarkingTypes_opt = reader.read_uint8_t();
	sptr->pMarkingTypes = nullptr;
	if (pMarkingTypes_opt)
	{
		VkMarkingTypeARM* backing = reader.pool.allocate<VkMarkingTypeARM>(sptr->count);
		memset(backing, 0, sptr->count * sizeof(VkMarkingTypeARM));
		reader.read_array(backing, sptr->count);
		sptr->pMarkingTypes = backing;
	}
	const bool pSubTypes_opt = reader.read_uint8_t();
	sptr->pSubTypes = nullptr;
	if (pSubTypes_opt)
	{
		VkMarkingSubTypeARM* backing = reader.pool.allocate<VkMarkingSubTypeARM>(sptr->count);
		memset(backing, 0, sptr->count * sizeof(VkMarkingSubTypeARM));
		reader.read_array(reinterpret_cast<uint8_t*>(backing), sptr->count * sizeof(VkMarkingSubTypeARM));
		sptr->pSubTypes = backing;
	}
	const bool pOffsets_opt = reader.read_uint8_t();
	sptr->pOffsets = nullptr;
	if (pOffsets_opt)
	{
		VkDeviceSize* backing = reader.pool.allocate<VkDeviceSize>(sptr->count);
		memset(backing, 0, sptr->count * sizeof(VkDeviceSize));
		reader.read_array(backing, sptr->count);
		sptr->pOffsets = backing;
	}
	DLOG("Got a memory markup struct with count=%u", (unsigned)sptr->count);
}

static void translate_marked_offsets(lava_file_reader& reader, const VkMarkedOffsetsARM* markings, void* ptr)
{
	if (!markings || !markings->pOffsets || !markings->pMarkingTypes) return;
	for (uint32_t i = 0; i < markings->count; i++)
	{
		if (markings->pMarkingTypes[i] != VK_MARKING_TYPE_DEVICE_ADDRESS_ARM) continue;
		const uint64_t offset = markings->pOffsets[i];
		void* addr = (char*)ptr + offset;
		uint64_t current = 0;
		// Use memcpy to handle unaligned device address storage.
		memcpy(&current, addr, sizeof(current));
		const uint64_t newval = reader.parent->device_address_remapping.translate_address(current);
		assert(newval != 0 || !reader.run);
		DLOG("%u: Changing memory value at offset %lu from %lu to %lu", (unsigned)i, (unsigned long)offset, (unsigned long)current, (unsigned long)newval);
		memcpy(addr, &newval, sizeof(newval));
	}
}

static void rewrite_descriptor_update_template_data(lava_file_reader& reader, VkDescriptorUpdateTemplate descriptorUpdateTemplate, void* pData)
{
	if (!pData || descriptorUpdateTemplate == VK_NULL_HANDLE) return;
	const uint32_t template_index = index_to_VkDescriptorUpdateTemplate.index(descriptorUpdateTemplate);
	auto& template_data = VkDescriptorUpdateTemplate_index.at(template_index);
	if (template_data.entries.empty()) return;

	uint8_t* base_data = reinterpret_cast<uint8_t*>(pData);
	for (const auto& entry : template_data.entries)
	{
		if (entry.descriptorCount == 0) continue;
		switch (entry.descriptorType)
		{
		case VK_DESCRIPTOR_TYPE_SAMPLER:
		case VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER:
		case VK_DESCRIPTOR_TYPE_SAMPLED_IMAGE:
		case VK_DESCRIPTOR_TYPE_STORAGE_IMAGE:
		case VK_DESCRIPTOR_TYPE_INPUT_ATTACHMENT:
			{
				const size_t element_size = sizeof(VkDescriptorImageInfo);
				const size_t stride = entry.stride ? entry.stride : element_size;
				uint8_t* base = base_data + entry.offset;
				for (uint32_t i = 0; i < entry.descriptorCount; i++)
				{
					VkDescriptorImageInfo info{};
					memcpy(&info, base + (size_t)i * stride, element_size);
					if (entry.descriptorType == VK_DESCRIPTOR_TYPE_SAMPLER || entry.descriptorType == VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER)
					{
						const uint32_t sampler_index = (uint32_t)(uintptr_t)info.sampler;
						info.sampler = (sampler_index == CONTAINER_NULL_VALUE) ? VK_NULL_HANDLE : index_to_VkSampler.at(sampler_index);
					}
					if (entry.descriptorType != VK_DESCRIPTOR_TYPE_SAMPLER)
					{
						const uint32_t view_index = (uint32_t)(uintptr_t)info.imageView;
						info.imageView = (view_index == CONTAINER_NULL_VALUE) ? VK_NULL_HANDLE : index_to_VkImageView.at(view_index);
					}
					memcpy(base + (size_t)i * stride, &info, element_size);
				}
			}
			break;
		case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER:
		case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER:
		case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC:
		case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER_DYNAMIC:
			{
				const size_t element_size = sizeof(VkDescriptorBufferInfo);
				const size_t stride = entry.stride ? entry.stride : element_size;
				uint8_t* base = base_data + entry.offset;
				for (uint32_t i = 0; i < entry.descriptorCount; i++)
				{
					VkDescriptorBufferInfo info{};
					memcpy(&info, base + (size_t)i * stride, element_size);
					const uint32_t buffer_index = (uint32_t)(uintptr_t)info.buffer;
					info.buffer = (buffer_index == CONTAINER_NULL_VALUE) ? VK_NULL_HANDLE : index_to_VkBuffer.at(buffer_index);
					memcpy(base + (size_t)i * stride, &info, element_size);
				}
			}
			break;
		case VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER:
		case VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER:
			{
				const size_t element_size = sizeof(VkBufferView);
				const size_t stride = entry.stride ? entry.stride : element_size;
				uint8_t* base = base_data + entry.offset;
				for (uint32_t i = 0; i < entry.descriptorCount; i++)
				{
					VkBufferView view = VK_NULL_HANDLE;
					memcpy(&view, base + (size_t)i * stride, element_size);
					const uint32_t view_index = (uint32_t)(uintptr_t)view;
					view = (view_index == CONTAINER_NULL_VALUE) ? VK_NULL_HANDLE : index_to_VkBufferView.at(view_index);
					memcpy(base + (size_t)i * stride, &view, element_size);
				}
			}
			break;
		case VK_DESCRIPTOR_TYPE_ACCELERATION_STRUCTURE_KHR:
			{
				const size_t element_size = sizeof(VkAccelerationStructureKHR);
				const size_t stride = entry.stride ? entry.stride : element_size;
				uint8_t* base = base_data + entry.offset;
				for (uint32_t i = 0; i < entry.descriptorCount; i++)
				{
					VkAccelerationStructureKHR as = VK_NULL_HANDLE;
					memcpy(&as, base + (size_t)i * stride, element_size);
					const uint32_t as_index = (uint32_t)(uintptr_t)as;
					as = (as_index == CONTAINER_NULL_VALUE) ? VK_NULL_HANDLE : index_to_VkAccelerationStructureKHR.at(as_index);
					memcpy(base + (size_t)i * stride, &as, element_size);
				}
			}
			break;
		case VK_DESCRIPTOR_TYPE_INLINE_UNIFORM_BLOCK:
			break;
		case VK_DESCRIPTOR_TYPE_TENSOR_ARM:
			assert(false); // TODO
			break;
		case VK_DESCRIPTOR_TYPE_PARTITIONED_ACCELERATION_STRUCTURE_NV:
			ABORT("VK_NV_partitioned_acceleration_structure not supported");
			break;
		case VK_DESCRIPTOR_TYPE_MUTABLE_EXT:
			ABORT("vkUpdateDescriptorSetWithTemplate using VK_EXT_mutable_descriptor_type not yet implemented");
			break;
		case VK_DESCRIPTOR_TYPE_BLOCK_MATCH_IMAGE_QCOM:
		case VK_DESCRIPTOR_TYPE_SAMPLE_WEIGHT_IMAGE_QCOM:
			ABORT("VK_QCOM_image_processing not supported");
			break;
		case VK_DESCRIPTOR_TYPE_ACCELERATION_STRUCTURE_NV:
			ABORT("VK_NV_ray_tracing not supported");
			break;
		case VK_DESCRIPTOR_TYPE_MAX_ENUM:
			ABORT("Bad descriptor type in vkUpdateDescriptorSetWithTemplate");
			break;
		}
	}
}

void replay_pre_vkCmdPushConstants2KHR(lava_file_reader& reader, VkCommandBuffer commandBuffer, const VkPushConstantsInfoKHR* pPushConstantsInfo)
{
	assert(pPushConstantsInfo);
	const VkMarkedOffsetsARM* remap = (const VkMarkedOffsetsARM*)find_extension(pPushConstantsInfo, VK_STRUCTURE_TYPE_MARKED_OFFSETS_ARM);
	if (!remap) return; // nothing to do here
	assert(pPushConstantsInfo->pValues);
	translate_marked_offsets(reader, remap, const_cast<void*>(pPushConstantsInfo->pValues));
	// make sure we don't leak this to the driver, as this would break validation
	purge_extension_parent(const_cast<VkPushConstantsInfoKHR*>(pPushConstantsInfo), VK_STRUCTURE_TYPE_MARKED_OFFSETS_ARM);
}

void replay_pre_vkCmdPushConstants2(lava_file_reader& reader, VkCommandBuffer commandBuffer, const VkPushConstantsInfoKHR* pPushConstantsInfo)
{
	replay_pre_vkCmdPushConstants2KHR(reader, commandBuffer, pPushConstantsInfo);
}

void replay_pre_vkUpdateDescriptorSetWithTemplate(lava_file_reader& reader, VkDevice device, VkDescriptorSet descriptorSet, VkDescriptorUpdateTemplate descriptorUpdateTemplate, const void* pData)
{
	(void)device;
	(void)descriptorSet;
	rewrite_descriptor_update_template_data(reader, descriptorUpdateTemplate, const_cast<void*>(pData));
}

void replay_pre_vkUpdateDescriptorSetWithTemplateKHR(lava_file_reader& reader, VkDevice device, VkDescriptorSet descriptorSet, VkDescriptorUpdateTemplate descriptorUpdateTemplate, const void* pData)
{
	replay_pre_vkUpdateDescriptorSetWithTemplate(reader, device, descriptorSet, descriptorUpdateTemplate, pData);
}

void replay_pre_vkCmdPushDescriptorSetWithTemplate(lava_file_reader& reader, VkCommandBuffer commandBuffer, VkDescriptorUpdateTemplate descriptorUpdateTemplate, VkPipelineLayout layout, uint32_t set, const void* pData)
{
	(void)commandBuffer;
	(void)layout;
	(void)set;
	rewrite_descriptor_update_template_data(reader, descriptorUpdateTemplate, const_cast<void*>(pData));
}

void replay_pre_vkCmdPushDescriptorSetWithTemplateKHR(lava_file_reader& reader, VkCommandBuffer commandBuffer, VkDescriptorUpdateTemplate descriptorUpdateTemplate, VkPipelineLayout layout, uint32_t set, const void* pData)
{
	replay_pre_vkCmdPushDescriptorSetWithTemplate(reader, commandBuffer, descriptorUpdateTemplate, layout, set, pData);
}

void replay_pre_vkCreateComputePipelines(lava_file_reader& reader, VkDevice device, VkPipelineCache pipelineCache, uint32_t createInfoCount,
	const VkComputePipelineCreateInfo* pCreateInfos, const VkAllocationCallbacks* pAllocator, VkPipeline* pPipelines)
{
	for (uint32_t i = 0; i < createInfoCount; i++)
	{
		const VkMarkedOffsetsARM* remap = (const VkMarkedOffsetsARM*)find_extension(&pCreateInfos[i].stage, VK_STRUCTURE_TYPE_MARKED_OFFSETS_ARM);
		if (!remap) continue; // nothing to do here

		assert(pCreateInfos[i].stage.pSpecializationInfo != nullptr);
		assert(pCreateInfos[i].stage.pSpecializationInfo->pData != nullptr);

		translate_marked_offsets(reader, remap, const_cast<void*>(pCreateInfos[i].stage.pSpecializationInfo->pData));

		// make sure we don't leak this to the driver, as this would break validation
		purge_extension_parent(const_cast<VkPipelineShaderStageCreateInfo*>(&pCreateInfos[i].stage), VK_STRUCTURE_TYPE_MARKED_OFFSETS_ARM);
	}
}

void replay_pre_vkCreateGraphicsPipelines(lava_file_reader& reader, VkDevice device, VkPipelineCache pipelineCache, uint32_t createInfoCount,
	const VkGraphicsPipelineCreateInfo* pCreateInfos, const VkAllocationCallbacks* pAllocator, VkPipeline* pPipelines)
{
	for (uint32_t i = 0; i < createInfoCount; i++)
	{
		for (uint32_t stage = 0; stage < pCreateInfos[i].stageCount; stage++)
		{
			const VkMarkedOffsetsARM* remap = (const VkMarkedOffsetsARM*)find_extension(&pCreateInfos[i].pStages[stage], VK_STRUCTURE_TYPE_MARKED_OFFSETS_ARM);
			if (!remap) continue; // nothing to do here

			assert(pCreateInfos[i].pStages[stage].pSpecializationInfo != nullptr);
			assert(pCreateInfos[i].pStages[stage].pSpecializationInfo->pData != nullptr);

			translate_marked_offsets(reader, remap, const_cast<void*>(pCreateInfos[i].pStages[stage].pSpecializationInfo->pData));

			// make sure we don't leak this to the driver, as this would break validation
			purge_extension_parent(const_cast<VkPipelineShaderStageCreateInfo*>(&pCreateInfos[i].pStages[stage]), VK_STRUCTURE_TYPE_MARKED_OFFSETS_ARM);
		}
	}
}

void replay_pre_vkCreateRayTracingPipelinesKHR(lava_file_reader& reader, VkDevice device, VkDeferredOperationKHR deferredOperation, VkPipelineCache pipelineCache,
	uint32_t createInfoCount, const VkRayTracingPipelineCreateInfoKHR* pCreateInfos, const VkAllocationCallbacks* pAllocator, VkPipeline* pPipelines)
{
	for (uint32_t i = 0; i < createInfoCount; i++)
	{
		for (uint32_t stage = 0; stage < pCreateInfos[i].stageCount; stage++)
		{
			const VkMarkedOffsetsARM* remap = (const VkMarkedOffsetsARM*)find_extension(&pCreateInfos[i].pStages[stage], VK_STRUCTURE_TYPE_MARKED_OFFSETS_ARM);
			if (!remap) continue; // nothing to do here

			assert(pCreateInfos[i].pStages[stage].pSpecializationInfo != nullptr);
			assert(pCreateInfos[i].pStages[stage].pSpecializationInfo->pData != nullptr);

			translate_marked_offsets(reader, remap, const_cast<void*>(pCreateInfos[i].pStages[stage].pSpecializationInfo->pData));

			// make sure we don't leak this to the driver, as this would break validation
			purge_extension_parent(const_cast<VkPipelineShaderStageCreateInfo*>(&pCreateInfos[i].pStages[stage]), VK_STRUCTURE_TYPE_MARKED_OFFSETS_ARM);
		}
	}
}

static char* mem_map(lava_file_reader& reader, VkDevice device, const suballoc_location& loc)
{
	char* ptr = nullptr;
	if (reader.run)
	{
		VkResult result = wrap_vkMapMemory(device, loc.memory, loc.offset, loc.size, 0, (void**)&ptr);
		assert(result == VK_SUCCESS);
	}
	else
	{
		ptr = (char*)loc.memory;
	}
	if (loc.needs_init)
	{
		memset(ptr, 0, loc.size);
	}
	return ptr;
}

static void mem_unmap(lava_file_reader& reader, VkDevice device, const suballoc_location& loc, VkMarkedOffsetsARM* ar, char* ptr)
{
	if (ar) translate_marked_offsets(reader, ar, ptr);
	if (loc.needs_flush && reader.run)
	{
		VkMappedMemoryRange flush = {};
		flush.sType = VK_STRUCTURE_TYPE_MAPPED_MEMORY_RANGE;
		flush.memory = loc.memory;
		flush.offset = loc.offset;
		flush.size = loc.size;
		wrap_vkFlushMappedMemoryRanges(device, 1, &flush);
	}
	if (reader.run) wrap_vkUnmapMemory(device, loc.memory);
}

VKAPI_ATTR void retrace_vkThreadBarrierTRACETOOLTEST(lava_file_reader& reader)
{
	const unsigned size = reader.read_uint32_t();
	for (int i = 0; i < (int)size; i++)
	{
		const unsigned call = reader.read_uint32_t();
		DLOG3("Thread barrier on thread %d, waiting for call %u on thread %d / %u", reader.thread_index(), call, i, size - 1);
		while (i != reader.thread_index() && call > reader.parent->thread_call_numbers->at(i).load(std::memory_order_relaxed)) usleep(1);
	}
	DLOG2("Passed thread barrier on thread %d, waited for %u threads", reader.thread_index(), size);
}

void read_VkUpdateMemoryInfoARM(lava_file_reader& reader, VkUpdateMemoryInfoARM* sptr)
{
	sptr->sType = (VkStructureType)reader.read_uint32_t();
	assert(sptr->sType == VK_STRUCTURE_TYPE_UPDATE_MEMORY_INFO_ARM);
	read_extension(reader, (VkBaseOutStructure**)&sptr->pNext);
	const uint32_t buffer_index = reader.read_handle(DEBUGPARAM("VkBuffer"));
	sptr->dstBuffer = index_to_VkBuffer.at(buffer_index);
	sptr->dstOffset = reader.read_uint64_t();
	sptr->dataSize = reader.read_uint64_t();
	uint8_t pData_opt = reader.read_uint8_t();
	sptr->pData = nullptr;
	if (pData_opt)
	{
		uint8_t* backing = reader.pool.allocate<uint8_t>(sptr->dataSize);
		memset(backing, 0, sptr->dataSize);
		reader.read_array(backing, sptr->dataSize);
		sptr->pData = backing;
	}
}

VKAPI_ATTR void retrace_vkCmdUpdateBuffer2ARM(lava_file_reader& reader)
{
	VkUpdateMemoryInfoARM info = {};
	const uint32_t commandbuffer_index = reader.read_handle(DEBUGPARAM("VkCommandBuffer"));
	VkCommandBuffer commandBuffer = index_to_VkCommandBuffer.at(commandbuffer_index);
	read_VkUpdateMemoryInfoARM(reader, &info);
	assert(info.dstBuffer != VK_NULL_HANDLE);
	uint32_t buffer_index = index_to_VkBuffer.index(info.dstBuffer);
	trackedobject& tbuf = VkBuffer_index.at(buffer_index);
	VkMarkedOffsetsARM* ar = (VkMarkedOffsetsARM*)find_extension(&info, VK_STRUCTURE_TYPE_MARKED_OFFSETS_ARM);
	// -- Execute --
	if (ar) translate_marked_offsets(reader, ar, const_cast<void*>(info.pData));
	if (reader.run) wrap_vkCmdUpdateBuffer(commandBuffer, info.dstBuffer, info.dstOffset, info.dataSize, info.pData);
	ILOG("Ran vkCmdUpdateBuffer2ARM"); // TBD REMOVE ME
	tbuf.last_modified = reader.current;
}

static void read_VkAccelerationStructureBuildGeometryInfoKHR(lava_file_reader& reader, VkAccelerationStructureBuildGeometryInfoKHR* sptr)
{
	// -- Declarations --
	uint32_t accelerationstructurekhr_index = 0;
	uint32_t geometryCount = 0;
	uint8_t tmp_uuint8t = 0;
	VkAccelerationStructureGeometryKHR* pGeometries_backing = nullptr;
	VkAccelerationStructureGeometryKHR** ppGeometries_backing = nullptr;
	// -- Instructions --
	sptr->sType = static_cast<VkStructureType>(reader.read_uint32_t());
	assert(sptr->sType == VK_STRUCTURE_TYPE_ACCELERATION_STRUCTURE_BUILD_GEOMETRY_INFO_KHR);
	read_extension(reader, (VkBaseOutStructure**)&sptr->pNext);
	sptr->type = static_cast<VkAccelerationStructureTypeKHR>(reader.read_uint32_t());
	sptr->flags = static_cast<VkBuildAccelerationStructureFlagsKHR>(reader.read_uint32_t());
	sptr->mode = static_cast<VkBuildAccelerationStructureModeKHR>(reader.read_uint32_t());
	accelerationstructurekhr_index = reader.read_handle(DEBUGPARAM("VkAccelerationStructureKHR"));
	sptr->srcAccelerationStructure = index_to_VkAccelerationStructureKHR.at(accelerationstructurekhr_index);
	accelerationstructurekhr_index = reader.read_handle(DEBUGPARAM("VkAccelerationStructureKHR"));
	sptr->dstAccelerationStructure = index_to_VkAccelerationStructureKHR.at(accelerationstructurekhr_index);
	geometryCount = reader.read_uint32_t(); // indirect read because it is a count
	sptr->geometryCount = geometryCount;
	tmp_uuint8t = reader.read_uint8_t(); // whether we should load pGeometries
	if (tmp_uuint8t)
	{
		pGeometries_backing = reader.pool.allocate<VkAccelerationStructureGeometryKHR>(geometryCount);
		memset(pGeometries_backing, 0, geometryCount * sizeof(VkAccelerationStructureGeometryKHR));
		sptr->pGeometries = pGeometries_backing;
		for (unsigned sidx = 0; sidx < sptr->geometryCount; sidx++) // varname=pGeometries_backing
		{
			read_VkAccelerationStructureGeometryKHR(reader, &pGeometries_backing[sidx]);
		}
	}
	tmp_uuint8t = reader.read_uint8_t(); // whether we should load ppGeometries
	if (tmp_uuint8t)
	{
		ppGeometries_backing = reader.pool.allocate<VkAccelerationStructureGeometryKHR*>(geometryCount);
		memset(ppGeometries_backing, 0, geometryCount * sizeof(VkAccelerationStructureGeometryKHR*));
		sptr->ppGeometries = ppGeometries_backing;
		for (unsigned sidx = 0; sidx < sptr->geometryCount; sidx++) // varname=ppGeometries_backing
		{
			ppGeometries_backing[sidx] = reader.pool.allocate<VkAccelerationStructureGeometryKHR>(1);
			read_VkAccelerationStructureGeometryKHR(reader, ppGeometries_backing[sidx]);
		}
	}

	const uint64_t stored_address = reader.read_uint64_t();
	sptr->scratchData.deviceAddress = reader.parent->device_address_remapping.translate_address(stored_address);
	ILOG("VkAccelerationStructureBuildGeometryInfoKHR changing device address from %lu to %lu", (unsigned long)stored_address, (unsigned long)sptr->scratchData.deviceAddress);
}

void retrace_vkGetSwapchainImagesKHR(lava_file_reader& reader)
{
	VkResult result;
	const uint32_t device_index = reader.read_handle(DEBUGPARAM("VkDevice"));
	const uint32_t swapchain_index = reader.read_handle(DEBUGPARAM("VkSwapchainKHR"));
	VkDevice device = index_to_VkDevice.at(device_index);
	auto& device_data = VkDevice_index.at(device_index);
	const uint8_t do_call = reader.read_uint8_t();
	const VkResult stored_retval = (VkResult)reader.read_uint32_t();
	const uint32_t stored_image_count = reader.read_uint32_t();
	if (!do_call) return;

	VkSwapchainKHR swapchain = index_to_VkSwapchainKHR.at(swapchain_index);
	trackedswapchain_replay& data = VkSwapchainKHR_index.at(swapchain_index);

	if (!is_noscreen() && reader.run)
	{
		uint32_t pSwapchainImageCount;
		result = wrap_vkGetSwapchainImagesKHR(device, swapchain, &pSwapchainImageCount, nullptr);
		if (!is_virtualswapchain()) assert(stored_image_count == pSwapchainImageCount);
		data.pSwapchainImages.resize(pSwapchainImageCount);
		result = wrap_vkGetSwapchainImagesKHR(device, swapchain, &pSwapchainImageCount, data.pSwapchainImages.data());
		assert(result == VK_SUCCESS);
		(void)result;
	}

	if (!data.initialized && is_virtualswapchain() && reader.run) // create virtual swapchain
	{
		// Make virtual images
		VkImageCreateInfo pinfo = {};
		pinfo.sType = VK_STRUCTURE_TYPE_IMAGE_CREATE_INFO;
		pinfo.flags = (data.info.flags & VK_SWAPCHAIN_CREATE_MUTABLE_FORMAT_BIT_KHR);
		pinfo.flags &= ~VK_SWAPCHAIN_CREATE_DEFERRED_MEMORY_ALLOCATION_BIT_KHR; // disable lazy swapchain image allocation
		pinfo.imageType = VK_IMAGE_TYPE_2D;
		pinfo.extent.height = data.info.imageExtent.height;
		pinfo.extent.width = data.info.imageExtent.width;
		pinfo.extent.depth = 1;
		pinfo.format = data.info.imageFormat;
		pinfo.mipLevels = 1;
		pinfo.arrayLayers = data.info.imageArrayLayers;
		pinfo.samples = VK_SAMPLE_COUNT_1_BIT;
		pinfo.tiling = VK_IMAGE_TILING_OPTIMAL;
		pinfo.usage = data.info.imageUsage | VK_IMAGE_USAGE_TRANSFER_SRC_BIT;
		pinfo.sharingMode = data.info.imageSharingMode;
		assert(pinfo.sharingMode == VK_SHARING_MODE_EXCLUSIVE); // TBD
		pinfo.queueFamilyIndexCount = selected_queue_family_index;
		pinfo.initialLayout = VK_IMAGE_LAYOUT_UNDEFINED;
		data.virtual_images.resize(stored_image_count);
		for (unsigned i = 0; i < stored_image_count; i++)
		{
			result = wrap_vkCreateImage(device, &pinfo, nullptr, &data.virtual_images[i]);
			assert(result == VK_SUCCESS);
		}
		device_data.allocator->virtualswap_images(data.virtual_images);

		if (is_noscreen())
		{
			data.pSwapchainImages.resize(stored_image_count);
			for (uint32_t i = 0; i < stored_image_count; i++) data.pSwapchainImages[i] = data.virtual_images[i];
		}

		// Setup image region for later copy
		data.virtual_image_copy_region.srcSubresource = { VK_IMAGE_ASPECT_COLOR_BIT, 0, 0, 1 };
		data.virtual_image_copy_region.srcOffset = { 0, 0, 0 };
		data.virtual_image_copy_region.dstSubresource = { VK_IMAGE_ASPECT_COLOR_BIT, 0, 0, 1 };
		data.virtual_image_copy_region.dstOffset = { 0, 0, 0 };
		data.virtual_image_copy_region.extent.width = data.info.imageExtent.width;
		data.virtual_image_copy_region.extent.height = data.info.imageExtent.height;
		data.virtual_image_copy_region.extent.depth = 1;
		// Make fences
		data.inflight.resize(stored_image_count, false);
		VkFenceCreateInfo fenceinfo = { VK_STRUCTURE_TYPE_FENCE_CREATE_INFO, nullptr, 0 };
		data.virtual_fences.resize(stored_image_count);
		for (unsigned i = 0; i < stored_image_count; i++)
		{
			result = wrap_vkCreateFence(device, &fenceinfo, nullptr, &data.virtual_fences[i]);
			assert(result == VK_SUCCESS);
		}
		// Make shared semaphore
		VkSemaphoreCreateInfo semainfo = { VK_STRUCTURE_TYPE_SEMAPHORE_CREATE_INFO, nullptr, 0 };
		result = wrap_vkCreateSemaphore(device, &semainfo, nullptr, &data.virtual_semaphore);
		assert(result == VK_SUCCESS);
		// Make virtual image commandbuffers
		VkCommandPoolCreateInfo command_pool_create_info = {};
		command_pool_create_info.sType = VK_STRUCTURE_TYPE_COMMAND_POOL_CREATE_INFO;
		command_pool_create_info.flags = VK_COMMAND_POOL_CREATE_RESET_COMMAND_BUFFER_BIT;
		command_pool_create_info.queueFamilyIndex = selected_queue_family_index;
		result = wrap_vkCreateCommandPool(device, &command_pool_create_info, NULL, &data.virtual_cmdpool);
		assert(result == VK_SUCCESS);
		assert(data.virtual_cmdpool != VK_NULL_HANDLE);
		VkCommandBufferAllocateInfo command_buffer_allocate_info = {};
		command_buffer_allocate_info.sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO;
		command_buffer_allocate_info.commandPool = data.virtual_cmdpool;
		command_buffer_allocate_info.commandBufferCount = stored_image_count;
		data.virtual_cmdbuffers.resize(stored_image_count);
		result = wrap_vkAllocateCommandBuffers(device, &command_buffer_allocate_info, data.virtual_cmdbuffers.data());
		assert(result == VK_SUCCESS);
	}

	for (uint32_t i = 0; i < stored_image_count; i++)
	{
		const uint32_t remap_index = reader.read_handle(DEBUGPARAM("VkImage"));
		if (!reader.run) index_to_VkImage.set(remap_index, fake_handle<VkImage>(remap_index));
		else if (!is_virtualswapchain()) index_to_VkImage.set(remap_index, data.pSwapchainImages[i]);
		else index_to_VkImage.set(remap_index, data.virtual_images[i]);
		DLOG("Image index %u is swapchain image index %u", remap_index, i);
	}
	data.initialized = true; // in case this function is called more than once
}

void retrace_vkCreateAndroidSurfaceKHR(lava_file_reader& reader)
{
	VkInstance instance = index_to_VkInstance.at(reader.read_handle(DEBUGPARAM("VkInstance")));
	const uint32_t sType = static_cast<VkStructureType>(reader.read_uint32_t());
	assert(sType == VK_STRUCTURE_TYPE_ANDROID_SURFACE_CREATE_INFO_KHR);

	VkBaseOutStructure* pNext = nullptr;
	read_extension(reader, (VkBaseOutStructure**)&pNext);

	const uint32_t flags = reader.read_uint32_t();
	(void)flags; // nothing here yet
	const int32_t x = reader.read_int32_t();
	const int32_t y = reader.read_int32_t();
	const int32_t width = reader.read_int32_t();
	const int32_t height = reader.read_int32_t();
	DLOG("window originally from android width=%d height=%d", width, height);
	const int32_t stride = reader.read_int32_t();
	(void)stride; // just here FYI
	const int32_t format = reader.read_int32_t();
	(void)format; // nothing here yet
	// Execute
	const uint32_t retval = reader.read_uint32_t();
	(void)retval;
	const uint32_t surface_index = reader.read_handle(DEBUGPARAM("VkSurfaceKHR"));
	VkSurfaceKHR pSurface = VK_NULL_HANDLE;
	if (!is_noscreen() && reader.run) window_create(instance, surface_index, x, y, width, height);
	else pSurface = fake_handle<VkSurfaceKHR>(surface_index);
	// Post
	index_to_VkSurfaceKHR.set(surface_index, pSurface);
}

void retrace_vkCreateXcbSurfaceKHR(lava_file_reader& reader)
{
	VkInstance instance = index_to_VkInstance.at(reader.read_handle(DEBUGPARAM("VkInstance")));
	const uint32_t sType = static_cast<VkStructureType>(reader.read_uint32_t());
	assert(sType == VK_STRUCTURE_TYPE_XCB_SURFACE_CREATE_INFO_KHR);

	VkBaseOutStructure* pNext = nullptr;
	read_extension(reader, (VkBaseOutStructure**)&pNext);

	const uint32_t flags = reader.read_uint32_t(); // VkXcbSurfaceCreateFlagsKHR
	(void)flags; // nothing here yet
	const int32_t x = reader.read_int32_t();
	const int32_t y = reader.read_int32_t();
	const int32_t width = reader.read_int32_t();
	const int32_t height = reader.read_int32_t();
	DLOG("window originally from xcb width=%d height=%d", width, height);
	const int32_t border_width = reader.read_int32_t();
	(void)border_width; // ignore
	const int32_t depth = reader.read_int32_t();
	(void)depth; // ignore
	// Execute
	const uint32_t retval = reader.read_uint32_t();
	(void)retval;
	const uint32_t surface_index = reader.read_handle(DEBUGPARAM("VkSurfaceKHR"));
	VkSurfaceKHR pSurface = VK_NULL_HANDLE;
	if (!is_noscreen() && reader.run)
	{
		pSurface = window_create(instance, surface_index, x, y, width, height);
	}
	else pSurface = fake_handle<VkSurfaceKHR>(surface_index);
	// Post
	index_to_VkSurfaceKHR.set(surface_index, pSurface);
}

void retrace_vkCreateXlibSurfaceKHR(lava_file_reader& reader)
{
	VkInstance instance = index_to_VkInstance.at(reader.read_handle(DEBUGPARAM("VkInstance")));
	const uint32_t sType = static_cast<VkStructureType>(reader.read_uint32_t());
	assert(sType == VK_STRUCTURE_TYPE_XLIB_SURFACE_CREATE_INFO_KHR);

	VkBaseOutStructure* pNext = nullptr;
	read_extension(reader, (VkBaseOutStructure**)&pNext);

	const uint32_t flags = reader.read_uint32_t();
	(void)flags; // nothing here yet
	const int32_t x = reader.read_int32_t();
	const int32_t y = reader.read_int32_t();
	const int32_t width = reader.read_int32_t();
	const int32_t height = reader.read_int32_t();
	DLOG("window originally from xlib width=%d height=%d", width, height);
	const int32_t border_width = reader.read_int32_t();
	(void)border_width; // ignore
	const int32_t depth = reader.read_int32_t();
	(void)depth; // ignore
	// Execute
	const uint32_t retval = reader.read_uint32_t();
	(void)retval;
	const uint32_t surface_index = reader.read_handle(DEBUGPARAM("VkSurfaceKHR"));
	VkSurfaceKHR pSurface = VK_NULL_HANDLE;
	if (!is_noscreen() && reader.run) pSurface = window_create(instance, surface_index, x, y, width, height);
	else pSurface = fake_handle<VkSurfaceKHR>(surface_index);
	// Post
	index_to_VkSurfaceKHR.set(surface_index, pSurface);
}

void retrace_vkCreateWaylandSurfaceKHR(lava_file_reader& reader)
{
	VkInstance instance = index_to_VkInstance.at(reader.read_handle(DEBUGPARAM("VkInstance")));
	const uint32_t sType = reader.read_uint32_t();
	assert(sType == VK_STRUCTURE_TYPE_WAYLAND_SURFACE_CREATE_INFO_KHR);

	VkBaseOutStructure* pNext = nullptr;
	read_extension(reader, (VkBaseOutStructure**)&pNext);

	const uint32_t flags = reader.read_uint32_t();
	(void)flags; // nothing
	// Execute
	const int32_t x = reader.read_int32_t();
	const int32_t y = reader.read_int32_t();
	const int32_t width = reader.read_int32_t();
	const int32_t height = reader.read_int32_t();
	DLOG("window originally from wayland width=%d height=%d", width, height);
	(void)reader.read_int32_t(); // reserved
	const uint32_t retval = reader.read_uint32_t();
	(void)retval;
	const uint32_t surface_index = reader.read_handle(DEBUGPARAM("VkSurfaceKHR"));
	VkSurfaceKHR pSurface = VK_NULL_HANDLE;
	if (!is_noscreen() && reader.run) pSurface = window_create(instance, surface_index, x, y, width, height);
	else pSurface = fake_handle<VkSurfaceKHR>(surface_index);
	// Post
	index_to_VkSurfaceKHR.set(surface_index, pSurface);
}

void retrace_vkCreateHeadlessSurfaceEXT(lava_file_reader& reader)
{
	VkInstance instance = index_to_VkInstance.at(reader.read_handle(DEBUGPARAM("VkInstance")));
	const uint32_t sType = reader.read_uint32_t();
	assert(sType == VK_STRUCTURE_TYPE_HEADLESS_SURFACE_CREATE_INFO_EXT);

	VkBaseOutStructure* pNext = nullptr;
	read_extension(reader, (VkBaseOutStructure**)&pNext);

	const uint32_t flags = reader.read_uint32_t();
	(void)flags; // nothing
	// Execute
	(void)reader.read_int32_t();
	(void)reader.read_int32_t();
	(void)reader.read_int32_t();
	(void)reader.read_int32_t();
	(void)reader.read_int32_t();
	(void)reader.read_int32_t();
	// We must read these values from JSON, since they are only set when the
	// first frame is drawn, from the swapchain imageExtent. Assuming only a
	// single swapchain and surface for now.
	trackedswapchain_replay& t = VkSwapchainKHR_index.at(0);
	const int32_t x = 0;
	const int32_t y = 0;
	const int32_t width = t.info.imageExtent.width;
	const int32_t height = t.info.imageExtent.height;
	DLOG("window originally from headless width=%d height=%d (values taken from json)", width, height);
	const uint32_t retval = reader.read_uint32_t();
	(void)retval;
	const uint32_t surface_index = reader.read_handle(DEBUGPARAM("VkSurfaceKHR"));
	VkSurfaceKHR pSurface = VK_NULL_HANDLE;
	if (!is_noscreen() && reader.run) pSurface = window_create(instance, surface_index, x, y, width, height);
	else pSurface = fake_handle<VkSurfaceKHR>(surface_index);
	// Post
	index_to_VkSurfaceKHR.set(surface_index, pSurface);
}

void retrace_vkCreateWin32SurfaceKHR(lava_file_reader& reader)
{
	assert(false);
}

void retrace_vkCreateDirectFBSurfaceEXT(lava_file_reader& reader)
{
	assert(false);
}

void retrace_vkCreateMetalSurfaceEXT(lava_file_reader& reader)
{
	assert(false);
}

void retrace_vkGetDeviceQueue2(lava_file_reader& reader)
{
	const uint32_t device_index = reader.read_handle(DEBUGPARAM("VkDevice"));
	VkDevice device = index_to_VkDevice.at(device_index);
	VkDeviceQueueInfo2 info_real = {};
	read_VkDeviceQueueInfo2(reader, &info_real);
	bool virtual_family = false;
	uint32_t realIndex = info_real.queueIndex;
	uint32_t realFamily = info_real.queueFamilyIndex;
	VkQueue queue = fake_handle<VkQueue>((info_real.queueFamilyIndex << 16) + info_real.queueIndex);
	if (info_real.queueFamilyIndex == LAVATUBE_VIRTUAL_QUEUE && reader.run)
	{
		info_real.queueFamilyIndex = selected_queue_family_index;
		virtual_family = true;
		const VkQueueFamilyProperties& props = device_VkQueueFamilyProperties.at(info_real.queueFamilyIndex);
		if (info_real.queueIndex >= props.queueCount) // we don't have enough queues
		{
			info_real.queueIndex = 0; // map to first queue
		}
	}
	else if (info_real.queueFamilyIndex >= device_VkQueueFamilyProperties.size())
	{
		ILOG("Changing queue family %u to %u", info_real.queueFamilyIndex, selected_queue_family_index);
		info_real.queueFamilyIndex = selected_queue_family_index;
	}
	if (reader.run)
	{
		wrap_vkGetDeviceQueue2(device, &info_real, &queue);
		assert(queue != VK_NULL_HANDLE);
	}
	const uint32_t stored_queue_index = reader.read_handle(DEBUGPARAM("VkQueue"));
	if (!index_to_VkQueue.contains(stored_queue_index))
	{
		index_to_VkQueue.set(stored_queue_index, queue);
		auto& queue_data = VkQueue_index.at(stored_queue_index);
		queue_data.device = device;
		queue_data.queueIndex = info_real.queueIndex;
		queue_data.queueFamily = info_real.queueFamilyIndex;
		queue_data.realIndex = realIndex;
		queue_data.realFamily = realFamily;
		queue_data.realQueue = queue;
		if (reader.run)
		{
			const VkQueueFamilyProperties& props = device_VkQueueFamilyProperties.at(info_real.queueFamilyIndex);
			queue_data.queueFlags = props.queueFlags;
		}
		queue_data.physicalDevice = VkDevice_index.at(device_index).physicalDevice;
	}
}

void retrace_vkGetDeviceQueue(lava_file_reader& reader)
{
	const uint32_t device_index = reader.read_handle(DEBUGPARAM("VkDevice"));
	VkDevice device = index_to_VkDevice.at(device_index);
	uint32_t queueFamilyIndex = reader.read_uint32_t();
	uint32_t queueIndex = reader.read_uint32_t();
	VkQueue queue = fake_handle<VkQueue>((queueFamilyIndex << 16) + queueIndex);
	bool virtual_family = false;
	uint32_t realIndex = queueIndex;
	uint32_t realFamily = queueFamilyIndex;
	if (queueFamilyIndex == LAVATUBE_VIRTUAL_QUEUE && reader.run)
	{
		queueFamilyIndex = selected_queue_family_index;
		virtual_family = true;
		const VkQueueFamilyProperties& props = device_VkQueueFamilyProperties.at(queueFamilyIndex);
		if (queueIndex >= props.queueCount) // we don't have enough queues
		{
			queueIndex = 0; // map to first queue
		}
	}
	else if (queueFamilyIndex >= device_VkQueueFamilyProperties.size())
	{
		ILOG("Changing queue family %u to %u", queueFamilyIndex, selected_queue_family_index);
		queueFamilyIndex = selected_queue_family_index;
	}
	if (reader.run)
	{
		wrap_vkGetDeviceQueue(device, queueFamilyIndex, queueIndex, &queue);
		assert(queue != VK_NULL_HANDLE);
	}
	const uint32_t stored_queue_index = reader.read_handle(DEBUGPARAM("VkQueue"));
	if (!index_to_VkQueue.contains(stored_queue_index))
	{
		index_to_VkQueue.set(stored_queue_index, queue);
		auto& queue_data = VkQueue_index.at(stored_queue_index);
		queue_data.device = device;
		queue_data.queueIndex = queueIndex;
		queue_data.queueFamily = queueFamilyIndex;
		queue_data.realIndex = realIndex;
		queue_data.realFamily = realFamily;
		queue_data.realQueue = queue;
		if (reader.run)
		{
			const VkQueueFamilyProperties& props = device_VkQueueFamilyProperties.at(queueFamilyIndex);
			queue_data.queueFlags = props.queueFlags;
		}
		queue_data.physicalDevice = VkDevice_index.at(device_index).physicalDevice;
	}
}

void read_hw_buffer(lava_file_reader& reader)
{
	reader.read_uint32_t(); // hw_buffer_description.width
	reader.read_uint32_t(); // hw_buffer_description.height
	reader.read_uint32_t(); // hw_buffer_description.layers
	reader.read_uint32_t(); // hw_buffer_description.format
	reader.read_uint64_t(); // hw_buffer_description.usage
	reader.read_uint32_t(); // hw_buffer_description.stride
	reader.read_uint32_t(); // hw_buffer_description.rfu0
	reader.read_uint64_t(); // hw_buffer_description.rfu1
	reader.read_uint32_t(); // bpp
}

void retrace_vkGetAndroidHardwareBufferPropertiesANDROID(lava_file_reader& reader)
{
	// Load
	const uint32_t device_index = reader.read_handle(DEBUGPARAM("VkDevice"));

	// Unused metadata
	read_hw_buffer(reader);

	// Execute
	VkResult retval = VK_SUCCESS;
	VkResult stored_retval = static_cast<VkResult>(reader.read_uint32_t());
	check_retval(stored_retval, retval);
	// Post
	// single length struct follows
	VkStructureType pProperties_sType = static_cast<VkStructureType>(reader.read_uint32_t());
	assert(pProperties_sType == VK_STRUCTURE_TYPE_ANDROID_HARDWARE_BUFFER_PROPERTIES_ANDROID);

	VkBaseOutStructure* pNext = nullptr;
	read_extension(reader, (VkBaseOutStructure**)&pNext);

	const uint64_t pProperties_allocationSize = reader.read_uint64_t();
	const uint32_t memoryTypeBits = reader.read_uint32_t();
}

// TBD - this needs fixing
void retrace_vkGetMemoryAndroidHardwareBufferANDROID(lava_file_reader& reader)
{
	const uint32_t device_index = reader.read_handle(DEBUGPARAM("VkDevice"));
	VkStructureType sType = static_cast<VkStructureType>(reader.read_uint32_t());
	assert(sType == VK_STRUCTURE_TYPE_MEMORY_GET_ANDROID_HARDWARE_BUFFER_INFO_ANDROID);

	VkBaseOutStructure* pNext = nullptr;
	read_extension(reader, (VkBaseOutStructure**)&pNext);

	const uint32_t memory_index = reader.read_handle(DEBUGPARAM("VkDeviceMemory"));
	(void)memory_index;

	// Execute
	VkResult retval = VK_SUCCESS;
	VkResult stored_retval = static_cast<VkResult>(reader.read_uint32_t());
	check_retval(stored_retval, retval);

	// Unused metadata
	read_hw_buffer(reader);
}

void retrace_vkEnumerateInstanceLayerProperties(lava_file_reader& reader)
{
	// Declarations
	std::vector<VkLayerProperties> pProperties;
	uint32_t pPropertyCount = 0;
	// Load
	uint8_t do_call = reader.read_uint8_t();
	// Execute
	if (do_call == 1 && reader.run)
	{
		VkResult retval = wrap_vkEnumerateInstanceLayerProperties(&pPropertyCount, nullptr);
		assert(retval == VK_SUCCESS);
		pProperties.resize(pPropertyCount);
		retval = wrap_vkEnumerateInstanceLayerProperties(&pPropertyCount, pProperties.data());
		assert(retval == VK_SUCCESS);
		(void)retval; // ignore return value
	}
	(void)reader.read_uint32_t(); // ignore stored return value
	// Post
}

void retrace_vkEnumerateInstanceExtensionProperties(lava_file_reader& reader)
{
	// Declarations
	const char* pLayerName = nullptr;
	std::vector<VkExtensionProperties> pProperties;
	uint32_t pPropertyCount = 0;
	// Load
	pLayerName = reader.read_string();
	uint8_t do_call = reader.read_uint8_t();
	// Execute
	if (do_call == 1 && reader.run)
	{
		VkResult retval = wrap_vkEnumerateInstanceExtensionProperties(pLayerName, &pPropertyCount, nullptr);
		assert(retval == VK_SUCCESS);
		pProperties.resize(pPropertyCount);
		retval = wrap_vkEnumerateInstanceExtensionProperties(pLayerName, &pPropertyCount, pProperties.data());
		assert(retval == VK_SUCCESS);
		(void)retval; // ignore return value
	}
	(void)reader.read_uint32_t(); // ignore stored return value
	// Post
}

void retrace_vkEnumerateDeviceLayerProperties(lava_file_reader& reader)
{
	// Declarations
	std::vector<VkLayerProperties> pProperties;
	uint32_t pPropertyCount = 0;
	// Load
	const uint8_t initialized = reader.read_uint8_t();
	uint32_t physicalDevice_index;
	if (initialized)
	{
		physicalDevice_index = reader.read_handle(DEBUGPARAM("VkPhysicalDevice"));
	}

	const uint8_t do_call = reader.read_uint8_t();
	// Execute
	if (do_call == 1 && reader.run)
	{
		VkResult retval = wrap_vkEnumerateDeviceLayerProperties(selected_physical_device, &pPropertyCount, nullptr);
		assert(retval == VK_SUCCESS);
		pProperties.resize(pPropertyCount);
		retval = wrap_vkEnumerateDeviceLayerProperties(selected_physical_device, &pPropertyCount, pProperties.data());
		assert(retval == VK_SUCCESS);
		(void)retval; // ignore return value
	}
	(void)reader.read_uint32_t(); // ignore stored return value
	// Post
}

void retrace_vkEnumerateDeviceExtensionProperties(lava_file_reader& reader)
{
	// Declarations
	const char* pLayerName = nullptr;
	std::vector<VkExtensionProperties> pProperties;
	uint32_t pPropertyCount = 0;
	// Load
	const uint8_t initialized = reader.read_uint8_t();
	uint32_t physicalDevice_index;
	if (initialized)
	{
		physicalDevice_index = reader.read_handle(DEBUGPARAM("VkPhysicalDevice"));
	}
	pLayerName = reader.read_string();
	const uint8_t do_call = reader.read_uint8_t();
	// Execute
	if (do_call == 1 && reader.run)
	{
		VkResult retval = wrap_vkEnumerateDeviceExtensionProperties(selected_physical_device, pLayerName, &pPropertyCount, nullptr);
		assert(retval == VK_SUCCESS);
		pProperties.resize(pPropertyCount);
		retval = wrap_vkEnumerateDeviceExtensionProperties(selected_physical_device, pLayerName, &pPropertyCount, pProperties.data());
		assert(retval == VK_SUCCESS);
		(void)retval; // ignore return value
	}
	(void)reader.read_uint32_t(); // ignore stored return value
	// Post
}

void retrace_vkGetPhysicalDeviceXlibPresentationSupportKHR(lava_file_reader& reader)
{
	uint32_t physicaldevice_index = reader.read_handle(DEBUGPARAM("VkPhysicalDevice"));
	uint32_t queueFamilyIndex = reader.read_uint32_t();
	// this function is ignored on replay
	(void)reader.read_uint32_t(); // also ignore result return value
}

// --- read helpers : legacy code ---

void image_update(lava_file_reader& reader, uint32_t device_index, uint32_t image_index, uint64_t size, const VkBaseOutStructure* sptr)
{
	VkDevice device = index_to_VkDevice.at(device_index);
	const auto& device_data = VkDevice_index.at(device_index);
	suballoc_location loc = device_data.allocator->find_image_memory(image_index);
	DLOG2("image update idx=%u flush=%s init=%s size=%lu", image_index, loc.needs_flush ? "yes" : "no", loc.needs_init ? "yes" : "no", (unsigned long)loc.size);
	assert(sptr == nullptr);
	trackedimage& image_data = VkImage_index.at(image_index);
	char* ptr = mem_map(reader, device, loc);
	int32_t changed = 0;
	if (!reader.run && loc.needs_init) image_data.source.register_source(0, loc.size, reader.current);
	if (!reader.run) reader.read_patch_tracking(ptr, loc.size, image_data.source);
	else reader.read_patch(ptr, loc.size);
	mem_unmap(reader, device, loc, nullptr, ptr);
}

void buffer_update(lava_file_reader& reader, uint32_t device_index, uint32_t buffer_index, uint64_t size, const VkBaseOutStructure* sptr)
{
	const auto& device_data = VkDevice_index.at(device_index);
	suballoc_location loc = device_data.allocator->find_buffer_memory(buffer_index);
	DLOG2("buffer update idx=%u flush=%s init=%s size=%lu", buffer_index, loc.needs_flush ? "yes" : "no", loc.needs_init ? "yes" : "no", (unsigned long)loc.size);
	assert(sptr == nullptr || sptr->sType == VK_STRUCTURE_TYPE_MARKED_OFFSETS_ARM);
	VkDevice device = index_to_VkDevice.at(device_index);
	trackedbuffer& buffer_data = VkBuffer_index.at(buffer_index);
	char* ptr = mem_map(reader, device, loc);
	int32_t changed = 0;

	if (!reader.run && loc.needs_init) buffer_data.source.register_source(0, loc.size, reader.current);

	if (reader.parent->remap_scan) reader.read_patch_scanning(ptr, loc.size, buffer_data);
	else if (!reader.run) reader.read_patch_tracking(ptr, loc.size, buffer_data.source);
	else reader.read_patch(ptr, loc.size);

	if (sptr) translate_marked_offsets(reader, (VkMarkedOffsetsARM*)sptr, ptr);

	mem_unmap(reader, device, loc, nullptr, ptr);
}

void tensor_update(lava_file_reader& reader, uint32_t device_index, uint32_t tensor_index, uint64_t size, const VkBaseOutStructure* sptr)
{
	const auto& device_data = VkDevice_index.at(device_index);
	suballoc_location loc = device_data.allocator->find_tensor_memory(tensor_index);
	DLOG2("tensor update idx=%u flush=%s init=%s size=%lu", tensor_index, loc.needs_flush ? "yes" : "no", loc.needs_init ? "yes" : "no", (unsigned long)loc.size);
	assert(sptr == nullptr);
	VkDevice device = index_to_VkDevice.at(device_index);
	trackedtensor& tensor_data = VkTensorARM_index.at(tensor_index);
	char* ptr = mem_map(reader, device, loc);
	int32_t changed = 0;
	if (!reader.run && loc.needs_init) tensor_data.source.register_source(0, loc.size, reader.current);
	if (!reader.run) reader.read_patch_tracking(ptr, loc.size, tensor_data.source);
	else reader.read_patch(ptr, loc.size);
	mem_unmap(reader, device, loc, nullptr, ptr);
}

static inline int64_t read_version2_packet(lava_file_reader& reader, VkBaseOutStructure** sptr)
{
	const uint64_t size = reader.read_uint64_t();
	const uint16_t flags = reader.read_uint16_t();
	if (flags & PACKET_FLAG_HAS_PNEXT) read_extension(reader, sptr);
	else assert(flags == 0);
	return size;
}

uint32_t update_image_packet(uint8_t instrtype, lava_file_reader& reader)
{
	DLOG2("Update image packet (%d) on thread %d", (int)instrtype, reader.thread_index());
	const uint32_t device_index = reader.read_handle(DEBUGPARAM("VkDevice"));
	const uint32_t image_index = reader.read_handle(DEBUGPARAM("VkImage"));
	VkBaseOutStructure* sptr = nullptr;
	uint64_t size = 0;
	if (instrtype == PACKET_IMAGE_UPDATE2) size = read_version2_packet(reader, &sptr);
	image_update(reader, device_index, image_index, size, sptr);
	return image_index;
}

uint32_t update_buffer_packet(uint8_t instrtype, lava_file_reader& reader)
{
	DLOG2("Update buffer packet (%d) on thread %d", (int)instrtype, reader.thread_index());
	const uint32_t device_index = reader.read_handle(DEBUGPARAM("VkDevice"));
	const uint32_t buffer_index = reader.read_handle(DEBUGPARAM("VkBuffer"));
	VkBaseOutStructure* sptr = nullptr;
	uint64_t size = 0;
	if (instrtype == PACKET_BUFFER_UPDATE2) size = read_version2_packet(reader, &sptr);
	buffer_update(reader, device_index, buffer_index, size, sptr);
	return buffer_index;
}

uint32_t update_tensor_packet(uint8_t instrtype, lava_file_reader& reader)
{
	DLOG2("Update tensor packet (%d) on thread %d", (int)instrtype, reader.thread_index());
	const uint32_t device_index = reader.read_handle(DEBUGPARAM("VkDevice"));
	const uint32_t tensor_index = reader.read_handle(DEBUGPARAM("VkTensorARM"));
	VkBaseOutStructure* sptr = nullptr;
	const uint64_t size = read_version2_packet(reader, &sptr);
	tensor_update(reader, device_index, tensor_index, size, sptr);
	return tensor_index;
}

void switchboard_packet(uint8_t instrtype, lava_file_reader& reader)
{
	if (instrtype == PACKET_VULKAN_API_CALL)
	{
		reader.read_apicall();
	}
	else if (instrtype == PACKET_THREAD_BARRIER)
	{
		reader.read_barrier();
	}
	else if (instrtype == PACKET_IMAGE_UPDATE || instrtype == PACKET_IMAGE_UPDATE2)
	{
		update_image_packet(instrtype, reader);
	}
	else if (instrtype == PACKET_BUFFER_UPDATE || instrtype == PACKET_BUFFER_UPDATE2)
	{
		update_buffer_packet(instrtype, reader);
	}
	else if (instrtype == PACKET_TENSOR_UPDATE)
	{
		update_tensor_packet(instrtype, reader);
	}
	reader.device = VK_NULL_HANDLE;
	reader.physicalDevice = VK_NULL_HANDLE;
}

// -- terminate everything cleanly

template<typename T, typename U, typename V>
void terminate(T vec, U owner, V destroyer)
{
	VkAllocationCallbacks allocator = {};
	VkAllocationCallbacks* pAllocator = &allocator;
	allocators_set(pAllocator);
	for (unsigned i = 0; i < vec.size(); i++)
	{
		if (vec.contains(i)) destroyer(owner, vec.at(i), pAllocator);
	}
}

void terminate_all(lava_file_reader& reader, VkDevice stored_device)
{
	for (unsigned i = 0; i < index_to_VkCommandPool.size(); i++)
	{
		VkCommandPool pool = index_to_VkCommandPool.at(i);
		wrap_vkResetCommandPool(stored_device, pool, VK_COMMAND_POOL_RESET_RELEASE_RESOURCES_BIT);
	}
	terminate(index_to_VkDebugUtilsMessengerEXT, stored_instance, wrap_vkDestroyDebugUtilsMessengerEXT);
	terminate(index_to_VkValidationCacheEXT, stored_device, wrap_vkDestroyValidationCacheEXT);
	terminate(index_to_VkDebugReportCallbackEXT, stored_instance, wrap_vkDestroyDebugReportCallbackEXT);
	terminate(index_to_VkSamplerYcbcrConversion, stored_device, wrap_vkDestroySamplerYcbcrConversionKHR);
	terminate(index_to_VkDescriptorUpdateTemplate, stored_device, wrap_vkDestroyDescriptorUpdateTemplateKHR);
	for (trackedswapchain_replay& t : VkSwapchainKHR_index)
	{
		replay_pre_vkDestroySwapchainKHR(reader, stored_device, index_to_VkSwapchainKHR.at(t.index), nullptr);
	}
	if (!is_noscreen()) terminate(index_to_VkSwapchainKHR, stored_device, wrap_vkDestroySwapchainKHR);
	if (!is_noscreen()) terminate(index_to_VkSurfaceKHR, stored_instance, wrap_vkDestroySurfaceKHR);
	for (uint32_t i = 0; i < index_to_VkSurfaceKHR.size() && !is_noscreen(); i++)
	{
		if (index_to_VkSurfaceKHR.contains(i)) window_destroy(stored_instance, i);
	}
	terminate(index_to_VkCommandPool, stored_device, wrap_vkDestroyCommandPool);
	terminate(index_to_VkFramebuffer, stored_device, wrap_vkDestroyFramebuffer);
	terminate(index_to_VkRenderPass, stored_device, wrap_vkDestroyRenderPass);
	terminate(index_to_VkDescriptorPool, stored_device, wrap_vkDestroyDescriptorPool);
	terminate(index_to_VkDescriptorSetLayout, stored_device, wrap_vkDestroyDescriptorSetLayout);
	terminate(index_to_VkSampler, stored_device, wrap_vkDestroySampler);
	terminate(index_to_VkPipelineLayout, stored_device, wrap_vkDestroyPipelineLayout);
	terminate(index_to_VkPipeline, stored_device, wrap_vkDestroyPipeline);
	for (auto& t : VkPipelineCache_index)
	{
		replay_pre_vkDestroyPipelineCache(reader, stored_device, index_to_VkPipelineCache.at(t.index), nullptr);
	}
	terminate(index_to_VkPipelineCache, stored_device, wrap_vkDestroyPipelineCache);
	terminate(index_to_VkShaderModule, stored_device, wrap_vkDestroyShaderModule);
	terminate(index_to_VkImageView, stored_device, wrap_vkDestroyImageView);
	for (uint32_t i = 0; i < index_to_VkImage.size(); i++) // do not attempt to delete swapchain images!
	{
		for (const trackedswapchain_replay& t : VkSwapchainKHR_index)
		{
			for (VkImage image : t.pSwapchainImages)
			{
				if (index_to_VkImage.contains(i) && index_to_VkImage.at(i) == image)
				{
					index_to_VkImage.unset(i);
				}
			}
		}
	}
	terminate(index_to_VkImage, stored_device, wrap_vkDestroyImage);
	terminate(index_to_VkBufferView, stored_device, wrap_vkDestroyBufferView);
	terminate(index_to_VkBuffer, stored_device, wrap_vkDestroyBuffer);
	terminate(index_to_VkQueryPool, stored_device, wrap_vkDestroyQueryPool);
	terminate(index_to_VkEvent, stored_device, wrap_vkDestroyEvent);
	terminate(index_to_VkSemaphore, stored_device, wrap_vkDestroySemaphore);
	terminate(index_to_VkFence, stored_device, wrap_vkDestroyFence);
	VkAllocationCallbacks allocator = {};
	VkAllocationCallbacks* pAllocator = &allocator;
	allocators_set(pAllocator);
	replay_pre_vkDestroyDevice(reader, stored_device, nullptr);
	wrap_vkDestroyDevice(stored_device, pAllocator);
	replay_pre_vkDestroyInstance(reader, stored_instance, nullptr);
	wrap_vkDestroyInstance(stored_instance, pAllocator);
}
